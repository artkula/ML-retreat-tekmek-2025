{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustafbjurstam/ML-retreat-tekmek-2025/blob/main/linear_and_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning for Material Science: Linear and Logistic Regression\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "This notebook introduces common machine learning techniques in the context of materials science and engineering. By working through realistic scenarios with engineering data, you will learn to:\n",
        "\n",
        "- Apply data analysis tools including Principal Component Analysis and feature importance evaluation to understand which material properties matter most for predicting mechanical behavior\n",
        "- Understand the concept of a cost function and how it guides the learning process in machine learning algorithms\n",
        "- Master linear regression for modeling continuous relationships such as stress-strain curves in materials testing\n",
        "- Apply logistic regression for binary classification problems such as predicting material failure under complex loading conditions\n",
        "- Combine machine learning methods with domain knowledge from materials science to build better predictive models and gain deeper insights into material behavior\n",
        "\n",
        "Throughout this notebook, you will see how machine learning works best when combined with physical understanding of the systems we are trying to model."
      ],
      "metadata": {
        "id": "main_title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "### Part 1: Linear Regression for Stress-Strain Modeling\n",
        "1. Problem Introduction: Tensile Testing\n",
        "2. Data Generation and Visualization\n",
        "3. Data Analysis Tools\n",
        "   - Principal Component Analysis\n",
        "   - R-squared for Feature Importance\n",
        "4. Linear Regression Fundamentals\n",
        "   - Hypothesis Definition\n",
        "   - Cost Function\n",
        "   - Gradient Descent\n",
        "5. Modeling Stress-Strain Relationships\n",
        "   - Simple Linear Model\n",
        "   - Polynomial Regression\n",
        "   - Interactive Feature Selection\n",
        "\n",
        "### Part 2: Logistic Regression for Failure Prediction\n",
        "1. Material Failure Under Complex Loading\n",
        "   - Von Mises Yield Criterion\n",
        "   - Multi-axial Stress States\n",
        "2. Logistic Regression Theory\n",
        "   - Sigmoid Function\n",
        "   - Cost Function for Classification\n",
        "   - Decision Boundaries\n",
        "3. Building a Failure Prediction Model\n",
        "   - Training the Model\n",
        "   - Comparing with Theory\n",
        "   - Interactive Threshold Selection\n",
        "4. Key Insights and Engineering Applications"
      ],
      "metadata": {
        "id": "table_of_contents"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup: Import all required libraries\n",
        "\n",
        "# Core scientific computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Interactive widgets and display\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Image, SVG, Video\n",
        "\n",
        "# Machine learning - preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "\n",
        "# Machine learning - decomposition\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Machine learning - models\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Machine learning - metrics\n",
        "from sklearn.metrics import r2_score, accuracy_score, classification_report\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(\"Ready to begin the machine learning journey through materials science.\")"
      ],
      "metadata": {
        "id": "setup_cell",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Linear Regression for Stress-Strain Modeling\n",
        "---"
      ],
      "metadata": {
        "id": "part1_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem introduction\n",
        "\n",
        "A colleague in the materials testing laboratory has been conducting tensile tests on a low-carbon steel alloy. During a tensile test, a specimen is stretched until it breaks, and the applied force and resulting elongation are measured. These measurements allow us to determine the stress-strain relationship of the material.\n",
        "\n",
        "**What are stress and strain?**\n",
        "\n",
        "Stress is the force per unit area applied to a material, measured in Pascals (Pa) or megapascals (MPa). When you pull on a metal bar, the stress tells us how much force is distributed across its cross-section.\n",
        "\n",
        "Strain is the relative deformation of the material - how much it stretches compared to its original length. If a 100 mm bar stretches to 105 mm, the strain is 0.05 or five percent. Strain is dimensionless since it is a ratio of lengths.\n",
        "\n",
        "**Elastic versus plastic deformation**\n",
        "\n",
        "When stress is low, materials deform elastically. This means they behave like a spring - remove the load and they return to their original shape. The relationship between stress and strain in this region is linear, following Hooke's law where stress equals Young's modulus times strain.\n",
        "\n",
        "Beyond a certain stress level called the yield strength, materials begin to deform plastically. Plastic deformation is permanent - the material does not return to its original shape when unloaded. In this region, the stress continues to increase with strain, but the relationship is no longer linear. The material work-hardens as dislocations in its crystal structure multiply and interact.\n",
        "\n",
        "Eventually, the specimen begins to neck - forming a localized region of reduced cross-section. After necking begins, the engineering stress (force divided by original area) actually decreases even though the true stress in the necked region continues to increase until fracture.\n",
        "\n",
        "Your colleague has collected data from 150 measurement points during a single tensile test. However, they also recorded various material properties and testing conditions. They have shared this data with you to help analyze which factors most strongly influence the stress-strain behavior. The challenge is to determine which material parameters are important for predicting stress and which provide little useful information.\n",
        "\n",
        "The data looks as follows:"
      ],
      "metadata": {
        "id": "problem_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate realistic stress-strain data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# =============================================================================\n",
        "# PHYSICS-BASED STRESS-STRAIN MODEL\n",
        "# =============================================================================\n",
        "\n",
        "def _yield_uts_epsu(carbon_wt, grain_um, temperature_C, strain_rate, heat_treatment):\n",
        "    \"\"\"\n",
        "    Compute yield strength, UTS and uniform elongation from metallurgy principles.\n",
        "    These relationships are based on established materials science:\n",
        "    - Hall-Petch: strength increases with decreasing grain size\n",
        "    - Carbon strengthening: higher carbon content increases strength\n",
        "    - Temperature effects: strength decreases at higher temperatures\n",
        "    - Strain rate sensitivity: higher rates increase flow stress\n",
        "    \"\"\"\n",
        "    # Heat treatment effect on strength\n",
        "    ht_bias = 0.0\n",
        "    if heat_treatment == 'normalized':\n",
        "        ht_bias = 70e6  # Normalized steel is stronger\n",
        "    elif heat_treatment == 'annealed':\n",
        "        ht_bias = -90e6  # Annealed steel is softer\n",
        "\n",
        "    # Hall-Petch relationship: strength proportional to 1/sqrt(grain_size)\n",
        "    k_hp = 220e6\n",
        "    hall_petch = k_hp / np.sqrt(grain_um)\n",
        "\n",
        "    # Yield strength calculation\n",
        "    sy_base = 260e6\n",
        "    sigma_y = (\n",
        "        sy_base\n",
        "        + 250e6 * carbon_wt                  # Carbon strengthening\n",
        "        + hall_petch                         # Grain size effect\n",
        "        + ht_bias                            # Heat treatment effect\n",
        "        - 0.35e6 * (temperature_C - 23.0)    # Temperature softening\n",
        "        + 25e6 * (np.log10(strain_rate) + 4) # Strain rate hardening\n",
        "    )\n",
        "    sigma_y = max(sigma_y, 200e6)\n",
        "\n",
        "    # Work-hardening capacity (decreases with carbon and fine grains)\n",
        "    delta_uts = 260e6 * (1.0 - 0.7 * carbon_wt) + 40e6 / np.sqrt(grain_um / 40.0)\n",
        "    sigma_uts = sigma_y + delta_uts\n",
        "\n",
        "    # Uniform elongation (ductility decreases with carbon)\n",
        "    epsilon_u = (\n",
        "        0.18\n",
        "        - 0.12 * carbon_wt\n",
        "        + 0.01 * np.log10(grain_um / 40.0)\n",
        "        - 0.0006 * (temperature_C - 23.0)\n",
        "    )\n",
        "    epsilon_u = float(np.clip(epsilon_u, 0.05, 0.18))\n",
        "    return sigma_y, sigma_uts, epsilon_u\n",
        "\n",
        "def _true_and_engineering_stress(strain, E, sigma_y, sigma_uts, epsilon_u, carbon_wt):\n",
        "    \"\"\"\n",
        "    Build a realistic stress-strain curve with:\n",
        "    - Elastic region (linear)\n",
        "    - Plastic region (work hardening)\n",
        "    - Necking region (engineering stress decreases)\n",
        "    \"\"\"\n",
        "    strain = np.asarray(strain)\n",
        "    epsilon_y = sigma_y / E\n",
        "    b = 12.0 + 10.0 * carbon_wt  # Work hardening rate\n",
        "\n",
        "    sigma_true = np.empty_like(strain, dtype=float)\n",
        "    for i, e in enumerate(strain):\n",
        "        if e <= epsilon_y:\n",
        "            # Elastic region: Hooke's law\n",
        "            sigma_true[i] = E * e\n",
        "        else:\n",
        "            # Plastic region: Voce hardening model\n",
        "            eps_p = e - epsilon_y\n",
        "            eps_p_u = max(epsilon_u - epsilon_y, 1e-6)\n",
        "            if e <= epsilon_u:\n",
        "                sigma_true[i] = sigma_uts - (sigma_uts - sigma_y) * np.exp(-b * eps_p / eps_p_u)\n",
        "            else:\n",
        "                # Post-uniform elongation\n",
        "                sigma_true[i] = sigma_uts * (1.0 + 0.04 * (e - epsilon_u))\n",
        "\n",
        "    # Engineering stress includes necking softening\n",
        "    ksoft = 8.0 + 18.0 * carbon_wt\n",
        "    sigma_eng = sigma_true.copy()\n",
        "    mask = strain > epsilon_u\n",
        "    sigma_eng[mask] = sigma_true[mask] * np.exp(-ksoft * (strain[mask] - epsilon_u))\n",
        "    return sigma_true, sigma_eng, epsilon_y\n",
        "\n",
        "def generate_realistic_stress_strain_data(N=150, seed=42):\n",
        "    \"\"\"\n",
        "    Generate a single-specimen data set with realistic material behavior.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    E = 210e9  # Young's modulus for steel (Pa)\n",
        "\n",
        "    # Baseline specimen properties (low-carbon steel)\n",
        "    baseline = dict(\n",
        "        carbon_wt=0.18,            # 0.18 wt% carbon\n",
        "        grain_um=40.0,             # 40 Œºm grain size\n",
        "        temperature_C=23.0,        # Room temperature\n",
        "        strain_rate=1e-4,          # Quasi-static testing\n",
        "        heat_treatment='as_rolled' # As-rolled condition\n",
        "    )\n",
        "    sy, uts, eps_u = _yield_uts_epsu(**baseline)\n",
        "\n",
        "    # Generate smooth fitted curve for reference\n",
        "    strain_grid = np.linspace(0.0, 0.25, 400)\n",
        "    sigma_true_grid, sigma_eng_grid, eps_y = _true_and_engineering_stress(\n",
        "        strain_grid, E, sy, uts, eps_u, baseline[\"carbon_wt\"]\n",
        "    )\n",
        "\n",
        "    # Sample N points from the same specimen with measurement noise\n",
        "    strain = np.sort(rng.uniform(0.0, 0.25, size=N))\n",
        "    sigma_true_pts, sigma_eng_pts, _ = _true_and_engineering_stress(\n",
        "        strain, E, sy, uts, eps_u, baseline[\"carbon_wt\"]\n",
        "    )\n",
        "    noise = rng.normal(0.0, 0.02 * np.maximum(sigma_eng_pts, 1.0), size=N)  # 2% noise\n",
        "    stress_measured = np.clip(sigma_eng_pts + noise, 0.0, None)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'strain': strain,\n",
        "        'stress': stress_measured,\n",
        "        'stress_eng_true': sigma_eng_pts,\n",
        "        'stress_true': sigma_true_pts,\n",
        "        'yield_strength': sy,\n",
        "        'uts': uts,\n",
        "        'epsilon_y': eps_y,\n",
        "        'epsilon_u': eps_u,\n",
        "        # Material properties\n",
        "        'carbon_content': np.full(N, baseline[\"carbon_wt\"]),\n",
        "        'grain_size_um': np.full(N, baseline[\"grain_um\"]),\n",
        "        'temperature_C': np.full(N, baseline[\"temperature_C\"]),\n",
        "        'strain_rate': np.full(N, baseline[\"strain_rate\"]),\n",
        "        'surface_roughness': rng.lognormal(-1, 0.2, size=N),\n",
        "        'specimen_thickness': np.full(N, 5.0),\n",
        "        'specimen_width': np.full(N, 10.0),\n",
        "        'heat_treatment': np.full(N, baseline[\"heat_treatment\"]),\n",
        "    })\n",
        "\n",
        "    fit = {\n",
        "        'strain_grid': strain_grid,\n",
        "        'sigma_eng_grid': sigma_eng_grid\n",
        "    }\n",
        "    return df, fit\n",
        "\n",
        "# Generate the data\n",
        "df, fit_curve = generate_realistic_stress_strain_data(N=150, seed=42)\n",
        "\n",
        "# Save to CSV for later use\n",
        "df.to_csv(\"realistic_stress_data.csv\", index=False)\n",
        "\n",
        "# Preview the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "generate_data",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, this raw data contains many columns. Some represent fundamental material properties like carbon content and grain size that we know from materials science affect mechanical behavior. Others like surface roughness might have minimal impact on the bulk stress-strain response. Our goal is to use data analysis methods to identify which features truly matter for predicting stress."
      ],
      "metadata": {
        "id": "6F3qsAkJZU23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data analysis\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis is among the most widely used tools for uncovering structure in data. The main idea behind PCA is to describe data in terms of directions of highest variance. In the two-dimensional example below, we can see that data can be described by the vector direction of the highest variance (the longest arrow) and another vector perpendicular to it. Variance can be thought of as how much spread there is in the data along a particular direction.\n",
        "\n",
        "What makes PCA powerful is that data of arbitrary dimensions can be described in terms of these variance-maximizing vector directions, called principal components. This helps us understand which combinations of features account for most of the variation in our measurements."
      ],
      "metadata": {
        "id": "qCx5E7XnZX-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure 1: PCA concept\n",
        "display(SVG(filename='/content/GaussianScatterPCA.svg'))"
      ],
      "metadata": {
        "id": "StApd81XZa5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image: https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg"
      ],
      "metadata": {
        "id": "xWgYPoNHZeaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is useful for data analysis to determine which aspects of data carry the most information. In the example below we can see how PCA enables dimensionality reduction. As we go from three dimensions to two dimensions, we omit PC3 (not shown in the image), which is a vector orthogonal to both PC1 and PC2. As we can see, removing PC3 does not significantly impact the shape of the data cloud because PC3 represents the direction in which the data varies the least. In proper terms, we say that PC3 has the lowest explained variance."
      ],
      "metadata": {
        "id": "LRW9r9NEZgih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure 2: Dimensionality reduction\n",
        "display(Image(filename='/content/PCA3to2.jpg'))"
      ],
      "metadata": {
        "id": "BMvYncjBZlMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image: https://medium.com/@TheDataGyan/dimensionality-reduction-with-pca-and-t-sne-in-r-2715683819"
      ],
      "metadata": {
        "id": "w07zD4rvZowN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply PCA to our materials testing data. We'll focus on the meaningful material properties and testing conditions. The table below shows how each material parameter contributes to each principal component. The columns are ordered with respect to the explained variance of each principal component, with the percentage shown in brackets. Take a look at the data and consider which material parameters seem to explain most of the variation in our stress-strain measurements."
      ],
      "metadata": {
        "id": "29eBTGYoZq8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PCA analysis on material properties\n",
        "\n",
        "# Select meaningful features for PCA\n",
        "# We include material properties and test conditions that we know from\n",
        "# materials science can influence stress-strain behavior\n",
        "feature_cols = [\n",
        "    'carbon_content',\n",
        "    'grain_size_um',\n",
        "    'temperature_C',\n",
        "    'strain_rate',\n",
        "    'specimen_thickness',\n",
        "    'specimen_width',\n",
        "    'surface_roughness'\n",
        "]\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Run PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "print(f\"Explained Variance Ratio: {np.round(explained_var, 2) * 100}%\")\n",
        "cumulative_var = explained_var.cumsum()\n",
        "\n",
        "# Loadings (feature contributions to each PC)\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    columns=[f\"PC{i+1} ({np.round(explained_var[i], 2) * 100:.0f}%)\" for i in range(len(feature_cols))],\n",
        "    index=feature_cols\n",
        ")\n",
        "print(\"\\nPCA Loadings (feature contributions to each principal component):\")\n",
        "print(loadings.round(3))\n",
        "print(\"\\nInterpretation: Large absolute values indicate that a feature strongly\")\n",
        "print(\"contributes to that principal component. Notice which material properties\")\n",
        "print(\"dominate the first few components that explain most variance.\")"
      ],
      "metadata": {
        "id": "viz_stress_strain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can plot the cumulative explained variance with respect to the principal components. This tells us how many principal components we need to capture most of the variation in our material parameters. For materials testing data where some properties are held relatively constant during a single test, we might expect the first few components to capture most of the variance."
      ],
      "metadata": {
        "id": "N6f7MVL-ZyqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cumulative explained variance\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(cumulative_var) + 1), cumulative_var, marker='o')\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Variance Explained\")\n",
        "plt.title(\"PCA: Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k41ak4zSZ8nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can visualize the first three principal components in three-dimensional space. The data points are colored by their corresponding stress value from the data set. This gives us a sense of how the material parameters and stress are related in this lower-dimensional representation."
      ],
      "metadata": {
        "id": "GFoMCL47Z_M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3D PCA projection\n",
        "\n",
        "# Use same features as before\n",
        "X = df[feature_cols].copy()\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Run PCA (first 3 components)\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "print(f\"Explained Variance Ratio (PC1‚Äì3): {np.round(explained_var, 3) * 100}%\")\n",
        "\n",
        "# Color by stress\n",
        "color_values = df['stress']\n",
        "\n",
        "# Create interactive 3D plot\n",
        "fig = go.Figure(data=[\n",
        "    go.Scatter3d(\n",
        "        x=X_pca[:, 0],\n",
        "        y=X_pca[:, 1],\n",
        "        z=X_pca[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=6,\n",
        "            color=color_values,\n",
        "            colorscale='Viridis',\n",
        "            colorbar=dict(title='Stress (Pa)'),\n",
        "            opacity=0.8\n",
        "        ),\n",
        "        text=[f\"Strain: {s:.4f}<br>Stress: {st/1e6:.1f} MPa\"\n",
        "              for s, st in zip(df['strain'], df['stress'])]\n",
        "    )\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Interactive 3D PCA Plot (PC1, PC2, PC3)\",\n",
        "    scene=dict(\n",
        "        xaxis_title='PC1',\n",
        "        yaxis_title='PC2',\n",
        "        zaxis_title='PC3'\n",
        "    ),\n",
        "    margin=dict(l=0, r=0, b=0, t=30)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "E1lbymqcaBkm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R-squared (R¬≤)\n",
        "\n",
        "Another useful method for making sense of our materials data is R-squared, denoted as R¬≤. This metric helps us determine which individual material parameter can best predict the stress on its own, better than if we were to always guess the mean stress value regardless of the input.\n",
        "\n",
        "The R-squared value is calculated as:\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$$\n",
        "\n",
        "Where RSS is the residual sum of squares (how much our predictions miss the actual values) and TSS is the total sum of squares (how much the actual values vary from their mean). An R¬≤ value close to 1 means that feature explains most of the variation in stress, while a value close to 0 means it explains very little.\n",
        "\n",
        "The interactive demo below shows how R¬≤ changes as we adjust the slope of a fitted line. The residuals (red dashed lines) show the vertical distance between each data point and the model prediction."
      ],
      "metadata": {
        "id": "woxXqvevaGU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title R-squared demo\n",
        "\n",
        "# Generate example data\n",
        "x = np.linspace(0, 10, 10)\n",
        "true_slope = 2.0\n",
        "true_intercept = 1.0\n",
        "\n",
        "# Noisy observations\n",
        "y_data_r2 = true_slope * x + true_intercept + np.random.normal(scale=2.0, size=len(x))\n",
        "y_mean_r2 = np.mean(y_data_r2)\n",
        "\n",
        "# Rotation center\n",
        "x_mid = (x.min() + x.max()) / 2\n",
        "y_mid = true_slope * x_mid + true_intercept\n",
        "\n",
        "def plot_rotated_line(slope):\n",
        "    clear_output()\n",
        "    # Rotated model prediction\n",
        "    y_model_r2 = slope * (x - x_mid) + y_mid\n",
        "\n",
        "    # Compute R¬≤ score\n",
        "    r2 = r2_score(y_data_r2, y_model_r2)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x, y_data_r2, label='Data', color='orange', zorder=3)\n",
        "\n",
        "    # Plot model line\n",
        "    plt.plot(x, y_model_r2, label=f'Model (Slope = {slope:.2f})', color='blue')\n",
        "\n",
        "    once = True\n",
        "    # Vertical residuals\n",
        "    for xi, yi_data, yi_model in zip(x, y_data_r2, y_model_r2):\n",
        "        if once:\n",
        "            plt.plot([xi, xi], [yi_data, yi_model], 'r--', linewidth=1, label=\"Residuals\")\n",
        "            once = False\n",
        "        else:\n",
        "            plt.plot([xi, xi], [yi_data, yi_model], 'r--', linewidth=1)\n",
        "\n",
        "    # Mean line\n",
        "    plt.axhline(y_mean_r2, color='green', linestyle=':', label=f'Mean = {y_mean_r2:.2f}')\n",
        "\n",
        "    plt.xlabel(\"Input feature\")\n",
        "    plt.ylabel(\"Output\")\n",
        "    plt.title(f\"R¬≤ Score: {r2:.3f}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive slider\n",
        "slope_slider = widgets.FloatSlider(\n",
        "    value=true_slope,\n",
        "    min=-5.0,\n",
        "    max=5.0,\n",
        "    step=0.1,\n",
        "    description='Slope:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "widgets.interact(plot_rotated_line, slope=slope_slider)"
      ],
      "metadata": {
        "id": "V8iFjAZzaKrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the final R¬≤ value is maximized when we find the best line of fit. But how do we find this line mathematically? This is where linear regression comes in."
      ],
      "metadata": {
        "id": "R9-nmozIaM-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "### Hypothesis definition\n",
        "\n",
        "Linear regression is one of the most fundamental tools in machine learning and engineering. It is used for finding coefficients that fit a model to data. The model is our hypothesis - it is an analytical formula that we think can describe the data. The method is called *linear* regression because it requires that the hypothesis coefficients appear linearly. Examples of valid hypotheses are:\n",
        "\n",
        "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x $$\n",
        "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x^2 + \\theta_2 \\sin{x} $$\n",
        "\n",
        "whereas invalid hypothesis examples would be:\n",
        "\n",
        "$$ h_{\\theta}(x) = x^{\\theta_0} $$\n",
        "$$ h_{\\theta}(x) = \\theta_0^{x} + \\sin{(\\theta_1 x)}$$\n",
        "\n",
        "Notice that in the valid examples, the parameters Œ∏ appear linearly (multiplied by features), while in the invalid examples they appear in exponents.\n",
        "\n",
        "For now, let's define our hypothesis as a simple linear model:\n",
        "\n",
        "$$h_{\\theta}(x) = \\theta_0 + \\theta_1 x$$\n",
        "\n",
        "We will fit this to the stress and strain data. Let's first visualize the stress-strain relationship we are trying to model."
      ],
      "metadata": {
        "id": "szjXWt9HaO2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stress-strain data visualization\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(df['strain'], df['stress'] / 1e6, color='orange', alpha=0.7, edgecolors='k', s=30)\n",
        "plt.plot(fit_curve['strain_grid'], fit_curve['sigma_eng_grid'] / 1e6,\n",
        "         'b-', linewidth=2, alpha=0.7, label='True behavior')\n",
        "plt.xlabel(\"Strain\")\n",
        "plt.ylabel(\"Stress (MPa)\")\n",
        "plt.title(\"Stress vs. Strain for Low-Carbon Steel\")\n",
        "plt.legend(['Measured points','True curve'])\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice the three distinct regions:\")\n",
        "print(\"1. Elastic region: Linear increase at low strain\")\n",
        "print(\"2. Plastic region: Nonlinear work hardening\")\n",
        "print(\"3. Necking region: Stress decreases as specimen localizes deformation\")"
      ],
      "metadata": {
        "id": "35cB9YtIaSyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "\n",
        "To find the line of best fit, we need to define a metric that tells us what makes one line better or worse than another. Such a function is called the *cost function*. For linear regression, the cost function is typically defined as the mean squared error:\n",
        "\n",
        "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum^m_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "Where $y^{(i)}$ is the i-th stress value of $m$ points in our data set. By minimizing $J(\\theta_0, \\theta_1)$ we essentially minimize the average squared distance between what the model predicts, $h_{\\theta}(x^{(i)})$, and what the actual measured value is, $y^{(i)}$.\n",
        "\n",
        "The shape of the cost function depends on the data. For example, if our data set consists of only one point $(x, y) = (1,1)$, then the cost function would be:\n",
        "\n",
        "\\begin{align*}\n",
        "J(\\theta_0, \\theta_1)\n",
        "&= \\frac{1}{2 \\cdot 1} \\left(\\theta_0 + \\theta_1 \\cdot 1 - 1\\right)^2 \\\\\n",
        "&= \\frac{1}{2} (\\theta_0 + \\theta_1 - 1)^2\n",
        "\\end{align*}\n",
        "\n",
        "Notice that whenever $\\theta_0 + \\theta_1 = 1$, we have $J(\\theta_0, \\theta_1) = 0$. This means there is no single best line fitting this data set, which makes sense since infinitely many lines pass through a single point, and the cost function reflects this ambiguity.\n",
        "\n",
        "## Gradient descent updates\n",
        "\n",
        "Rather than solving for the minimum analytically, we can iteratively \"move\" the model coefficients toward values that reduce the cost. We do this by computing the gradient of the cost function, which tells us the direction of steepest increase. The update step is:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
        "\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where the partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "The hyperparameter $\\alpha$ is called the *learning rate*. A suitable value of $\\alpha$ results in quick convergence to a solution through a process called *gradient descent*. To better understand how linear regression converges, explore the demo below. First set a learning rate value with the slider, then run the following cell to simulate gradient descent. Experiment with what happens when $\\alpha$ is too large or too small."
      ],
      "metadata": {
        "id": "pN_UzhJXaYbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set the learning rate\n",
        "\n",
        "slider_log = widgets.FloatLogSlider(\n",
        "    value=0.7,\n",
        "    base=10,\n",
        "    min=-1,\n",
        "    max=0.5,\n",
        "    step=0.05,\n",
        "    description='Learning rate'\n",
        ")\n",
        "\n",
        "display(slider_log)"
      ],
      "metadata": {
        "id": "PbGasTujabgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradient descent demo\n",
        "\n",
        "# Generate simple data for demo\n",
        "m = 20\n",
        "theta0_true = 2\n",
        "theta1_true = 0.5\n",
        "x = np.linspace(-1, 1, m)\n",
        "y = theta0_true + theta1_true * x + np.random.randn(m) * 0.2\n",
        "\n",
        "def hypothesis(x, theta0, theta1):\n",
        "    return theta0 + theta1 * x\n",
        "\n",
        "def cost(theta0, theta1):\n",
        "    return np.mean((hypothesis(x, theta0, theta1) - y) ** 2) / 2\n",
        "\n",
        "# Cost surface\n",
        "theta0_vals = np.linspace(-1, 4, 100)\n",
        "theta1_vals = np.linspace(-5, 5, 100)\n",
        "T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
        "J_vals = np.vectorize(cost)(T0, T1)\n",
        "\n",
        "# Gradient descent\n",
        "N = 15\n",
        "alpha = slider_log.value\n",
        "theta0_initial = -0.9\n",
        "theta1_initial = -4.2\n",
        "theta_path = [np.array([theta0_initial, theta1_initial])]\n",
        "J_path = [cost(theta0_initial, theta1_initial)]\n",
        "\n",
        "for _ in range(N - 1):\n",
        "    theta0, theta1 = theta_path[-1]\n",
        "    pred = hypothesis(x, theta0, theta1)\n",
        "    grad0 = np.mean(pred - y)\n",
        "    grad1 = np.mean((pred - y) * x)\n",
        "    new_theta = np.array([theta0 - alpha * grad0, theta1 - alpha * grad1])\n",
        "    theta_path.append(new_theta)\n",
        "    J_path.append(cost(*new_theta))\n",
        "\n",
        "theta_path = np.array(theta_path)\n",
        "J_path = np.array(J_path)\n",
        "\n",
        "# Create animation frames\n",
        "frames = []\n",
        "x_line = np.linspace(-1, 1, 100)\n",
        "\n",
        "for i in range(1, len(theta_path) + 1):\n",
        "    t0, t1 = theta_path[i - 1]\n",
        "    y_pred = hypothesis(x_line, t0, t1)\n",
        "\n",
        "    frames.append(go.Frame(\n",
        "        name=f\"step{i}\",\n",
        "        data=[\n",
        "            go.Surface(x=theta0_vals, y=theta1_vals, z=J_vals,\n",
        "                       colorscale='Viridis', opacity=0.8, showscale=False),\n",
        "            go.Scatter3d(\n",
        "                x=theta_path[:i, 0],\n",
        "                y=theta_path[:i, 1],\n",
        "                z=J_path[:i],\n",
        "                mode='lines+markers',\n",
        "                line=dict(color='red', width=4),\n",
        "                marker=dict(size=5, color='red')\n",
        "            ),\n",
        "            go.Scatter(\n",
        "                x=x, y=y, mode='markers',\n",
        "                marker=dict(color='black', symbol='x', size=8),\n",
        "                xaxis='x2', yaxis='y2'\n",
        "            ),\n",
        "            go.Scatter(\n",
        "                x=x_line, y=y_pred,\n",
        "                mode='lines',\n",
        "                line=dict(color='red', width=3),\n",
        "                xaxis='x2', yaxis='y2'\n",
        "            )\n",
        "        ]\n",
        "    ))\n",
        "\n",
        "# Initial traces\n",
        "t0, t1 = theta_path[0]\n",
        "y_pred = hypothesis(x_line, t0, t1)\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2,\n",
        "                    specs=[[{\"type\": \"surface\"}, {\"type\": \"xy\"}]],\n",
        "                    column_widths=[0.6, 0.4],\n",
        "                    subplot_titles=(\"Cost Function Surface\", \"Line Fit During Gradient Descent\"))\n",
        "\n",
        "fig.add_trace(go.Surface(\n",
        "    x=theta0_vals, y=theta1_vals, z=J_vals,\n",
        "    colorscale='Viridis', opacity=0.8, showscale=False\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=[theta_path[0, 0]],\n",
        "    y=[theta_path[0, 1]],\n",
        "    z=[J_path[0]],\n",
        "    mode='lines+markers',\n",
        "    marker=dict(size=5, color='red'),\n",
        "    line=dict(color='red', width=4)\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=x, y=y, mode='markers',\n",
        "    marker=dict(color='black', symbol='x', size=8),\n",
        "    xaxis='x2', yaxis='y2'\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=x_line, y=hypothesis(x_line, *theta_path[0]),\n",
        "    mode='lines',\n",
        "    line=dict(color='red', width=3),\n",
        "    xaxis='x2', yaxis='y2'\n",
        "), row=1, col=2)\n",
        "\n",
        "# Animation buttons\n",
        "buttons = [\n",
        "    dict(label=\"‚ñ∂Ô∏è Play\",\n",
        "         method=\"animate\",\n",
        "         args=[None, {\n",
        "             \"frame\": {\"duration\": 1000, \"redraw\": True},\n",
        "             \"fromcurrent\": True,\n",
        "             \"transition\": {\"duration\": 200}\n",
        "         }]\n",
        "    ),\n",
        "    dict(label=\"‚è∏Ô∏è Pause\",\n",
        "         method=\"animate\",\n",
        "         args=[[None], {\n",
        "             \"mode\": \"immediate\",\n",
        "             \"frame\": {\"duration\": 0, \"redraw\": False},\n",
        "             \"transition\": {\"duration\": 0}\n",
        "         }]\n",
        "    ),\n",
        "    dict(label=\"üîÑ Restart\",\n",
        "         method=\"animate\",\n",
        "         args=[[f\"step1\"], {\n",
        "             \"frame\": {\"duration\": 0, \"redraw\": True},\n",
        "             \"mode\": \"immediate\",\n",
        "             \"transition\": {\"duration\": 0}\n",
        "         }]\n",
        "    )\n",
        "]\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(text=\"Gradient Descent: Surface + Line Fit\", x=0.5, y=0.95),\n",
        "    updatemenus=[dict(\n",
        "        type=\"buttons\",\n",
        "        direction=\"left\",\n",
        "        showactive=False,\n",
        "        buttons=buttons,\n",
        "        x=0.5,\n",
        "        y=1.02,\n",
        "        xanchor=\"center\",\n",
        "        yanchor=\"bottom\",\n",
        "        pad={\"r\": 10, \"t\": 10, \"b\": 25}\n",
        "    )],\n",
        "    margin=dict(t=120),\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    scene=dict(\n",
        "        xaxis_title='Œ∏‚ÇÄ',\n",
        "        yaxis_title='Œ∏‚ÇÅ',\n",
        "        zaxis_title='Cost J(Œ∏‚ÇÄ, Œ∏‚ÇÅ)'\n",
        "    ),\n",
        "    xaxis2=dict(title=\"x\", domain=[0.6, 1.0]),\n",
        "    yaxis2=dict(title=\"y\", domain=[0.0, 1.0])\n",
        ")\n",
        "\n",
        "fig.frames = frames\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nObserve how the red path descends the cost surface toward the minimum.\")\n",
        "print(\"The right plot shows how the fitted line improves at each iteration.\")"
      ],
      "metadata": {
        "id": "15awAAbIad0R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple stress-strain linear model\n",
        "\n",
        "Using linear regression, we can attempt to model the stress-strain relationship with a simple linear hypothesis. However, as we will see, a purely linear model has limitations for capturing the full complexity of material behavior, particularly the plastic deformation and necking regions."
      ],
      "metadata": {
        "id": "PnnYfR0TagCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stress-strain simple linear model\n",
        "\n",
        "# Prepare data\n",
        "X = df['strain'].values.reshape(-1, 1)\n",
        "y = df['stress'].values\n",
        "\n",
        "# Fit linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Generate points for the line of best fit\n",
        "x_fit = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "y_fit = model.predict(x_fit)\n",
        "\n",
        "# Calculate R¬≤\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y, model.predict(X))\n",
        "\n",
        "# Plot with line of best fit\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(df['strain'], df['stress'] / 1e6, color='orange', alpha=0.7, edgecolors='k', label='Data')\n",
        "plt.plot(x_fit, y_fit / 1e6, 'r-', linewidth=2,\n",
        "         label=f'Linear fit: œÉ = {model.coef_[0]/1e9:.2f}Œµ + {model.intercept_/1e6:.1f} (R¬≤={r2:.3f})')\n",
        "plt.plot(fit_curve['strain_grid'], fit_curve['sigma_eng_grid'] / 1e6,\n",
        "         'b--', linewidth=1.5, alpha=0.5, label='True behavior')\n",
        "\n",
        "plt.xlabel(\"Strain\")\n",
        "plt.ylabel(\"Stress (MPa)\")\n",
        "plt.title(\"Stress vs. Strain with Linear Fit\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nYoung's modulus from linear fit: {model.coef_[0]/1e9:.2f} GPa\")\n",
        "print(f\"True Young's modulus: 210 GPa\")\n",
        "print(f\"\\nThe linear model captures the overall trend but misses the nonlinear\")\n",
        "print(f\"plastic deformation behavior. This shows the importance of choosing\")\n",
        "print(f\"appropriate models based on physical understanding of the system.\")"
      ],
      "metadata": {
        "id": "aYAxJ7hmajJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R-squared (R¬≤) ‚Äî Feature importance analysis\n",
        "\n",
        "Now that we understand linear regression and R¬≤, let's apply this tool to learn which material parameters most strongly influence stress in our data set. Recall that R¬≤ tells us how well a given material parameter can predict the stress value. Higher R¬≤ values indicate stronger predictive power, with a maximum of R¬≤ = 1 representing perfect prediction.\n",
        "\n",
        "From materials science, we expect certain parameters to matter more than others. For example, carbon content significantly affects strength because carbon atoms impede dislocation motion in the steel crystal structure. The Hall-Petch relationship tells us that strength should be proportional to the inverse square root of grain size, so we will test that engineered feature as well."
      ],
      "metadata": {
        "id": "633JUYPfamdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create engineered features\n",
        "# Add physically meaningful features based on materials science\n",
        "\n",
        "# Material property features\n",
        "df['grain_size_inv_sqrt'] = 1.0 / np.sqrt(df['grain_size_um'])\n",
        "df['cross_section_area'] = df['specimen_thickness'] * df['specimen_width']\n",
        "df['temp_deviation'] = df['temperature_C'] - 23.0\n",
        "df['log_strain_rate'] = np.log10(df['strain_rate'])\n",
        "\n",
        "# Polynomial strain features for capturing nonlinear behavior\n",
        "# These are physically meaningful because stress-strain relationships\n",
        "# exhibit nonlinear work hardening and necking behavior\n",
        "df['strain_squared'] = df['strain'] ** 2\n",
        "df['strain_cubed'] = df['strain'] ** 3\n",
        "df['strain_fourth'] = df['strain'] ** 4\n",
        "\n",
        "print(\"Created physically meaningful features:\")\n",
        "print(\"\\nMaterial property features:\")\n",
        "print(\"- grain_size_inv_sqrt: Hall-Petch relationship (strength ‚àù 1/‚àöd)\")\n",
        "print(\"- cross_section_area: Geometric property\")\n",
        "print(\"- temp_deviation: Temperature effect relative to room temp\")\n",
        "print(\"- log_strain_rate: Strain rate sensitivity (logarithmic scale)\")\n",
        "print(\"\\nPolynomial strain features (for capturing nonlinear behavior):\")\n",
        "print(\"- strain_squared, strain_cubed, strain_fourth\")\n",
        "print(\"  These capture the nonlinear work-hardening and necking behavior\")\n",
        "print(\"  that occurs during plastic deformation.\")"
      ],
      "metadata": {
        "id": "EMX5wMgaaqkm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title R-squared scores for material parameters\n",
        "\n",
        "# Select meaningful features to test\n",
        "features_to_test = [\n",
        "    'strain',\n",
        "    'carbon_content',\n",
        "    'grain_size_um',\n",
        "    'grain_size_inv_sqrt',\n",
        "    'temperature_C',\n",
        "    'temp_deviation',\n",
        "    'strain_rate',\n",
        "    'log_strain_rate',\n",
        "    'surface_roughness',\n",
        "    'cross_section_area'\n",
        "]\n",
        "\n",
        "y = df[\"stress\"]\n",
        "r2_scores = {}\n",
        "\n",
        "for feature in features_to_test:\n",
        "    if feature in df.columns:\n",
        "        x_feat = df[[feature]]\n",
        "        model = LinearRegression()\n",
        "        model.fit(x_feat, y)\n",
        "        y_pred = model.predict(x_feat)\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        r2_scores[feature] = r2\n",
        "\n",
        "# Sort and print\n",
        "r2_sorted = dict(sorted(r2_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"R¬≤ score per feature (univariate linear regression):\\n\")\n",
        "for feat, r2 in r2_sorted.items():\n",
        "    print(f\"{feat:<25}: {r2:.6f}\")\n",
        "\n",
        "print(\"\\nPhysical interpretation:\")\n",
        "print(\"- Strain has highest R¬≤ because stress fundamentally depends on strain\")\n",
        "print(\"- Material properties show lower R¬≤ because they are constant for this\")\n",
        "print(\"  single specimen test (no variation across measurements)\")\n",
        "print(\"- In a multi-specimen study, carbon_content and grain_size would show\")\n",
        "print(\"  much stronger correlations with yield strength and UTS\")"
      ],
      "metadata": {
        "id": "15_r-cksati0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title R-squared bar chart\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(r2_sorted.keys(), r2_sorted.values(), color='steelblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"R¬≤ Score\")\n",
        "plt.title(\"Univariate R¬≤ Scores per Feature\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1beCpUcrawME",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining machine learning with materials science\n",
        "\n",
        "Throughout this notebook, we have explored how machine learning tools like PCA and linear regression can help analyze materials data. However, the most powerful approach combines these mathematical tools with domain knowledge from materials science.\n",
        "\n",
        "Key lessons:\n",
        "\n",
        "First, feature selection should be guided by physical principles. We know from metallurgy that carbon content, grain size, temperature, and strain rate all influence mechanical properties through well-understood mechanisms. Features like the Hall-Petch relationship (strength proportional to inverse square root of grain size) are not arbitrary - they emerge from the physics of dislocation motion in crystals.\n",
        "\n",
        "Second, model choice matters. A simple linear model can capture the elastic region of stress-strain behavior, but fails to represent plastic deformation and necking. However, by using polynomial features of strain (strain¬≤, strain¬≥, strain‚Å¥), we can capture the nonlinear work-hardening and necking behavior with linear regression. This approach is called polynomial regression, which is still linear regression because the coefficients appear linearly, even though the features themselves are nonlinear functions of strain.\n",
        "\n",
        "Third, machine learning helps us discover patterns in complex data, but domain knowledge helps us interpret whether those patterns are meaningful or spurious. The polynomial strain features are physically meaningful because they mathematically represent the nonlinear constitutive behavior of materials under plastic deformation.\n",
        "\n",
        "In the interactive demo below, you can select different material parameters and see how well linear regression captures their relationship to stress. Try selecting just strain first to see the linear fit, then add strain_squared and strain_cubed to capture the nonlinear behavior. Pay attention to how the R¬≤ value improves dramatically when you include polynomial terms."
      ],
      "metadata": {
        "id": "3FoS6mjyazCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive feature selection and modeling OLD VERSION\n",
        "\n",
        "# Available features for selection\n",
        "features = [\n",
        "    'strain',\n",
        "    'carbon_content',\n",
        "    'grain_size_um',\n",
        "    'grain_size_inv_sqrt',\n",
        "    'temperature_C',\n",
        "    'temp_deviation',\n",
        "    'strain_rate',\n",
        "    'log_strain_rate',\n",
        "    'surface_roughness'\n",
        "]\n",
        "\n",
        "y = df['stress']\n",
        "strain = df['strain']\n",
        "\n",
        "checkboxes = [widgets.Checkbox(value=(feat == 'strain'), description=feat) for feat in features]\n",
        "row1 = widgets.HBox(checkboxes[:5])\n",
        "row2 = widgets.HBox(checkboxes[5:])\n",
        "button = widgets.Button(description=\"Train & Plot\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        selected_features = [cb.description for cb in checkboxes if cb.value]\n",
        "        if not selected_features:\n",
        "            print(\"Please select at least one feature.\")\n",
        "            return\n",
        "\n",
        "        X = df[selected_features]\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        print(f\"Selected features: {selected_features}\")\n",
        "        print(f\"R¬≤ score: {r2:.6f}\")\n",
        "        print(f\"\\nCoefficients:\")\n",
        "        for f, c in zip(selected_features, model.coef_):\n",
        "            print(f\"  {f}: {c:.4e}\")\n",
        "        print(f\"  Intercept: {model.intercept_:.4e}\")\n",
        "\n",
        "        # Plot modeled vs. true stress-strain relationship\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.scatter(strain, y / 1e6, alpha=0.4, label='Measured stress', color='orange', s=20)\n",
        "        plt.scatter(strain, y_pred / 1e6, label='Predicted stress', color='blue', s=25)\n",
        "        plt.plot(fit_curve['strain_grid'], fit_curve['sigma_eng_grid'] / 1e6,\n",
        "                 'k--', linewidth=1.5, alpha=0.5, label='True behavior')\n",
        "        plt.xlabel(\"Strain\")\n",
        "        plt.ylabel(\"Stress (MPa)\")\n",
        "        plt.title(f\"Model Performance (R¬≤ = {r2:.4f})\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nTip: To reconstruct the nonlinear stress-strain curve:\")\n",
        "        print(\"1. Start with just 'strain' to see the linear fit (poor R¬≤)\")\n",
        "        print(\"2. Add 'strain_squared' and 'strain_cubed' to capture work hardening\")\n",
        "        print(\"3. Optionally add 'strain_fourth' for even better fit in necking region\")\n",
        "        print(\"\\nMaterial properties show weak correlation because they're constant\")\n",
        "        print(\"in this single-specimen test. In multi-specimen studies, they would\")\n",
        "        print(\"strongly correlate with yield strength and ultimate tensile strength.\")\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(widgets.VBox([row1, row2, button, output]))"
      ],
      "metadata": {
        "id": "2BPoolIkaySL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive feature selection and modeling NEW VERSION\n",
        "\n",
        "y = df['stress']\n",
        "strain = df['strain']\n",
        "\n",
        "# Create widget for polynomial degree selection\n",
        "degree_slider = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=50,\n",
        "    step=1,\n",
        "    description='Polynomial Degree:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "button = widgets.Button(description=\"Train & Plot\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        degree = degree_slider.value\n",
        "\n",
        "        # Generate polynomial features up to specified degree\n",
        "        X_data = {'strain': strain}\n",
        "        for d in range(2, degree + 1):\n",
        "            X_data[f'strain^{d}'] = strain ** d\n",
        "\n",
        "        X = np.column_stack(list(X_data.values()))\n",
        "        feature_names = list(X_data.keys())\n",
        "\n",
        "        # Fit model\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Polynomial Degree: {degree}\")\n",
        "        print(f\"Features: {feature_names}\")\n",
        "        print(f\"R¬≤ score: {r2:.6f}\")\n",
        "        print(f\"\\nCoefficients:\")\n",
        "        for f, c in zip(feature_names, model.coef_):\n",
        "            if f == 'strain':\n",
        "                print(f\"  {f:15s}: {c/1e9:.4f} GPa\")\n",
        "            else:\n",
        "                print(f\"  {f:15s}: {c:.4e}\")\n",
        "        print(f\"  {'Intercept':15s}: {model.intercept_/1e6:.4f} MPa\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Plot modeled vs. true stress-strain relationship\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(strain, y / 1e6, alpha=0.5, label='Measured stress',\n",
        "                   color='orange', s=30, edgecolors='k', linewidth=0.5)\n",
        "\n",
        "        # Sort for smooth prediction line\n",
        "        sort_idx = np.argsort(strain)\n",
        "        plt.plot(strain[sort_idx], y_pred[sort_idx] / 1e6,\n",
        "                'r-', linewidth=2.5, label=f'Polynomial fit (degree {degree})', alpha=0.8)\n",
        "\n",
        "        plt.plot(fit_curve['strain_grid'], fit_curve['sigma_eng_grid'] / 1e6,\n",
        "                 'b--', linewidth=1.5, alpha=0.6, label='True behavior')\n",
        "\n",
        "        plt.xlabel(\"Strain\", fontsize=12)\n",
        "        plt.ylabel(\"Stress (MPa)\", fontsize=12)\n",
        "        plt.title(f\"Model Performance (R¬≤ = {r2:.4f})\", fontsize=14)\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(widgets.VBox([degree_slider, button, output]))"
      ],
      "metadata": {
        "id": "LWyhgnkL72Fp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression: Predicting Material Failure\n",
        "\n",
        "After analyzing the stress-strain behavior, you want to predict when the material will fail under complex loading conditions. In real engineering applications, materials experience multi-axial stress states - stress acting in multiple directions simultaneously. Your goal is to develop a model that can predict whether a material will yield (permanently deform) under these conditions.\n",
        "\n",
        "## Understanding Material Failure\n",
        "\n",
        "When engineers design structures, they need to know when materials will fail. For ductile materials like steel, failure typically means yielding - the onset of permanent plastic deformation. Under complex loading with multiple stress components, we need a way to combine these stresses into a single value that we can compare to the material's yield strength.\n",
        "\n",
        "The most widely used criterion for ductile materials is the **von Mises yield criterion**, which states that yielding occurs when the distortion energy in the material reaches a critical value. This is based on the physical observation that materials fail due to shear stress, not hydrostatic (uniform) pressure.\n",
        "\n",
        "## Principal Stresses and the von Mises Criterion\n",
        "\n",
        "In any stressed material, we can identify three perpendicular directions called principal directions, along which the stresses are purely normal (no shear). These principal stresses are denoted $\\sigma_1$, $\\sigma_2$, and $\\sigma_3$. For a 2D stress state (like a thin plate), $\\sigma_3$ = 0.\n",
        "\n",
        "The von Mises equivalent stress combines these principal stresses into a single value:\n",
        "\n",
        "$$\\sigma_{VM} = \\sqrt{\\sigma_1^2 - \\sigma_1\\sigma_2 + \\sigma_2^2}$$\n",
        "\n",
        "This equation has deep physical meaning - it represents the stress that would cause the same distortion energy as the actual multi-axial stress state. When $\\sigma_{VM}$ exceeds the material's yield strength $\\sigma_y$ ($\\sigma_{VM} > \\sigma_y$), the material yields.\n",
        "\n",
        "## Data: Multi-axial Loading Tests\n",
        "\n",
        "You've conducted experiments on steel specimens under various combinations of biaxial loading (stress in two perpendicular directions). Each test applied different principal stresses and recorded whether the specimen yielded."
      ],
      "metadata": {
        "id": "MPbEcJs1bLt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate physically realistic failure data\n",
        "\n",
        "def generate_realistic_failure_data(N=300, seed=42):\n",
        "    \"\"\"\n",
        "    Generate realistic 2D stress state failure data using the von Mises criterion.\n",
        "\n",
        "    This function creates synthetic experimental data for biaxial loading tests.\n",
        "    The failure prediction is based on von Mises equivalent stress compared to\n",
        "    material yield strength, with realistic experimental scatter included.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    N : int\n",
        "        Number of test samples to generate\n",
        "    seed : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame with columns:\n",
        "        - sigma1_MPa: Principal stress in first direction (MPa)\n",
        "        - sigma2_MPa: Principal stress in second direction (MPa)\n",
        "        - sigma_vm_MPa: von Mises equivalent stress (MPa)\n",
        "        - sigma_y_MPa: Yield strength for each specimen (MPa)\n",
        "        - failure: Binary indicator (1=yielded, 0=elastic)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Principal stresses in Pascals (converted to MPa for output)\n",
        "    # Generate stress states covering a range from zero to well beyond yield\n",
        "    sigma1 = rng.uniform(0, 450e6, size=N)\n",
        "    sigma2 = rng.uniform(0, 450e6, size=N)\n",
        "\n",
        "    # Yield strength with realistic specimen-to-specimen scatter\n",
        "    # Mean: 350 MPa (typical for mild steel), StdDev: 30 MPa\n",
        "    sigma_y = rng.normal(350e6, 30e6, size=N)\n",
        "\n",
        "    # Calculate von Mises equivalent stress\n",
        "    # This formula represents the distortion energy in the material\n",
        "    sigma_vm = np.sqrt(sigma1**2 - sigma1 * sigma2 + sigma2**2)\n",
        "\n",
        "    # Probabilistic failure based on how close œÉ_vm is to œÉ_y\n",
        "    # Using sigmoid function to model gradual transition near yield point\n",
        "    # The 0.10 factor controls the \"fuzziness\" of the boundary (10% of yield strength)\n",
        "    failure_probability = 1.0 / (1.0 + np.exp(-(sigma_vm - sigma_y) / (0.10 * sigma_y)))\n",
        "    failure = rng.binomial(1, failure_probability)\n",
        "\n",
        "    # Return data in MPa for easier interpretation\n",
        "    return pd.DataFrame({\n",
        "        'sigma1_MPa': sigma1 / 1e6,\n",
        "        'sigma2_MPa': sigma2 / 1e6,\n",
        "        'sigma_vm_MPa': sigma_vm / 1e6,\n",
        "        'sigma_y_MPa': sigma_y / 1e6,\n",
        "        'failure': failure\n",
        "    })\n",
        "\n",
        "# Generate the failure data\n",
        "df_failure = generate_realistic_failure_data(N=300, seed=42)\n",
        "\n",
        "# Extract key variables for later use\n",
        "sigma1 = df_failure['sigma1_MPa'].values\n",
        "sigma2 = df_failure['sigma2_MPa'].values\n",
        "failure = df_failure['failure'].values\n",
        "yield_strength = 350  # Mean yield strength in MPa\n",
        "\n",
        "# Save the data\n",
        "df_failure.to_csv('biaxial_failure_data.csv', index=False)\n",
        "\n",
        "print(f\"Generated {len(df_failure)} biaxial loading tests\")\n",
        "print(f\"Mean yield strength: {df_failure['sigma_y_MPa'].mean():.1f} MPa\")\n",
        "print(f\"Yield strength std dev: {df_failure['sigma_y_MPa'].std():.1f} MPa\")\n",
        "print(f\"Number of failures: {failure.sum()}/{len(df_failure)}\")\n",
        "print(f\"Failure rate: {failure.sum()/len(df_failure)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "6HHL-XIcbPol",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot below shows your experimental data. Blue points remained elastic (no permanent deformation, class=0), while red points yielded (class=1). The dashed line shows the theoretical von Mises yield surface - the boundary where yielding should occur according to the theory."
      ],
      "metadata": {
        "id": "qXTk0LOLbR7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize the failure data with theoretical yield surface\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Plot experimental points\n",
        "colors = ['blue' if f == 0 else 'red' for f in failure]\n",
        "scatter = ax.scatter(sigma1, sigma2, c=colors, alpha=0.6, s=40,\n",
        "                    edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot theoretical von Mises yield surface (only first quadrant portion)\n",
        "# For 2D stress state, the von Mises criterion forms an ellipse\n",
        "t = np.linspace(0, 2*np.pi, 200)\n",
        "vm_sigma1 = yield_strength * (np.cos(t) + np.sin(t)/np.sqrt(3))\n",
        "vm_sigma2 = yield_strength * (2*np.sin(t)/np.sqrt(3))\n",
        "\n",
        "ax.plot(vm_sigma1, vm_sigma2, 'k--', linewidth=2,\n",
        "        label=f'von Mises surface (œÉ_y = {yield_strength} MPa)')\n",
        "\n",
        "# Add labels and formatting\n",
        "ax.set_xlabel('Principal Stress œÉ‚ÇÅ (MPa)', fontsize=12)\n",
        "ax.set_ylabel('Principal Stress œÉ‚ÇÇ (MPa)', fontsize=12)\n",
        "ax.set_title('Material Failure Under Biaxial Loading\\n(von Mises Criterion)', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Focus on first quadrant where data is located\n",
        "ax.set_xlim(0, 500)\n",
        "ax.set_ylim(0, 500)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "# Add legend for points\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='blue', alpha=0.6, label='Elastic (no failure)'),\n",
        "                  Patch(facecolor='red', alpha=0.6, label='Yielded (failure)'),\n",
        "                  plt.Line2D([0], [0], color='k', linestyle='--',\n",
        "                            label=f'Theoretical yield surface')]\n",
        "ax.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNote: Some scatter around the theoretical boundary is expected due to:\")\n",
        "print(\"- Specimen-to-specimen variation in yield strength\")\n",
        "print(\"- Experimental measurement uncertainty\")\n",
        "print(\"- Material inhomogeneities\")\n",
        "print(\"\\nWe focus on the first quadrant (positive stresses in both directions)\")\n",
        "print(\"which represents biaxial tension - the most common loading scenario.\")"
      ],
      "metadata": {
        "id": "6g_jZbsRbUT2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the von Mises Criterion as the boundary condition\n",
        "\n",
        "A mathematically useful reformulation of the von Mises Criterion is:\n",
        "\n",
        "$$\\sqrt{\\sigma_1^2 - \\sigma_1\\sigma_2 + \\sigma_2^2} - \\sigma_{y} = 0$$\n",
        "\n",
        "This formulation defines a decision boundary that will return negative values when inside the yield surface and positive values when outside. This will be useful when solving the logistic regression problem.\n",
        "\n",
        "Use this interactive tool to explore how different stress states relate to the von Mises equivalent stress. Move the point to see whether it would cause yielding."
      ],
      "metadata": {
        "id": "k8cupP3BbWsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive von Mises stress calculator with color intensity\n",
        "\n",
        "def plot_stress_state(sigma1=100.0, sigma2=100.0):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Calculate von Mises stress\n",
        "    sigma_vm = np.sqrt(sigma1**2 - sigma1*sigma2 + sigma2**2)\n",
        "\n",
        "    # Calculate how far from yield surface (normalized)\n",
        "    distance_ratio = sigma_vm / yield_strength\n",
        "\n",
        "    # Calculate decision boundary value\n",
        "    boundary_value = sigma_vm - yield_strength\n",
        "\n",
        "    # Color intensity based on distance from yield surface\n",
        "    # Similar to the simple decision boundary demo!\n",
        "    if distance_ratio > 1:  # Yielded\n",
        "        # Red intensity increases with distance beyond yield\n",
        "        intensity = min((distance_ratio - 1) * 2, 1.0)  # Scale for visibility\n",
        "        point_color = (1.0, 1.0 - intensity, 1.0 - intensity)\n",
        "        status = \"YIELDED (Failure)\"\n",
        "    else:  # Elastic\n",
        "        # Blue intensity increases with safety (further from yield)\n",
        "        intensity = 1.0 - distance_ratio\n",
        "        point_color = (1.0 - intensity, 1.0 - intensity, 1.0)\n",
        "        status = \"ELASTIC (Safe)\"\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Left plot: Stress state in principal stress space\n",
        "    # Plot theoretical yield surface\n",
        "    t = np.linspace(0, 2*np.pi, 200)\n",
        "    vm_sigma1 = yield_strength * (np.cos(t) + np.sin(t)/np.sqrt(3))\n",
        "    vm_sigma2 = yield_strength * (2*np.sin(t)/np.sqrt(3))\n",
        "    ax1.plot(vm_sigma1, vm_sigma2, 'k--', linewidth=2, alpha=0.5,\n",
        "             label='von Mises yield surface')\n",
        "\n",
        "    # Plot current point with color intensity\n",
        "    ax1.scatter(sigma1, sigma2, s=200, c=[point_color], edgecolor='black',\n",
        "               linewidth=2, zorder=5)\n",
        "\n",
        "    # Add stress state vectors\n",
        "    ax1.arrow(0, 0, sigma1, 0, head_width=10, head_length=15,\n",
        "             fc='gray', ec='gray', alpha=0.5)\n",
        "    ax1.arrow(0, 0, 0, sigma2, head_width=10, head_length=15,\n",
        "             fc='gray', ec='gray', alpha=0.5)\n",
        "\n",
        "    # Focus on first quadrant where data is located\n",
        "    ax1.set_xlim(-20, 500)\n",
        "    ax1.set_ylim(-20, 500)\n",
        "    ax1.set_xlabel('œÉ‚ÇÅ (MPa)', fontsize=12)\n",
        "    ax1.set_ylabel('œÉ‚ÇÇ (MPa)', fontsize=12)\n",
        "    ax1.set_title('Principal Stress Space', fontsize=14)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_aspect('equal')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Right plot: Stress information\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # Create background color for info box matching point intensity\n",
        "    if distance_ratio > 1:\n",
        "        bg_color = 'red'\n",
        "    else:\n",
        "        bg_color = 'blue'\n",
        "\n",
        "    info_text = f\"\"\"\n",
        "    Current Stress State:\n",
        "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    œÉ‚ÇÅ = {sigma1:.1f} MPa\n",
        "    œÉ‚ÇÇ = {sigma2:.1f} MPa\n",
        "\n",
        "    von Mises Equivalent Stress:\n",
        "    œÉ_VM = ‚àö(œÉ‚ÇÅ¬≤ - œÉ‚ÇÅœÉ‚ÇÇ + œÉ‚ÇÇ¬≤)\n",
        "    œÉ_VM = {sigma_vm:.1f} MPa\n",
        "\n",
        "    Yield Strength: œÉ_y = {yield_strength} MPa\n",
        "\n",
        "    Decision Boundary Value:\n",
        "    œÉ_VM - œÉ_y = {boundary_value:.1f} MPa\n",
        "    {\"(> 0: outside boundary)\" if boundary_value > 0 else \"(< 0: inside boundary)\" if boundary_value < 0 else \"(= 0: on boundary)\"}\n",
        "\n",
        "    Distance Ratio: {distance_ratio:.2f}\n",
        "    Safety Factor: {yield_strength/sigma_vm if sigma_vm > 0 else np.inf:.2f}\n",
        "\n",
        "    Status: {status}\n",
        "    \"\"\"\n",
        "\n",
        "    # Move info box up to make room for status messages\n",
        "    ax2.text(0.1, 0.65, info_text, fontsize=12, family='monospace',\n",
        "            verticalalignment='center',\n",
        "            bbox=dict(boxstyle='round', facecolor=bg_color, alpha=0.2))\n",
        "\n",
        "    # Add status messages with more space at the bottom\n",
        "    posx, posy = 0.35, 0.06\n",
        "    if distance_ratio > 1:\n",
        "        ax2.text(posx, posy+0.07, '‚ö†Ô∏è Material has yielded!', fontsize=16,\n",
        "                color='red', weight='bold', ha='center')\n",
        "        ax2.text(posx, posy, 'Color intensity shows severity\\n(darker = further beyond yield)',\n",
        "                fontsize=10, style='italic', ha='center', color='darkred')\n",
        "    else:\n",
        "        ax2.text(posx, posy+0.07, '‚úì Material remains elastic', fontsize=16,\n",
        "                color='green', weight='bold', ha='center')\n",
        "        ax2.text(posx, posy, 'Color intensity shows safety margin\\n(darker = closer to yield)',\n",
        "                fontsize=10, style='italic', ha='center', color='darkgreen')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive sliders\n",
        "sigma1_slider = widgets.FloatSlider(\n",
        "    value=100.0, min=0.0, max=450.0, step=10.0,\n",
        "    description='œÉ‚ÇÅ (MPa):', continuous_update=False\n",
        ")\n",
        "sigma2_slider = widgets.FloatSlider(\n",
        "    value=100.0, min=0.0, max=450.0, step=10.0,\n",
        "    description='œÉ‚ÇÇ (MPa):', continuous_update=False\n",
        ")\n",
        "\n",
        "widgets.interact(plot_stress_state, sigma1=sigma1_slider, sigma2=sigma2_slider)"
      ],
      "metadata": {
        "id": "-Ah9my_2bY_o",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem definition\n",
        "\n",
        "In reality we do not move the points with respect to the decision boundary. The decision boundary is something we want to derive from data! Thus, the points stay fixed while the ellipse moves across data to find the best fit.\n",
        "\n",
        "In the demo below you can move around the ellipse yourself and try to get as low cost as possible. What this cost is will be explained later. Pay attention to the relation between the cost and how well the ellipse fits the data."
      ],
      "metadata": {
        "id": "suUdnu0rbeYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Manual Optimization: Move and Rotate the Decision Boundary to Minimize Cost\n",
        "\n",
        "def plot_manual_optimization(scale=1.0, shift_x=0.0, shift_y=0.0, rotation=0.0):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Use the existing failure data\n",
        "    X = df_failure[['sigma1_MPa', 'sigma2_MPa']].values\n",
        "    y_true = df_failure['failure'].values\n",
        "\n",
        "    # Calculate the \"yield strength\" for this ellipse position/size\n",
        "    adjusted_yield = yield_strength * scale\n",
        "\n",
        "    # Generate the base ellipse (decision boundary)\n",
        "    t = np.linspace(0, 2*np.pi, 200)\n",
        "    base_sigma1 = adjusted_yield * (np.cos(t) + np.sin(t)/np.sqrt(3))\n",
        "    base_sigma2 = adjusted_yield * (2*np.sin(t)/np.sqrt(3))\n",
        "\n",
        "    # Apply rotation (convert degrees to radians)\n",
        "    theta = np.radians(rotation)\n",
        "    cos_theta = np.cos(theta)\n",
        "    sin_theta = np.sin(theta)\n",
        "\n",
        "    # Rotate the ellipse points\n",
        "    ellipse_sigma1 = cos_theta * base_sigma1 - sin_theta * base_sigma2 + shift_x\n",
        "    ellipse_sigma2 = sin_theta * base_sigma1 + cos_theta * base_sigma2 + shift_y\n",
        "\n",
        "    # Calculate predictions based on adjusted and rotated boundary\n",
        "    y_pred_proba = []\n",
        "    for i in range(len(X)):\n",
        "        # Translate point to ellipse-centered coordinates\n",
        "        s1_translated = X[i, 0] - shift_x\n",
        "        s2_translated = X[i, 1] - shift_y\n",
        "\n",
        "        # Rotate point back to align with standard von Mises orientation\n",
        "        s1_rotated = cos_theta * s1_translated + sin_theta * s2_translated\n",
        "        s2_rotated = -sin_theta * s1_translated + cos_theta * s2_translated\n",
        "\n",
        "        # Calculate von Mises stress in rotated frame\n",
        "        sigma_vm = np.sqrt(s1_rotated**2 - s1_rotated*s2_rotated + s2_rotated**2)\n",
        "\n",
        "        # Convert to probability using sigmoid-like function\n",
        "        ratio = sigma_vm / adjusted_yield\n",
        "        # Use a steep sigmoid to approximate hard classification\n",
        "        prob = 1 / (1 + np.exp(-10*(ratio - 1)))\n",
        "        y_pred_proba.append(prob)\n",
        "\n",
        "    y_pred_proba = np.array(y_pred_proba)\n",
        "\n",
        "    # Calculate logistic loss (cross-entropy)\n",
        "    epsilon = 1e-7  # Small value to avoid log(0)\n",
        "    cost = -np.mean(y_true * np.log(y_pred_proba + epsilon) +\n",
        "                    (1 - y_true) * np.log(1 - y_pred_proba + epsilon))\n",
        "\n",
        "    # Calculate accuracy for reference\n",
        "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
        "    accuracy = np.mean(y_pred_binary == y_true)\n",
        "\n",
        "    # Count misclassifications\n",
        "    false_positives = np.sum((y_pred_binary == 1) & (y_true == 0))\n",
        "    false_negatives = np.sum((y_pred_binary == 0) & (y_true == 1))\n",
        "\n",
        "    # Left plot: Data points and adjustable boundary\n",
        "    colors = ['blue' if f == 0 else 'red' for f in y_true]\n",
        "    ax1.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6, s=40,\n",
        "                edgecolors='black', linewidth=0.5, label='Data points')\n",
        "\n",
        "    # Plot user's adjusted boundary\n",
        "    ax1.plot(ellipse_sigma1, ellipse_sigma2, 'orange', linewidth=3,\n",
        "             label=f'Your boundary')\n",
        "\n",
        "    # Add center point of user's ellipse\n",
        "    ax1.plot(shift_x, shift_y, 'go', markersize=8, label='Ellipse center')\n",
        "\n",
        "    ax1.set_xlabel('œÉ‚ÇÅ (MPa)')\n",
        "    ax1.set_ylabel('œÉ‚ÇÇ (MPa)')\n",
        "    ax1.set_title('Adjust the Decision Boundary')\n",
        "    # Zoom in to focus on data (first quadrant) but show enough context\n",
        "    # to see the ellipse structure\n",
        "    ax1.set_xlim(-100, 550)\n",
        "    ax1.set_ylim(-100, 550)\n",
        "    ax1.set_aspect('equal')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend(loc='upper left', fontsize=9)\n",
        "\n",
        "    # Right plot: Cost visualization\n",
        "    ax2.clear()\n",
        "\n",
        "    # Create a bar chart showing cost components\n",
        "    categories = ['Current\\nCost', 'Best\\nPossible']\n",
        "\n",
        "    # Calculate best possible cost (with optimal boundary)\n",
        "    y_best_proba = []\n",
        "    for i in range(len(X)):\n",
        "        sigma_vm = np.sqrt(X[i,0]**2 - X[i,0]*X[i,1] + X[i,1]**2)\n",
        "        ratio = sigma_vm / yield_strength\n",
        "        prob = 1 / (1 + np.exp(-10*(ratio - 1)))\n",
        "        y_best_proba.append(prob)\n",
        "    y_best_proba = np.array(y_best_proba)\n",
        "    best_cost = -np.mean(y_true * np.log(y_best_proba + epsilon) +\n",
        "                        (1 - y_true) * np.log(1 - y_best_proba + epsilon))\n",
        "\n",
        "    costs = [cost, best_cost]\n",
        "    colors_bar = ['orange', 'green']\n",
        "    bars = ax2.bar(categories, costs, color=colors_bar, alpha=0.7)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, costs):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "    ax2.set_ylabel('Cross-Entropy Cost', fontsize=12)\n",
        "    ax2.set_title(f'Cost Function (Lower is Better)', fontsize=14)\n",
        "    ax2.set_ylim(0, max(cost * 1.2, 1.0))\n",
        "    ax2.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Add metrics text\n",
        "    metrics_text = f\"\"\"\n",
        "    Accuracy: {accuracy:.1%}\n",
        "    False Positives: {false_positives}\n",
        "    False Negatives: {false_negatives}\n",
        "\n",
        "    Parameters:\n",
        "    Scale: {scale:.2f}\n",
        "    Shift: ({shift_x:.0f}, {shift_y:.0f})\n",
        "    Rotation: {rotation:.1f}¬∞\n",
        "    \"\"\"\n",
        "    ax2.text(0.98, 0.5, metrics_text, transform=ax2.transAxes,\n",
        "             fontsize=10, family='monospace', ha='right',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
        "\n",
        "    # Add feedback\n",
        "    cost_diff = cost - best_cost\n",
        "    if cost_diff < 0.01:\n",
        "        feedback = \"Excellent! You've found the optimal boundary!\"\n",
        "        feedback_color = 'green'\n",
        "    elif cost_diff < 0.05:\n",
        "        feedback = \"Very close! Fine-tune the parameters a bit more.\"\n",
        "        feedback_color = 'darkgreen'\n",
        "    elif cost_diff < 0.15:\n",
        "        feedback = \"Good progress! Keep adjusting to reduce the cost.\"\n",
        "        feedback_color = 'orange'\n",
        "    else:\n",
        "        feedback = \"Keep trying! Adjust scale, position, and rotation.\"\n",
        "        feedback_color = 'red'\n",
        "\n",
        "    ax2.text(0.5, -0.15, feedback, transform=ax2.transAxes,\n",
        "             ha='center', fontsize=12, color=feedback_color, weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed feedback\n",
        "    print(f\"Current Cost: {cost:.4f}\")\n",
        "    print(f\"Optimal Cost: {best_cost:.4f}\")\n",
        "    print(f\"Difference: {cost_diff:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.1%}\")\n",
        "    print(\"\\nHint: The von Mises ellipse has a specific size, position, and orientation.\")\n",
        "    print(\"Watch how the cost changes as you adjust each parameter!\")\n",
        "\n",
        "# Create sliders\n",
        "scale_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.5, max=1.5, step=0.02,\n",
        "    description='Scale:', continuous_update=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "shift_x_slider = widgets.FloatSlider(\n",
        "    value=-60.0, min=-100, max=100, step=5,\n",
        "    description='Shift X (œÉ‚ÇÅ):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "shift_y_slider = widgets.FloatSlider(\n",
        "    value=-60.0, min=-100, max=100, step=5,\n",
        "    description='Shift Y (œÉ‚ÇÇ):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "rotation_slider = widgets.FloatSlider(\n",
        "    value=90.0, min=-180, max=180, step=2,\n",
        "    description='Rotation (¬∞):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Display instructions and sliders\n",
        "print(\"=\" * 60)\n",
        "print(\"MANUAL OPTIMIZATION CHALLENGE - 4D Parameter Space\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Your task: Find the optimal decision boundary by minimizing the cost!\")\n",
        "print(\"\\nYou now have 4 parameters to optimize:\")\n",
        "print(\"‚Ä¢ Scale: Changes the size of the ellipse\")\n",
        "print(\"‚Ä¢ Shift X/Y: Moves the ellipse center in stress space\")\n",
        "print(\"‚Ä¢ Rotation: Rotates the ellipse orientation\")\n",
        "print(\"\\nThis mimics what gradient descent does automatically -\")\n",
        "print(\"searching through parameter space to minimize the cost function!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "widgets.interact(plot_manual_optimization,\n",
        "                scale=scale_slider,\n",
        "                shift_x=shift_x_slider,\n",
        "                shift_y=shift_y_slider,\n",
        "                rotation=rotation_slider)"
      ],
      "metadata": {
        "id": "nYVgamqabgdg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression for Failure Prediction\n",
        "\n",
        "Now we'll use logistic regression to learn the failure boundary from the experimental data. Unlike linear regression which predicts continuous values, logistic regression predicts probabilities - perfect for our binary classification problem (failed/not failed).\n",
        "\n",
        "### The Physical Hypothesis\n",
        "\n",
        "For the von Mises criterion, we know the theoretical relationship involves œÉ‚ÇÅ¬≤ - œÉ‚ÇÅœÉ‚ÇÇ + œÉ‚ÇÇ¬≤. Let's see if logistic regression can discover this relationship from the experimental data alone.\n",
        "\n",
        "Our hypothesis for logistic regression is:\n",
        "$$P(\\text{failure}) = h_\\theta(x^{(i)}) =  \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1\\sigma_1 + \\theta_2\\sigma_2 + \\theta_3\\sigma_1^2 + \\theta_4\\sigma_1\\sigma_2 + \\theta_5\\sigma_2^2)}}$$\n",
        "\n",
        "This allows the model to learn polynomial relationships between the stresses. The definition of the probability is called a sigmoid function and it is the standard way the probabilities are given by the logistic regression method."
      ],
      "metadata": {
        "id": "vPaXEf8MbjRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sigmoid plot with classification visualization\n",
        "\n",
        "# Define sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Generate input range for the curve\n",
        "z = np.linspace(-10, 10, 300)\n",
        "y = sigmoid(z)\n",
        "\n",
        "# Generate sample points to show as colored dots\n",
        "z_points = np.linspace(-8, 8, 17)  # Sample points along z-axis\n",
        "y_points = sigmoid(z_points)\n",
        "\n",
        "# Calculate colors for each point based on distance from z=0\n",
        "colors = []\n",
        "for z_val in z_points:\n",
        "    sig_val = sigmoid(z_val)\n",
        "\n",
        "    if abs(z_val) < 0.1:  # Near zero - white\n",
        "        colors.append((1.0, 1.0, 1.0))\n",
        "    elif z_val > 0:  # Positive z - red (failure)\n",
        "        # Intensity increases with distance from zero\n",
        "        intensity = min(abs(z_val) / 8, 1.0)\n",
        "        colors.append((1.0, 1.0 - intensity, 1.0 - intensity))\n",
        "    else:  # Negative z - blue (safe)\n",
        "        # Intensity increases with distance from zero\n",
        "        intensity = min(abs(z_val) / 8, 1.0)\n",
        "        colors.append((1.0 - intensity, 1.0 - intensity, 1.0))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the sigmoid curve\n",
        "ax.plot(z, y, color='black', linewidth=2, zorder=1, alpha=0.7)\n",
        "\n",
        "# Plot colored dots\n",
        "for i, (z_val, y_val, color) in enumerate(zip(z_points, y_points, colors)):\n",
        "    ax.scatter(z_val, y_val, s=150, c=[color], edgecolor='black',\n",
        "              linewidth=1.5, zorder=3)\n",
        "\n",
        "# Add special emphasis on z=0 point\n",
        "ax.scatter(0, 0.5, s=200, c='white', edgecolor='black',\n",
        "          linewidth=2, zorder=4, marker='D')\n",
        "\n",
        "# Reference lines\n",
        "ax.axvline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax.axhline(0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "\n",
        "# Add colored regions to show classification zones\n",
        "ax.axvspan(-10, 0, alpha=0.1, color='blue', label='Safe region (z<0)')\n",
        "ax.axvspan(0, 10, alpha=0.1, color='red', label='Failure region (z>0)')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('z = decision boundary value', fontsize=12)\n",
        "ax.set_ylabel(r'$\\sigma(z)$ = P(failure)', fontsize=12)\n",
        "ax.set_title('Sigmoid Function: Converting Distance to Probability\\n'\n",
        "            'Color intensity shows confidence in classification', fontsize=14)\n",
        "\n",
        "# Add text annotations\n",
        "ax.text(-5, 0.15, 'High confidence\\nSAFE', ha='center', fontsize=10,\n",
        "        color='darkblue', weight='bold')\n",
        "ax.text(5, 0.85, 'High confidence\\nFAILURE', ha='center', fontsize=10,\n",
        "        color='darkred', weight='bold')\n",
        "ax.text(0.2, 0.52, 'Decision\\nboundary', ha='left', fontsize=9,\n",
        "        color='black', style='italic')\n",
        "\n",
        "# Custom legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    plt.Line2D([0], [0], color='black', linewidth=2, label=r'$\\sigma(z) = \\frac{1}{1 + e^{-z}}$'),\n",
        "    Patch(facecolor='red', alpha=0.3, label='P(failure) > 0.5'),\n",
        "    Patch(facecolor='blue', alpha=0.3, label='P(failure) < 0.5'),\n",
        "    plt.scatter([], [], c='white', edgecolor='black', s=100, label='z = 0 (threshold)')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper left')\n",
        "\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(-10, 10)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Add explanatory text below\n",
        "print(\"Key Insights:\")\n",
        "print(\"‚Ä¢ z < 0: Material is safe (blue) - more negative = more confident\")\n",
        "print(\"‚Ä¢ z = 0: Decision boundary (white) - P(failure) = 0.5\")\n",
        "print(\"‚Ä¢ z > 0: Material fails (red) - more positive = more confident\")\n",
        "print(\"\\nFor von Mises: z ‚àù (œÉ_VM¬≤ - œÉ_y¬≤)\")\n",
        "print(\"The sigmoid smoothly converts this continuous value into a probability!\")"
      ],
      "metadata": {
        "id": "xbpvM4h8bulZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "\n",
        "Just like in linear regression, we need to find a way to measure what makes for a good or bad decision boundary. In the logistic regression case, the cost function is defined as:\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ - y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right] $$\n",
        "\n",
        "Here once again, the cost function is shaped by the data as we sum over every data point. Since $y$ can be either 0 or 1, only one of the logarithms contributes to the cost function. Now, why is $J(\\theta)$ defined this way? Let's take a look at the log components plotted below. If $y=1$ (left plot) then the cost for this data point will be close to zero if the sigmoid is indeed close to 1. And sigmoid will approach a value close to one if the decision boundary $h_{\\theta}(x_0, x_1)$ returns a positive value. In this situation the decision boundary predicts correctly the value $y=1$. If the opposite happens, that is, $h_{\\theta}(x_0, x_1)$ is negative then the sigmoid value will be negative. In turn, the cost increases sharply. Similar reasoning follows for the case when $y=0$."
      ],
      "metadata": {
        "id": "StkdvUiybyhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cost function components plot\n",
        "\n",
        "# Define sigmoid and related functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.linspace(-10, 10, 500)\n",
        "sig = sigmoid(x)\n",
        "log_sig = -np.log(sig)\n",
        "log_one_minus_sig = -np.log(1 - sig)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Subplot 1: sigmoid and log(sigmoid)\n",
        "axes[0].plot(x, sig, label=r'$\\sigma(x)$', color='blue')\n",
        "axes[0].plot(x, log_sig, label=r'$-\\log(\\sigma(x))$', color='red')\n",
        "axes[0].set_title(\"Sigmoid and log(sigmoid)\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Subplot 2: sigmoid and log(1 - sigmoid)\n",
        "axes[1].plot(x, sig, label=r'$\\sigma(x)$', color='blue')\n",
        "axes[1].plot(x, log_one_minus_sig, label=r'$-\\log(1 - \\sigma(x))$', color='orange')\n",
        "axes[1].set_title(\"Sigmoid and log(1 - sigmoid)\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jce7YEESb2Ou",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update step\n",
        "\n",
        "Once again, the update is defined using the gradient of the cost function $J(\\theta)$. In our 2D example:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "\\theta_0 - \\alpha \\dfrac{\\partial J(\\theta)}{\\partial \\theta_0} & \\text{for } i = 0 \\\\\n",
        "\\theta_1 - \\alpha \\dfrac{\\partial J(\\theta)}{\\partial \\theta_1} & \\text{for } i = 1 \\\\\n",
        "\\theta_2 - \\alpha \\dfrac{\\partial J(\\theta)}{\\partial \\theta_2} & \\text{for } i = 2 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where the partial derivative is defined as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}\n",
        "$$\n",
        "\n",
        "\n",
        "Notice that this formula, like in the case of linear regression, **is of the form of a quadratic surface with a single global minimum**. In fact, **it is the same formula despite different cost function definitions**. Now we are ready to train our logistic regression model!"
      ],
      "metadata": {
        "id": "o1B0hMzGb7Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train logistic regression model\n",
        "\n",
        "# Prepare features - include polynomial terms to capture von Mises relationship\n",
        "X = df_failure[['sigma1_MPa', 'sigma2_MPa']].values\n",
        "y = df_failure['failure'].values\n",
        "\n",
        "# Create polynomial features up to degree 2\n",
        "# This gives us: [1, œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤]\n",
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train logistic regression\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_poly, y)\n",
        "\n",
        "# Print the learned coefficients\n",
        "feature_names = poly.get_feature_names_out(['œÉ‚ÇÅ', 'œÉ‚ÇÇ'])\n",
        "print(\"Learned coefficients:\")\n",
        "print(\"‚îÄ\" * 40)\n",
        "for name, coef in zip(feature_names, log_reg.coef_[0]):\n",
        "    print(f\"{name:8s}: {coef:8.4f}\")\n",
        "print(f\"Intercept: {log_reg.intercept_[0]:8.4f}\")\n",
        "\n",
        "# Model accuracy\n",
        "y_pred = log_reg.predict(X_poly)\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "print(f\"\\nModel accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Note about the coefficients\n",
        "print(\"\\nPhysical interpretation:\")\n",
        "print(\"The model should learn that œÉ‚ÇÅ¬≤ and œÉ‚ÇÇ¬≤ have positive coefficients\")\n",
        "print(\"while œÉ‚ÇÅ*œÉ‚ÇÇ has a negative coefficient, matching von Mises theory.\")"
      ],
      "metadata": {
        "id": "Ki63lnr1b6JI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Learned vs Theoretical Failure Boundary\n",
        "\n",
        "Let's see how well the logistic regression model learned the failure criterion compared to the theoretical von Mises surface."
      ],
      "metadata": {
        "id": "zSXJfjLacAx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize learned decision boundary vs theoretical (with threshold slider)\n",
        "\n",
        "def plot_boundary_with_threshold(threshold=0.5):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Create grid for decision boundary - expanded range to prevent clipping\n",
        "    xx, yy = np.meshgrid(np.linspace(-50, 550, 250),\n",
        "                         np.linspace(-50, 550, 250))\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_poly = poly.transform(grid_points)\n",
        "    Z = log_reg.predict_proba(grid_poly)[:, 1].reshape(xx.shape)\n",
        "\n",
        "    # Left plot: Learned boundary\n",
        "    colors = ['blue' if f == 0 else 'red' for f in failure]\n",
        "    ax1.scatter(sigma1, sigma2, c=colors, alpha=0.4, s=30, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    # Add theoretical von Mises FIRST (so it appears below)\n",
        "    t = np.linspace(0, 2*np.pi, 200)\n",
        "    vm_sigma1 = yield_strength * (np.cos(t) + np.sin(t)/np.sqrt(3))\n",
        "    vm_sigma2 = yield_strength * (2*np.sin(t)/np.sqrt(3))\n",
        "    ax1.plot(vm_sigma1, vm_sigma2, 'k--', linewidth=2, alpha=0.7, label='Theoretical (von Mises)')\n",
        "\n",
        "    # Draw contour at the selected threshold AFTER (so it appears on top)\n",
        "    contour = ax1.contour(xx, yy, Z, levels=[threshold], colors='orange', linewidths=3)\n",
        "    ax1.clabel(contour, inline=True, fmt=f'P={threshold:.2f}')\n",
        "\n",
        "    ax1.set_xlabel('œÉ‚ÇÅ (MPa)')\n",
        "    ax1.set_ylabel('œÉ‚ÇÇ (MPa)')\n",
        "    ax1.set_title(f'Learned Decision Boundary (Threshold = {threshold:.2f})')\n",
        "    ax1.set_xlim(0, 500)\n",
        "    ax1.set_ylim(0, 500)\n",
        "    ax1.set_aspect('equal')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Update legend based on whether reference line is shown\n",
        "    legend_labels = [f'Learned boundary (P={threshold:.2f})', 'Theoretical von Mises']\n",
        "    ax1.legend(legend_labels)\n",
        "\n",
        "    # Right plot: Probability heatmap\n",
        "    im = ax2.contourf(xx, yy, Z, levels=20, cmap='RdBu_r', alpha=0.8)\n",
        "\n",
        "    # Add theoretical von Mises FIRST (lower z-order)\n",
        "    ax2.plot(vm_sigma1, vm_sigma2, 'k--', linewidth=2, alpha=0.7, zorder=2)\n",
        "\n",
        "    # Draw contour at the selected threshold AFTER (higher z-order)\n",
        "    threshold_contour = ax2.contour(xx, yy, Z, levels=[threshold], colors='yellow', linewidths=3, zorder=3)\n",
        "\n",
        "    # Also show 0.5 contour for reference if threshold is different\n",
        "    if abs(threshold - 0.5) > 0.01:\n",
        "        reference_contour = ax2.contour(xx, yy, Z, levels=[0.5], colors='white', linewidths=1,\n",
        "                                       linestyles='--', alpha=0.5, zorder=3)\n",
        "\n",
        "    # Create legend handles list\n",
        "    legend_handles = []\n",
        "    legend_labels = []\n",
        "\n",
        "    # Add current threshold line to legend\n",
        "    legend_handles.append(plt.Line2D([0], [0], color='yellow', linewidth=3))\n",
        "    legend_labels.append(f'Current threshold (P={threshold:.2f})')\n",
        "\n",
        "    # Also show 0.5 contour for reference if threshold is different\n",
        "    if abs(threshold - 0.5) > 0.01:\n",
        "        legend_handles.append(plt.Line2D([0], [0], color='white', linewidth=1, linestyle='--'))\n",
        "        legend_labels.append('Standard threshold (P=0.50)')\n",
        "\n",
        "    # Add theoretical von Mises\n",
        "    legend_handles.append(plt.Line2D([0], [0], color='black', linewidth=2, linestyle='--'))\n",
        "    legend_labels.append('Theoretical von Mises')\n",
        "\n",
        "    # Add legend to right plot\n",
        "    ax2.legend(legend_handles, legend_labels, loc='upper right')\n",
        "\n",
        "    plt.colorbar(im, ax=ax2, label='Probability of Failure')\n",
        "\n",
        "    ax2.set_xlabel('œÉ‚ÇÅ (MPa)')\n",
        "    ax2.set_ylabel('œÉ‚ÇÇ (MPa)')\n",
        "    ax2.set_title('Failure Probability Map')\n",
        "    # Expanded limits to prevent clipping at higher thresholds\n",
        "    ax2.set_xlim(0, 550)\n",
        "    ax2.set_ylim(0, 550)\n",
        "    ax2.set_aspect('equal')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print information about the threshold effect\n",
        "    if threshold < 0.5:\n",
        "        print(f\"Threshold = {threshold:.2f}: More conservative (larger safety region)\")\n",
        "        print(\"The model predicts failure for fewer cases - reducing false negatives but increasing false positives\")\n",
        "    elif threshold > 0.5:\n",
        "        print(f\"Threshold = {threshold:.2f}: Less conservative (smaller safety region)\")\n",
        "        print(\"The model predicts failure for more cases - reducing false positives but increasing false negatives\")\n",
        "    else:\n",
        "        print(f\"Threshold = {threshold:.2f}: Standard classification boundary\")\n",
        "        print(\"This is the default threshold where P(failure) = P(no failure)\")\n",
        "\n",
        "# Create threshold slider\n",
        "threshold_slider = widgets.FloatSlider(\n",
        "    value=0.5,\n",
        "    min=0.1,\n",
        "    max=0.9,\n",
        "    step=0.05,\n",
        "    description='Threshold:',\n",
        "    continuous_update=False,\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "# Display the interactive plot\n",
        "print(\"Adjust the threshold to see how it affects the decision boundary:\")\n",
        "print(\"‚Ä¢ Lower threshold (< 0.5): More conservative, predicts failure earlier\")\n",
        "print(\"‚Ä¢ Higher threshold (> 0.5): Less conservative, allows higher stresses before predicting failure\\n\")\n",
        "\n",
        "widgets.interact(plot_boundary_with_threshold, threshold=threshold_slider)"
      ],
      "metadata": {
        "id": "F4Lf_9RCcEhV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Insights and Engineering Applications\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "Through this exploration of linear and logistic regression applied to materials science, several key insights emerge.\n",
        "\n",
        "First, the logistic regression successfully learned an approximation of the von Mises failure criterion from experimental data alone. The learned boundary closely matches the theoretical von Mises ellipse, validating both our experimental data and the machine learning approach.\n",
        "\n",
        "Second, the model discovered the quadratic relationship. By including polynomial features, the model could learn that failure depends on squared stress terms and their interaction, matching the von Mises formula.\n",
        "\n",
        "Third, physics-informed features improve learning. For both linear and logistic regression, including features based on our understanding of material mechanics helped the models learn the true relationships.\n",
        "\n",
        "Fourth, the probability output from logistic regression provides valuable uncertainty information. Unlike a hard threshold, logistic regression gives us failure probabilities, which is useful for safety factors in design.\n",
        "\n",
        "### Engineering Applications\n",
        "\n",
        "These approaches are used extensively in real engineering contexts. Design optimization allows engineers to quickly evaluate if a design will fail under complex loading without running expensive physical tests.\n",
        "\n",
        "Safety analysis benefits from the probability output, which helps engineers apply appropriate safety factors based on the consequences of failure. Material testing becomes more efficient because we can reduce the number of expensive tests by learning from existing data.\n",
        "\n",
        "### The Key Lesson\n",
        "\n",
        "The most important takeaway is that machine learning is most powerful when combined with domain knowledge. We used our understanding of material mechanics to choose appropriate features, select suitable model architectures, interpret results in the context of physical laws, and validate predictions against theoretical expectations.\n",
        "\n",
        "This synergy between data-driven methods and physical understanding represents the future of engineering analysis. Machine learning provides the tools to extract patterns from complex data, while materials science provides the framework to ensure those patterns are physically meaningful and generalizable."
      ],
      "metadata": {
        "id": "key_insights"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5jzhMtqeVyW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}