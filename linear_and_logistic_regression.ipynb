{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artkula/ML-retreat-tekmek-2025/blob/main/linear_and_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbd2534",
      "metadata": {
        "id": "2cbd2534"
      },
      "source": [
        "# Machine Learning for Materials Science: Linear and Logistic Regression\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This notebook demonstrates machine learning applications in materials engineering through two case studies:\n",
        "\n",
        "**Part 1: Linear Regression for Stress-Strain Modeling**\n",
        "1. Feature importance analysis using R¬≤ for understanding material property influence\n",
        "2. Polynomial regression for capturing elastic-plastic-necking transitions\n",
        "3. Regularization techniques (L2/Ridge) to prevent overfitting\n",
        "4. Bias-variance tradeoff in model complexity selection\n",
        "\n",
        "**Part 2: Binary Classification for Multi-Axial Yield Prediction**\n",
        "1. Von Mises yield criterion as a physics-based decision boundary\n",
        "2. Logistic regression with polynomial features for nonlinear classification\n",
        "3. Decision threshold optimization for engineering risk management\n",
        "4. Model validation against theoretical predictions\n",
        "\n",
        "**Key Principle**: Machine learning performance improves when informed by physical understanding of the material system.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "**Part 1: Stress-Strain Modeling via Polynomial Regression**\n",
        "- 1.1 Constitutive Model and Data Generation\n",
        "- 1.2 Feature Importance Analysis (R¬≤)\n",
        "- 1.3 Linear Regression Framework\n",
        "- 1.4 Underfitting: Linear Model Limitations\n",
        "- 1.5 Regularization Effects\n",
        "- 1.6 Polynomial Features and Overfitting\n",
        "- 1.7 Interactive Feature Engineering\n",
        "\n",
        "**Part 2: Multi-Axial Yield Prediction via Logistic Regression**\n",
        "- 2.1 Multi-Axial Loading and Failure Criteria\n",
        "- 2.2 Von Mises Yield Criterion (Plane Stress)\n",
        "- 2.3 Experimental Data Generation\n",
        "- 2.4 Interactive Yield Surface Exploration\n",
        "- 2.5 Decision Boundary Optimization\n",
        "- 2.6 Logistic Regression Theory\n",
        "- 2.7 Model Training and Validation\n",
        "- 2.8 Learned vs. Theoretical Boundaries\n",
        "- 2.9 Threshold Selection for Risk Management\n",
        "\n",
        "**Conclusions**: Key Insights and Engineering Applications\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c52b0c",
      "metadata": {
        "id": "94c52b0c"
      },
      "source": [
        "## Setup: Import Required Libraries\n",
        "\n",
        "Before we begin, we need to import all the necessary Python libraries for data manipulation, visualization, machine learning, and interactive widgets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ef2b09a0",
      "metadata": {
        "id": "ef2b09a0"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "# Optional seaborn\n",
        "try:\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    pass  # Seaborn optional, matplotlib defaults are fine\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    log_loss\n",
        ")\n",
        "\n",
        "# Interactive widgets\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import interact\n",
        "\n",
        "# Plotting configuration\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "try:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "except NameError:\n",
        "    pass  # sns is not imported if seaborn was not installed\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Font configuration for Greek symbols and subscripts\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"DejaVu Sans\",\n",
        "    \"mathtext.fontset\": \"dejavusans\",\n",
        "    \"axes.unicode_minus\": False\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d16f7c",
      "metadata": {
        "id": "04d16f7c"
      },
      "source": [
        "## Notation & Units\n",
        "\n",
        "### Physical Quantities\n",
        "\n",
        "| Symbol | Quantity | Units (code) | Units (display) |\n",
        "|--------|----------|--------------|-----------------|\n",
        "| $\\sigma$ | Stress | Pa | MPa, GPa |\n",
        "| $\\varepsilon$ | Strain | dimensionless |  -  |\n",
        "| $E$ | Young's modulus | Pa | GPa |\n",
        "| $\\sigma_y$ | Yield strength | Pa | MPa |\n",
        "| $K$, $n$ | Hardening parameters | Pa, dimensionless | MPa,  -  |\n",
        "| $C$ | Carbon content | wt% | wt% |\n",
        "| $d$ | Grain size | Œºm | Œºm |\n",
        "\n",
        "### Machine Learning Notation\n",
        "\n",
        "| Symbol | Quantity |\n",
        "|--------|----------|\n",
        "| $\\lambda$ | Ridge regularization parameter |\n",
        "| $R^2$ | Coefficient of determination |\n",
        "| RMSE | Root mean squared error |\n",
        "\n",
        "### Units Policy\n",
        "\n",
        "**Internal representation:** All stress values stored in **Pa** (Pascals).\n",
        "but converted to **MPa** (√∑10‚Å∂) or **GPa** (√∑10‚Åπ) in plots and table to avoid numerical precision issues while keeping printouts readable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b819f7",
      "metadata": {
        "id": "e2b819f7"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: Polynomial Regression for Stress-Strain Modeling\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Constitutive Model and Data Generation\n",
        "\n",
        "### Engineering Context\n",
        "\n",
        "Tensile testing provides the fundamental stress-strain relationship œÉ(Œµ) for material characterization. This relationship exhibits three distinct regimes:\n",
        "\n",
        "**1. Elastic Region (Œµ < Œµ_y)**\n",
        "$$\\sigma = E \\varepsilon$$\n",
        "\n",
        "where E is Young's modulus and Œµ_y is the yield strain.\n",
        "\n",
        "**2. Plastic Region (Œµ_y ‚â§ Œµ ‚â§ Œµ_u)**\n",
        "\n",
        "Hollomon Power-Law Hardening (Teaching Form):\n",
        "\n",
        "**Note**: The classical Hollomon form is œÉ = K Œµ‚Çö‚Åø. This notebook uses œÉ = œÉ·µß + K(Œµ - Œµ·µß)‚Åø for pedagogical clarity.\n",
        "\n",
        "Hollomon power-law hardening:\n",
        "$$\\sigma = \\sigma_y + K(\\varepsilon - \\varepsilon_y)^n$$\n",
        "\n",
        "where:\n",
        "- œÉ_y = yield strength\n",
        "- K = strength coefficient\n",
        "- n = strain hardening exponent (‚âà 0.2-0.3 for low-carbon steel)\n",
        "- Œµ_u = uniform elongation (necking initiation)\n",
        "\n",
        "**3. Necking Region (Œµ > Œµ_u)**\n",
        "\n",
        "Engineering stress decreases as deformation localizes:\n",
        "$$\\sigma = \\sigma_u \\exp[-\\alpha(\\varepsilon - \\varepsilon_u)]$$\n",
        "\n",
        "where œÉ_u is the ultimate tensile strength and Œ± controls the softening rate.\n",
        "\n",
        "> **‚ö†Ô∏è Pedagogical Model Warning**\n",
        ">\n",
        "> These coefficients are **scaled for teaching purposes only**. Do not use these values for design, certification, or engineering calculations. Consult materials handbooks and standards for real applications.\n",
        "\n",
        "### Machine Learning Challenge\n",
        "\n",
        "**Goal**: Learn œÉ(Œµ) from experimental data without explicit knowledge of the piecewise model.\n",
        "\n",
        "**Key Questions**:\n",
        "1. Can polynomial regression capture this nonlinear, multi-regime behavior?\n",
        "2. What polynomial degree balances accuracy and generalization?\n",
        "3. How does regularization affect the learned relationship?\n",
        "4. Which material parameters (if any) improve predictions from a single test?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m5Z-py-iQuxz",
      "metadata": {
        "id": "m5Z-py-iQuxz"
      },
      "source": [
        "Configure material properties and plotting parameters for all experiments in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rh8iu7nzkjp",
      "metadata": {
        "id": "rh8iu7nzkjp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION: Adjust these parameters to explore different scenarios\n",
        "# =============================================================================\n",
        "\n",
        "# Material properties (low-carbon steel)\n",
        "E = 210e9          # Young's modulus (Pa)\n",
        "sigma_y_base = 250e6  # Base yield strength (Pa)\n",
        "sigma_u = 400e6    # Ultimate tensile strength (Pa)\n",
        "\n",
        "# Hardening parameters\n",
        "K = 600e9          # Strength coefficient (Pa)\n",
        "n = 0.15           # Strain hardening exponent (dimensionless)\n",
        "\n",
        "# Necking parameters\n",
        "alpha = 20.0       # Exponential softening rate (dimensionless)\n",
        "\n",
        "# Material variability ranges\n",
        "carbon_range = (0.05, 0.30)     # Carbon content range (wt%) ‚Äì broadened to introduce more variability\n",
        "grain_size_range = (10, 100)    # Grain size range (micrometers)\n",
        "\n",
        "# Hall-Petch effect (toy coefficients for pedagogy)\n",
        "base_strength = 200e6      # Pa\n",
        "hall_petch_coeff = 50e6    # Pa ‚Äì larger toy coefficient to increase grain‚Äësize effect\n",
        "carbon_effect_coeff = 2e9  # Pa ‚Äì strong carbon dependence for demonstration\n",
        "\n",
        "# Data generation\n",
        "n_specimens = 30   # Number of synthetic specimens\n",
        "n_points = 100     # Points per stress-strain curve\n",
        "noise_level = 5e6  # Stress measurement noise (Pa)\n",
        "\n",
        "# Train/test splitting\n",
        "test_size = 0.25   # Fraction of data for testing\n",
        "random_seed = 42   # Random seed for reproducibility\n",
        "\n",
        "# Part 2: Failure criterion\n",
        "sigma_y_failure = 250e6  # Yield strength for classification (Pa)\n",
        "n_samples_failure = 300  # Number of synthetic stress states\n",
        "\n",
        "# Unit conversion helpers (prevent Pa/MPa errors)\n",
        "def MPa(x):\n",
        "    \"\"\"Convert Pa to MPa\"\"\"\n",
        "    return np.asarray(x) / 1e6\n",
        "\n",
        "def GPa(x):\n",
        "    \"\"\"Convert Pa to GPa\"\"\"\n",
        "    return np.asarray(x) / 1e9\n",
        "\n",
        "# Helper function for polynomial + ridge pipeline (used throughout Part 1)\n",
        "def make_poly_ridge_pipeline(degree: int, alpha: float):\n",
        "    \"\"\"\n",
        "    Create a pipeline for polynomial regression with ridge regularization.\n",
        "\n",
        "    Parameters:\n",
        "    - degree: Polynomial degree for feature expansion\n",
        "    - alpha: Ridge regularization strength (L2 penalty)\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline with polynomial features, standard scaling, and ridge regression\n",
        "    \"\"\"\n",
        "    return Pipeline([\n",
        "        (\"poly\",   PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"ridge\",  Ridge(alpha=float(alpha), fit_intercept=True, max_iter=10000))\n",
        "    ])\n",
        "\n",
        "print(\"Configuration loaded successfully.\")\n",
        "print(f\"  Material: Low-carbon steel (E={E/1e9:.0f} GPa, œÉ_y={sigma_y_base/1e6:.0f} MPa)\")\n",
        "print(f\"  Specimens: {n_specimens} with {n_points} points each\")\n",
        "print(f\"  Random seed: {random_seed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b749aca",
      "metadata": {
        "id": "7b749aca"
      },
      "source": [
        "### Data Generation and Visualization\n",
        "\n",
        "We will generate realistic stress-strain data that mimics actual tensile testing of low-carbon steel. The data includes elastic, plastic, and necking regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ff5e3d",
      "metadata": {
        "id": "b6ff5e3d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Material properties for low-carbon steel (using E from config)\n",
        "sigma_y = 250e6 + np.random.normal(0, 20e6)   # Yield strength (Pa)\n",
        "sigma_u = 400e6 + np.random.normal(0, 30e6) # Ultimate tensile strength (Pa)\n",
        "\n",
        "# Override config values for Part 1 synthetic data generation\n",
        "# Work hardening (Hollomon) parameters for teaching curve\n",
        "K = sigma_u * 1.5\n",
        "n = 0.25\n",
        "epsilon_y = sigma_y / E\n",
        "epsilon_u = 0.15 + np.random.normal(0, 0.01)  # uniform elongation (~ ultimate point)\n",
        "\n",
        "# --- Strain sampling with extra density around yield and no duplicated endpoints ---\n",
        "n_points = 150\n",
        "n_elastic = int(n_points * 0.20)\n",
        "n_knee    = int(n_points * 0.30)\n",
        "n_plastic = int(n_points * 0.30)\n",
        "n_neck    = n_points - (n_elastic + n_knee + n_plastic)\n",
        "\n",
        "knee_hi = min(epsilon_u, epsilon_y * 1.12)\n",
        "\n",
        "epsilon_elastic = np.linspace(0.0, epsilon_y, n_elastic, endpoint=False)\n",
        "epsilon_knee    = np.linspace(epsilon_y, knee_hi,  n_knee, endpoint=False)\n",
        "epsilon_plastic = np.linspace(knee_hi,  epsilon_u, n_plastic, endpoint=False)\n",
        "epsilon_neck    = np.linspace(epsilon_u, epsilon_u * 1.20, n_neck, endpoint=True)\n",
        "\n",
        "strain_data = np.concatenate([epsilon_elastic, epsilon_knee, epsilon_plastic, epsilon_neck])\n",
        "\n",
        "# --- Clean piecewise truth œÉ_true(Œµ) ---\n",
        "sigma_true = np.empty_like(strain_data)\n",
        "\n",
        "mask_elastic = strain_data < epsilon_y\n",
        "mask_plastic = (strain_data >= epsilon_y) & (strain_data <= epsilon_u)\n",
        "mask_neck    = strain_data > epsilon_u\n",
        "\n",
        "sigma_true[mask_elastic] = E * strain_data[mask_elastic]\n",
        "eps_p = np.maximum(strain_data[mask_plastic] - epsilon_y, 0.0)\n",
        "sigma_true[mask_plastic] = sigma_y + K * eps_p**n\n",
        "\n",
        "sigma_at_u = sigma_y + K * np.maximum(epsilon_u - epsilon_y, 0.0)**n\n",
        "sigma_true[mask_neck] = sigma_at_u * np.exp(-alpha * (strain_data[mask_neck] - epsilon_u))\n",
        "\n",
        "# --- Add measurement noise ONLY to measured stress ---\n",
        "noise = np.random.normal(0, 20e6, size=strain_data.size)\n",
        "stress_noisy = sigma_true + noise\n",
        "\n",
        "# DataFrame with properties and test conditions\n",
        "df = pd.DataFrame({\n",
        "    'strain': strain_data,\n",
        "    'stress': stress_noisy,        # measured (noisy)\n",
        "    'stress_true': sigma_true,     # clean reference\n",
        "    'yield_strength': sigma_y,\n",
        "    'uts': sigma_u,\n",
        "    'epsilon_y': epsilon_y,\n",
        "    'epsilon_u': epsilon_u,\n",
        "    'carbon_content': 0.18,\n",
        "    'grain_size_um': 40.0,\n",
        "    'temperature_C': 23.0,\n",
        "    'strain_rate': 1e-4,\n",
        "    'surface_roughness': np.random.uniform(0.2, 0.6, strain_data.size),\n",
        "    'specimen_thickness': 5.0,\n",
        "    'specimen_width': 10.0,\n",
        "    'heat_treatment': 'as_rolled'\n",
        "})\n",
        "\n",
        "# Smooth reference curve for plotting\n",
        "strain_grid = np.linspace(0, strain_data.max(), 600)\n",
        "sigma_ref = np.empty_like(strain_grid)\n",
        "\n",
        "m_e = strain_grid < epsilon_y\n",
        "m_p = (strain_grid >= epsilon_y) & (strain_grid <= epsilon_u)\n",
        "m_n = strain_grid > epsilon_u\n",
        "\n",
        "sigma_ref[m_e] = E * strain_grid[m_e]\n",
        "sigma_ref[m_p] = sigma_y + K * np.maximum(strain_grid[m_p] - epsilon_y, 0.0)**n\n",
        "sigma_ref[m_n] = sigma_at_u * np.exp(-alpha * (strain_grid[m_n] - epsilon_u))\n",
        "\n",
        "fit_curve = pd.DataFrame({'strain_grid': strain_grid, 'sigma_eng_grid': sigma_ref})\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['strain'], df['stress'] / 1e6, color='orange', alpha=0.6, edgecolors='k', s=26, label='Measured data')\n",
        "plt.plot(fit_curve['strain_grid'], fit_curve['sigma_eng_grid'] / 1e6, 'b-', linewidth=2, alpha=0.85, label='True behavior')\n",
        "\n",
        "plt.axvline(epsilon_y, color='g', linestyle='--', alpha=0.7, label=f'Yield (Œµ_y={epsilon_y:.4f})')\n",
        "plt.axvline(epsilon_u, color='r', linestyle='--', alpha=0.7, label=f'Ultimate (Œµ_u={epsilon_u:.3f})')\n",
        "plt.axhline(sigma_y / 1e6, color='g', linestyle=':', alpha=0.5)\n",
        "plt.axhline(sigma_u / 1e6, color='r', linestyle=':', alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Strain (dimensionless)\")\n",
        "plt.ylabel(\"Engineering Stress (MPa)\")\n",
        "plt.title(\"Stress vs Strain: clean truth vs noisy observations\")\n",
        "plt.legend(fontsize=9, loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c631ca",
      "metadata": {
        "id": "64c631ca"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.2 Feature Importance Analysis Using R¬≤\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Before building complex models, quantify which parameters influence stress. The coefficient of determination (R¬≤) measures the proportion of variance explained by a single feature:\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$$\n",
        "\n",
        "where:\n",
        "- $y_i$ = measured stress (target variable)\n",
        "- $\\hat{y}_i$ = stress predicted by linear regression using one feature\n",
        "- $\\bar{y}$ = mean of measured stress values\n",
        "\n",
        "**Interpretation**:\n",
        "- R¬≤ = 1: Feature perfectly predicts target\n",
        "- R¬≤ = 0: Feature no better than mean prediction\n",
        "- R¬≤ < 0: Feature worse than mean (poor model)\n",
        "\n",
        "### Expected Results\n",
        "\n",
        "For a **single tensile test** (constant material properties):\n",
        "- **Strain**: Should dominate (R¬≤ ‚âà 0.5-0.7) - stress fundamentally depends on strain\n",
        "- **Material parameters** (C content, grain size): R¬≤ ‚âà 0 - constant within one test\n",
        "- **Geometry** (thickness, width): R¬≤ ‚âà 0 - normalized stress independent of dimensions\n",
        "\n",
        "To assess material parameter importance, we would need data from **multiple specimens** with varying composition and microstructure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eca25b4",
      "metadata": {
        "id": "4eca25b4"
      },
      "outputs": [],
      "source": [
        "# Generate example data for R¬≤ demonstration\n",
        "x_demo = np.linspace(0, 10, 10)\n",
        "true_slope = 2.0\n",
        "true_intercept = 1.0\n",
        "y_demo = true_slope * x_demo + true_intercept + np.random.normal(scale=2.0, size=len(x_demo))\n",
        "y_mean_demo = np.mean(y_demo)\n",
        "\n",
        "# Rotation center\n",
        "x_mid = (x_demo.min() + x_demo.max()) / 2\n",
        "y_mid = true_slope * x_mid + true_intercept\n",
        "\n",
        "def plot_rotated_line(slope):\n",
        "    \"\"\"Interactive function to show how R¬≤ changes with model slope.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Rotated model prediction\n",
        "    y_model = slope * (x_demo - x_mid) + y_mid\n",
        "\n",
        "    # Compute R¬≤ score\n",
        "    r2 = r2_score(y_demo, y_model)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(x_demo, y_demo, label='Data', color='orange', s=80, zorder=3, edgecolors='k')\n",
        "    plt.plot(x_demo, y_model, label=f'Model (Slope = {slope:.2f})', color='blue', linewidth=2)\n",
        "\n",
        "    # Vertical residuals\n",
        "    for xi, yi_data, yi_model in zip(x_demo, y_demo, y_model):\n",
        "        plt.plot([xi, xi], [yi_data, yi_model], 'r--', linewidth=1, alpha=0.6)\n",
        "\n",
        "    # Mean line\n",
        "    plt.axhline(y_mean_demo, color='green', linestyle=':', linewidth=2,\n",
        "                label=f'Mean = {y_mean_demo:.2f}')\n",
        "\n",
        "    # Residual indicator\n",
        "    plt.plot([], [], 'r--', linewidth=1, label='Residuals')\n",
        "\n",
        "    plt.xlabel(\"Input Feature\", fontsize=12)\n",
        "    plt.ylabel(\"Output\", fontsize=12)\n",
        "    plt.title(f\"R¬≤ Score: {r2:.3f}\\n(Higher is better, max = 1.0)\",\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10, loc='best')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if r2 > 0.9:\n",
        "        print(\"‚úÖ Excellent fit! R¬≤ > 0.9 indicates the model explains >90% of variance.\")\n",
        "    elif r2 > 0.7:\n",
        "        print(\"üü¢ Good fit! R¬≤ > 0.7 indicates the model captures most of the pattern.\")\n",
        "    elif r2 > 0.3:\n",
        "        print(\"üü° Moderate fit. R¬≤ > 0.3 indicates some predictive power.\")\n",
        "    else:\n",
        "        print(\"üî¥ Poor fit. R¬≤ < 0.3 indicates little predictive power.\")\n",
        "\n",
        "# Interactive slider\n",
        "slope_slider = widgets.FloatSlider(\n",
        "    value=true_slope, min=-5.0, max=5.0, step=0.1,\n",
        "    description='Slope:', continuous_update=False, style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "print(\"üìä Interactive R¬≤ Demonstration\")\n",
        "print(\"   Use the slider to adjust the model slope and observe how R¬≤ changes.\")\n",
        "print(\"   Try to maximize R¬≤ by finding the best fit!\")\n",
        "print()\n",
        "interact(plot_rotated_line, slope=slope_slider)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b377ad",
      "metadata": {
        "id": "b7b377ad"
      },
      "source": [
        "### Computing R¬≤ for Material Parameters\n",
        "\n",
        "Now we will calculate R¬≤ for each material parameter to see which ones best predict stress. From materials science, we know that certain parameters should matter more than others:\n",
        "\n",
        "- **Strain**: Should strongly correlate with stress (fundamental stress-strain relationship)\n",
        "- **Carbon content**: Affects strength through solid solution strengthening\n",
        "- **Grain size**: Hall-Petch relationship (strength ‚àù 1/‚àöd)\n",
        "- **Temperature**: Higher temperatures reduce strength\n",
        "- **Specimen geometry**: Should have minimal effect on normalized stress\n",
        "\n",
        "We will also create some engineered features based on physical principles:\n",
        "- **grain_size_inv_sqrt**: 1/‚àö(grain_size) for Hall-Petch relationship\n",
        "- **temp_deviation**: Temperature deviation from room temperature\n",
        "\n",
        "---\n",
        "\n",
        "**R¬≤ with Different Units (quick note)**\n",
        "\n",
        "- **R¬≤ is unitless and scale-invariant.** Rescaling a feature (e.g., converting strain from fraction to %) **does not change R¬≤**.\n",
        "\n",
        "- **Why this matters:** You can **compare univariate R¬≤ across features with different units**. R¬≤ measures the fraction of variance explained, not a per-unit effect, so strain's R¬≤ as fraction (0.05) equals its R¬≤ as percentage (5%).\n",
        "\n",
        "- **If \"influence per typical change\" is needed:** Standardize inputs to z-scores (mean=0, std=1) and compare **standardized coefficients**.\n",
        "  - **Why z-scores?** They put all features on the same scale (1 unit = 1 standard deviation), making coefficients directly comparable.\n",
        "  - **Standardized coefficients** show the change in outcome per 1-SD change in predictor, revealing which variable has more impact for its typical variation range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de0bd72",
      "metadata": {
        "id": "0de0bd72"
      },
      "outputs": [],
      "source": [
        "# Create engineered features based on physical principles\n",
        "df['grain_size_inv_sqrt'] = 1 / np.sqrt(df['grain_size_um'])\n",
        "df['temp_deviation'] = abs(df['temperature_C'] - 23.0)\n",
        "\n",
        "# Select features to evaluate\n",
        "features_to_test = [\n",
        "    'strain',\n",
        "    'carbon_content',\n",
        "    'grain_size_um',\n",
        "    'grain_size_inv_sqrt',\n",
        "    'temperature_C',\n",
        "    'temp_deviation',\n",
        "    'strain_rate',\n",
        "    'surface_roughness',\n",
        "    'specimen_thickness',\n",
        "    'specimen_width'\n",
        "]\n",
        "\n",
        "# Calculate R¬≤ for each feature\n",
        "r2_scores = {}\n",
        "for feature in features_to_test:\n",
        "    if feature in df.columns:\n",
        "        X = df[[feature]].values.reshape(-1, 1)\n",
        "        y = df['stress'].values\n",
        "\n",
        "        # Fit simple linear regression\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "        # Calculate R¬≤\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        r2_scores[feature] = r2\n",
        "\n",
        "# Sort by R¬≤ score\n",
        "r2_df = pd.DataFrame(list(r2_scores.items()), columns=['Feature', 'R¬≤ Score'])\n",
        "r2_df = r2_df.sort_values('R¬≤ Score', ascending=False)\n",
        "\n",
        "print(\"üìä R¬≤ Scores for Material Parameters\")\n",
        "print(\"=\" * 50)\n",
        "print(r2_df.to_string(index=False))\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - strain shows dominant R¬≤ because stress fundamentally depends on strain\")\n",
        "print(\"   - Material properties (carbon, grain size) have much lower R¬≤\")\n",
        "print(\"   - This is expected: during a SINGLE test, material properties are constant\")\n",
        "print(\"   - To see their effect, we would need data from MULTIPLE specimens\")\n",
        "print(\"   - Specimen geometry and surface roughness have minimal predictive power\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e8a4a5",
      "metadata": {
        "id": "d7e8a4a5"
      },
      "source": [
        "### Visualizing R¬≤ Scores\n",
        "\n",
        "We will create a bar chart to visualize the R¬≤ scores and see which features matter most."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076a7f5e",
      "metadata": {
        "id": "076a7f5e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "# Calculate dynamic x-axis limits to accommodate negative R¬≤\n",
        "xmin = min(-0.2, r2_df['R¬≤ Score'].min() - 0.05)\n",
        "xmax = 1.0\n",
        "\n",
        "colors = ['green' if x > 0.5 else 'orange' if x > 0.1 else 'red' for x in r2_df['R¬≤ Score']]\n",
        "bars = plt.barh(r2_df['Feature'], r2_df['R¬≤ Score'], color=colors, edgecolor='black', alpha=0.8)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (feature, score) in enumerate(zip(r2_df['Feature'], r2_df['R¬≤ Score'])):\n",
        "    plt.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.xlabel('R¬≤ Score (Higher = Better Predictor)', fontsize=12)\n",
        "plt.ylabel('Material Parameter', fontsize=12)\n",
        "plt.title('Feature Importance: R¬≤ Scores for Predicting Stress\\n(Single Tensile Test Data)',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlim([xmin, xmax])\n",
        "plt.axvline(0.5, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Key Insight:\")\n",
        "print(\"   Strain dominates because stress is fundamentally a function of strain.\")\n",
        "print(\"   Other material parameters are constant in this single test.\")\n",
        "print(\"   To assess their importance, we would need multi-specimen testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9106051",
      "metadata": {
        "id": "a9106051"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.2.1 The Critical Limitation: Why Material Parameters Show R¬≤ ‚âà 0\n",
        "\n",
        "### The Problem\n",
        "\n",
        "The R¬≤ analysis above reveals a puzzling result: only strain matters. Material properties (carbon content, grain size) show R¬≤ ‚âà 0.\n",
        "\n",
        "**Is this correct?** Do carbon and grain size really not affect stress?\n",
        "\n",
        "**No!** This is a **data collection artifact**, not a physical truth.\n",
        "\n",
        "### Why This Happens\n",
        "\n",
        "In a **single tensile test**:\n",
        "- Specimen has ONE carbon content (e.g., 0.18%)\n",
        "- Specimen has ONE grain size (e.g., 40 Œºm)\n",
        "- These values are **constant** throughout the test\n",
        "- ML algorithm sees: strain varies (0 ‚Üí 0.2), stress varies (0 ‚Üí 400 MPa)\n",
        "- ML algorithm sees: carbon = 0.18% always, stress varies\n",
        "- Conclusion: carbon does not predict stress (R¬≤ = 0)\n",
        "\n",
        "**This is wrong!** Carbon DOES affect strength, but we need **multiple specimens** to see it.\n",
        "\n",
        "### The Physical Reality\n",
        "\n",
        "Yield strength depends on microstructure:\n",
        "\n",
        "**Carbon content effect** (Solid solution strengthening):\n",
        "$$\\Delta \\sigma_y^{\\text{carbon}} \\approx 2000 \\text{ MPa} \\times C_{\\text{wt\\%}}$$\n",
        "\n",
        "Example: 0.20% C adds ~400 MPa compared to pure Fe\n",
        "\n",
        "**Grain size effect** (Hall-Petch relationship):\n",
        "$$\\Delta \\sigma_y^{\\text{grain}} = k_y d^{-1/2}$$\n",
        "\n",
        "where d = grain diameter, k_y ‚âà 0.6 MPa¬∑m^(1/2) for steel\n",
        "\n",
        "Example: 20 Œºm grains ‚Üí +134 MPa compared to 80 Œºm grains\n",
        "\n",
        "**Combined effect on stress-strain curve**:\n",
        "$$\\sigma_y(C, d) = \\sigma_{y,\\text{base}} + 2000C + \\frac{k_y}{\\sqrt{d}}$$\n",
        "\n",
        "This shifts the entire stress-strain curve vertically!\n",
        "\n",
        "### The Solution: Multi-Specimen Testing\n",
        "\n",
        "To quantify material parameter effects, we need:\n",
        "- **Multiple specimens** with varying C and d\n",
        "- Each specimen produces a stress-strain curve\n",
        "- ML can now see: high C ‚Üí higher curve, small d ‚Üí higher curve\n",
        "\n",
        "**We will demonstrate this.**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39bbb7ae",
      "metadata": {
        "id": "39bbb7ae"
      },
      "source": [
        "### The Single-Specimen Limitation\n",
        "\n",
        "**Critical Observation**: Material parameters show R¬≤ ‚âà 0 above.\n",
        "\n",
        "**Why?** They are constant in a single test. We need multiple specimens.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee9ae77",
      "metadata": {
        "id": "9ee9ae77"
      },
      "source": [
        "### Multi-Specimen Analysis: What to Expect\n",
        "\n",
        "**Experimental design**:\n",
        "- Generate 30 specimens with:\n",
        "  - Carbon content: 0.10% to 0.30% (3√ó range)\n",
        "  - Grain size: 20 to 80 Œºm (4√ó range)\n",
        "- Each specimen: ~50 strain measurements\n",
        "- Total: ~1500 data points\n",
        "\n",
        "**Expected stress-strain behavior**:\n",
        "- Higher C ‚Üí Higher œÉ_y ‚Üí Entire curve shifts up\n",
        "- Smaller d ‚Üí Higher œÉ_y ‚Üí Entire curve shifts up\n",
        "- Elastic modulus E: Constant (210 GPa for all steel)\n",
        "- Strain at failure: Similar (~0.15-0.20)\n",
        "\n",
        "**Expected R¬≤ results**:\n",
        "- Strain: Still dominant (R¬≤ ‚âà 0.7) - fundamental œÉ-Œµ relationship\n",
        "- Carbon content: NOW non-zero (R¬≤ ‚âà 0.2-0.4)\n",
        "- Grain size (1/‚àöd): NOW non-zero (R¬≤ ‚âà 0.1-0.3)\n",
        "- Combined model: Even better\n",
        "\n",
        "**Key learning**: Material parameters only emerge with multi-specimen data!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e91c6d",
      "metadata": {
        "id": "44e91c6d"
      },
      "outputs": [],
      "source": [
        "# Generate 30 tensile tests with varying material properties\n",
        "# Necking rate parameter (same as single specimen)\n",
        "alpha = 10.0\n",
        "np.random.seed(42)\n",
        "\n",
        "n_specimens = 30\n",
        "n_points_per_test = 50\n",
        "all_data = []\n",
        "\n",
        "for spec_id in range(n_specimens):\n",
        "    carbon = np.random.uniform(carbon_range[0], carbon_range[1])  # wt%\n",
        "    grain_size = np.random.uniform(grain_size_range[0], grain_size_range[1])  # Œºm\n",
        "\n",
        "    base_strength = base_strength  # use config value\n",
        "    carbon_effect = carbon * carbon_effect_coeff\n",
        "    hall_petch = hall_petch_coeff / np.sqrt(grain_size)\n",
        "\n",
        "    sigma_y_spec = base_strength + carbon_effect + hall_petch  # yield strength influenced by C and grain size\n",
        "    sigma_u_spec = sigma_y_spec * 1.5  # ultimate taken as 1.5√ó yield\n",
        "\n",
        "    E_spec = 210e9\n",
        "    K_spec = sigma_u_spec * 1.2  # adjust K for moderate hardening\n",
        "    n_spec = 0.25\n",
        "    eps_y_spec = sigma_y_spec / E_spec\n",
        "    eps_u_spec = 0.20 + np.random.normal(0, 0.015)  # uniform elongation with variation\n",
        "\n",
        "    strains = np.linspace(0, eps_u_spec * 1.1, n_points_per_test)\n",
        "\n",
        "    for eps in strains:\n",
        "        if eps < eps_y_spec:\n",
        "            stress = E_spec * eps\n",
        "        elif eps <= eps_u_spec:\n",
        "            eps_p = eps - eps_y_spec\n",
        "            stress = sigma_y_spec + K_spec * eps_p**n_spec\n",
        "        else:\n",
        "            sigma_u = sigma_y_spec + K_spec * (eps_u_spec - eps_y_spec)**n_spec\n",
        "            stress = sigma_u * np.exp(-alpha * (eps - eps_u_spec))\n",
        "\n",
        "        stress += np.random.normal(0, noise_level)\n",
        "\n",
        "        all_data.append({\n",
        "            'specimen_id': spec_id,\n",
        "            'strain': eps,\n",
        "            'stress': stress,\n",
        "            'carbon_content': carbon,\n",
        "            'grain_size_um': grain_size,\n",
        "            'grain_size_inv_sqrt': 1 / np.sqrt(grain_size),\n",
        "            'yield_strength': sigma_y_spec\n",
        "        })\n",
        "\n",
        "df_multi = pd.DataFrame(all_data)\n",
        "\n",
        "print(f\"Generated {n_specimens} specimens, {len(df_multi)} total points\")\n",
        "print(f\"Carbon: {df_multi['carbon_content'].min():.2f}-{df_multi['carbon_content'].max():.2f}%\")\n",
        "print(f\"Yield: {df_multi['yield_strength'].min()/1e6:.0f}-{df_multi['yield_strength'].max()/1e6:.0f} MPa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U1m6ArOSQux2",
      "metadata": {
        "id": "U1m6ArOSQux2"
      },
      "source": [
        "Visualize all 30 stress-strain curves to observe material variability across specimens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a362a9",
      "metadata": {
        "id": "33a362a9"
      },
      "outputs": [],
      "source": [
        "# Visualize the 30 stress-strain curves\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "for spec_id in range(n_specimens):\n",
        "    spec_data = df_multi[df_multi['specimen_id'] == spec_id]\n",
        "    carbon = spec_data['carbon_content'].iloc[0]\n",
        "\n",
        "    plt.plot(spec_data['strain'], spec_data['stress'] / 1e6,\n",
        "             alpha=0.6, linewidth=1.5,\n",
        "             color=plt.cm.viridis(carbon / 0.30))\n",
        "\n",
        "plt.xlabel('Strain', fontsize=12)\n",
        "plt.ylabel('Stress (MPa)', fontsize=12)\n",
        "plt.title(f'{n_specimens} Tensile Tests: Varying Carbon Content and Grain Size',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add colorbar\n",
        "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis,\n",
        "                           norm=plt.Normalize(vmin=0.10, vmax=0.30))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=plt.gca())\n",
        "cbar.set_label('Carbon Content (%)', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Observation:\")\n",
        "print(\"   Higher carbon ‚Üí Higher yield strength ‚Üí Shifted curves\")\n",
        "print(\"   This variation allows ML to learn material parameter effects\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z34iB2M8Qux3",
      "metadata": {
        "id": "z34iB2M8Qux3"
      },
      "source": [
        "Compute R¬≤ scores for each feature across multiple specimens to quantify their predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56c135f",
      "metadata": {
        "id": "f56c135f"
      },
      "outputs": [],
      "source": [
        "# R¬≤ analysis on multi-specimen dataset\n",
        "features_multi = ['strain', 'carbon_content', 'grain_size_um', 'grain_size_inv_sqrt']\n",
        "\n",
        "r2_scores_multi = {}\n",
        "for feature in features_multi:\n",
        "    X = df_multi[[feature]].values\n",
        "    y = df_multi['stress'].values\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    r2_scores_multi[feature] = r2_score(y, y_pred)\n",
        "\n",
        "r2_df_multi = pd.DataFrame(list(r2_scores_multi.items()),\n",
        "                           columns=['Feature', 'R¬≤ Score'])\n",
        "r2_df_multi = r2_df_multi.sort_values('R¬≤ Score', ascending=False)\n",
        "\n",
        "# Comparison plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Single specimen (from earlier r2_df)\n",
        "colors1 = ['green' if x > 0.5 else 'orange' if x > 0.1 else 'red'\n",
        "           for x in r2_df['R¬≤ Score']]\n",
        "ax1.barh(r2_df['Feature'], r2_df['R¬≤ Score'], color=colors1,\n",
        "         edgecolor='black', alpha=0.8)\n",
        "ax1.set_xlabel('R¬≤ Score', fontsize=11)\n",
        "ax1.set_title('Single Specimen\\n(Constant properties)',\n",
        "             fontsize=12, fontweight='bold')\n",
        "ax1.set_xlim([0, 1])\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Multi specimen\n",
        "colors2 = ['green' if x > 0.5 else 'orange' if x > 0.1 else 'red'\n",
        "           for x in r2_df_multi['R¬≤ Score']]\n",
        "ax2.barh(r2_df_multi['Feature'], r2_df_multi['R¬≤ Score'], color=colors2,\n",
        "         edgecolor='black', alpha=0.8)\n",
        "ax2.set_xlabel('R¬≤ Score', fontsize=11)\n",
        "ax2.set_title(f'{n_specimens} Specimens\\n(Varying properties)',\n",
        "             fontsize=12, fontweight='bold')\n",
        "ax2.set_xlim([0, 1])\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY INSIGHT: The Single-Specimen Trap\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get strain R¬≤ from single specimen\n",
        "strain_r2_single = r2_df[r2_df['Feature']=='strain']['R¬≤ Score'].values[0]\n",
        "\n",
        "print(f\"\\nSingle specimen:\")\n",
        "print(f\"  Strain R¬≤ = {strain_r2_single:.3f}\")\n",
        "print(f\"  Material params R¬≤ ‚âà 0 (constant in single test)\")\n",
        "\n",
        "print(f\"\\n{n_specimens} specimens:\")\n",
        "print(f\"  Strain R¬≤ = {r2_scores_multi['strain']:.3f}\")\n",
        "print(f\"  Carbon R¬≤ = {r2_scores_multi['carbon_content']:.3f}\")\n",
        "print(f\"  Grain size R¬≤ = {r2_scores_multi['grain_size_inv_sqrt']:.3f}\")\n",
        "\n",
        "print(\"\\nüí° Material parameters only emerge with multi-specimen data!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fbec588",
      "metadata": {
        "id": "9fbec588"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.3 Linear Regression Framework\n",
        "\n",
        "### Model Definition\n",
        "\n",
        "Linear regression fits a hyperplane to minimize prediction error:\n",
        "\n",
        "$$h_\\theta(x) = \\theta_0 + \\sum_{j=1}^{n} \\theta_j x_j$$\n",
        "\n",
        "**Note**: \"Linear\" refers to parameters Œ∏, not features x. We can model nonlinear relationships using polynomial features: x ‚Üí [x, x¬≤, x¬≥, ...].\n",
        "\n",
        "### Cost Function\n",
        "\n",
        "Mean squared error (MSE) quantifies model performance:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "### Optimization via Gradient Descent\n",
        "\n",
        "Iteratively update parameters to minimize J(Œ∏):\n",
        "\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
        "\n",
        "where:\n",
        "- Œ± = learning rate (step size)\n",
        "- $\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$\n",
        "\n",
        "**Learning rate selection**:\n",
        "- Too small ‚Üí slow convergence\n",
        "- Too large ‚Üí oscillation or divergence\n",
        "- Optimal ‚Üí rapid convergence to global minimum (convex J(Œ∏))\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74947984",
      "metadata": {
        "id": "74947984"
      },
      "source": [
        "### What to Expect\n",
        "\n",
        "**What we will do**:\n",
        "- Interactively adjust learning rate Œ±\n",
        "- Watch cost function J(Œ∏) decrease over iterations\n",
        "- Observe convergence behavior\n",
        "\n",
        "**Expected outcomes**:\n",
        "- Œ± too small ‚Üí Slow, steady convergence\n",
        "- Œ± optimal ‚Üí Rapid convergence in ~20-50 iterations\n",
        "- Œ± too large ‚Üí Oscillation or divergence\n",
        "\n",
        "**Goal**: Find Œ± that balances speed and stability.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c294e57d",
      "metadata": {
        "id": "c294e57d"
      },
      "source": [
        "### Interactive Gradient Descent Demonstration\n",
        "\n",
        "We will visualize how gradient descent works by watching the algorithm find the optimal parameters. First, set the learning rate using the slider below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7371ef4e",
      "metadata": {
        "id": "7371ef4e"
      },
      "outputs": [],
      "source": [
        "# Generate demonstration data for gradient descent\n",
        "x_gd = np.linspace(0, 10, 20)\n",
        "y_gd = 2.0 * x_gd + 1.0 + np.random.normal(0, 2.0, size=len(x_gd))\n",
        "\n",
        "def run_gradient_descent(alpha, max_iters=100):\n",
        "    \"\"\"Simulate gradient descent for simple linear regression, with overflow/NaN guards.\n",
        "    Keeps return signature unchanged and preserves list lengths for plotting.\"\"\"\n",
        "    # Fixed seed for reproducible demo - prevents random appearance on slider moves\n",
        "    np.random.seed(42)\n",
        "    theta0 = float(np.random.randn())\n",
        "    theta1 = float(np.random.randn())\n",
        "\n",
        "    m = len(x_gd)\n",
        "\n",
        "    cost_history = []\n",
        "    theta0_history = [theta0]\n",
        "    theta1_history = [theta1]\n",
        "\n",
        "    # Soft ceiling to avoid axis/ticker overflows downstream\n",
        "    CLIP_MAX = 1e20\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        # Predictions\n",
        "        y_pred = theta0 + theta1 * x_gd\n",
        "\n",
        "        # Cost (MSE/2) using stable dot product\n",
        "        err = y_pred - y_gd\n",
        "        cost = float((err @ err) / (2.0 * m))\n",
        "\n",
        "        # Guard against non-finite or absurdly large costs\n",
        "        if (not np.isfinite(cost)) or (cost <= 0.0) or (cost > CLIP_MAX):\n",
        "            cost_history.append(np.nan)\n",
        "            # pad remaining to keep plotting code happy\n",
        "            remaining = max_iters - (iteration + 1)\n",
        "            if remaining > 0:\n",
        "                cost_history.extend([np.nan] * remaining)\n",
        "                theta0_history.extend([theta0] * remaining)\n",
        "                theta1_history.extend([theta1] * remaining)\n",
        "            break\n",
        "\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Gradients\n",
        "        grad_theta0 = float(err.mean())\n",
        "        grad_theta1 = float((err * x_gd).mean())\n",
        "\n",
        "        # Update steps with basic sanity checks\n",
        "        step0 = alpha * grad_theta0\n",
        "        step1 = alpha * grad_theta1\n",
        "\n",
        "        if (not np.isfinite(step0)) or (not np.isfinite(step1)) \\\n",
        "           or (abs(step0) > CLIP_MAX) or (abs(step1) > CLIP_MAX):\n",
        "            # Divergence detected; pad and stop\n",
        "            remaining = max_iters - (iteration + 1)\n",
        "            if remaining > 0:\n",
        "                cost_history.extend([np.nan] * remaining)\n",
        "                theta0_history.extend([theta0] * remaining)\n",
        "                theta1_history.extend([theta1] * remaining)\n",
        "            break\n",
        "\n",
        "        # Parameter update\n",
        "        theta0 = float(theta0 - step0)\n",
        "        theta1 = float(theta1 - step1)\n",
        "\n",
        "        theta0_history.append(theta0)\n",
        "        theta1_history.append(theta1)\n",
        "\n",
        "    return theta0, theta1, cost_history, theta0_history, theta1_history\n",
        "\n",
        "\n",
        "# Interactive gradient descent with live cost visualization\n",
        "def interactive_gd_with_plot(alpha_log):\n",
        "    alpha = 10**alpha_log\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Run gradient descent\n",
        "    theta0_final, theta1_final, cost_history, theta0_hist, theta1_hist = run_gradient_descent(alpha, max_iters=100)\n",
        "\n",
        "    # Create figure with 2 subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Left: Cost vs iteration\n",
        "    valid_costs = [c for c in cost_history if np.isfinite(c) and c > 0]\n",
        "    if len(valid_costs) > 0:\n",
        "        iterations = range(len(valid_costs))\n",
        "        ax1.plot(iterations, valid_costs, 'b-', linewidth=2, marker='o', markersize=4)\n",
        "        ax1.set_xlabel('Iteration', fontsize=12)\n",
        "        ax1.set_ylabel('Cost J(Œ∏)', fontsize=12)\n",
        "        ax1.set_title(f'Cost Function Convergence\\nLearning Rate Œ± = {alpha:.4f}',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Use log scale if cost varies over orders of magnitude\n",
        "        if len(valid_costs) > 1 and valid_costs[0] / valid_costs[-1] > 100:\n",
        "            ax1.set_yscale('log')\n",
        "\n",
        "        # Check convergence status\n",
        "        # First check for divergence (NaN appeared OR cost exploded)\n",
        "        if len(valid_costs) < len(cost_history):\n",
        "            status = \"‚úó DIVERGED\"\n",
        "            color = 'red'\n",
        "        # Check if cost exploded (increased by 100x from minimum)\n",
        "        elif len(valid_costs) > 10:\n",
        "            min_cost = min(valid_costs)\n",
        "            if valid_costs[-1] > min_cost * 100:\n",
        "                status = \"‚úó DIVERGED\"\n",
        "                color = 'red'\n",
        "            # Check for good convergence\n",
        "            elif valid_costs[-1] < valid_costs[0] * 0.01:\n",
        "                status = \"‚úì CONVERGED\"\n",
        "                color = 'green'\n",
        "            # Ran full iterations but didn't converge well\n",
        "            elif len(valid_costs) >= 100:\n",
        "                status = \"‚ö† SLOW\"\n",
        "                color = 'orange'\n",
        "            # Stopped early without divergence\n",
        "            else:\n",
        "                status = \"‚ö† STOPPED EARLY\"\n",
        "                color = 'orange'\n",
        "        # Too few iterations to determine\n",
        "        else:\n",
        "            status = \"‚ö† STOPPED EARLY\"\n",
        "            color = 'orange'\n",
        "\n",
        "        ax1.text(0.5, 0.95, status, transform=ax1.transAxes,\n",
        "                 fontsize=14, fontweight='bold', color=color,\n",
        "                 ha='center', va='top',\n",
        "                 bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor=color, linewidth=2))\n",
        "\n",
        "    # Right: Data with fitted line\n",
        "    ax2.scatter(x_gd, y_gd, color='orange', s=80, edgecolors='k',\n",
        "                zorder=3, label='Data', alpha=0.7)\n",
        "\n",
        "    if np.isfinite(theta0_final) and np.isfinite(theta1_final):\n",
        "        x_line = np.linspace(x_gd.min(), x_gd.max(), 100)\n",
        "        y_line = theta0_final + theta1_final * x_line\n",
        "        ax2.plot(x_line, y_line, 'g-', linewidth=3, alpha=0.8,\n",
        "                 label=f'Fit: y = {theta1_final:.2f}x + {theta0_final:.2f}')\n",
        "\n",
        "    ax2.set_xlabel('x', fontsize=12)\n",
        "    ax2.set_ylabel('y', fontsize=12)\n",
        "    ax2.set_title('Fitted Line', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Learning rate Œ± = {alpha:.4f}\")\n",
        "    if len(valid_costs) > 0:\n",
        "        print(f\"Final cost: {valid_costs[-1]:.2f}\")\n",
        "        print(f\"Iterations: {len(valid_costs)}\")\n",
        "        print(f\"Cost reduction: {(1 - valid_costs[-1]/valid_costs[0])*100:.1f}%\")\n",
        "    else:\n",
        "        print(\"DIVERGED - No valid cost values\")\n",
        "\n",
        "# Widget with better range\n",
        "slider_alpha = widgets.FloatSlider(\n",
        "    value=-0.5, min=-2, max=0.5, step=0.1,\n",
        "    description='log‚ÇÅ‚ÇÄ(Œ±):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='600px')\n",
        ")\n",
        "\n",
        "print(\"üéÆ Interactive Gradient Descent with Live Cost Plot\")\n",
        "print(\"=\"*60)\n",
        "print(\"Adjust learning rate and observe:\")\n",
        "print(\"  ‚Ä¢ Cost function convergence (left plot)\")\n",
        "print(\"  ‚Ä¢ Fitted line evolution (right plot)\")\n",
        "print(\"\\nTry different values:\")\n",
        "print(\"  ‚Ä¢ log‚ÇÅ‚ÇÄ(Œ±) = -2.0  ‚Üí  Œ± = 0.01  (too small)\")\n",
        "print(\"  ‚Ä¢ log‚ÇÅ‚ÇÄ(Œ±) = -0.5  ‚Üí  Œ± = 0.32  (good)\")\n",
        "print(\"  ‚Ä¢ log‚ÇÅ‚ÇÄ(Œ±) =  0.5  ‚Üí  Œ± = 3.16  (too large)\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "interact(interactive_gd_with_plot, alpha_log=slider_alpha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9traAGyQux4",
      "metadata": {
        "id": "q9traAGyQux4"
      },
      "source": [
        "Define a group-aware data splitting function to prevent specimen data leakage during cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05082c4",
      "metadata": {
        "id": "f05082c4"
      },
      "outputs": [],
      "source": [
        "# Group-aware train/test split helper (prevents specimen leakage)\n",
        "def grouped_split(df, X_cols, y_col, group_col='specimen_id', test_size=0.25, seed=42):\n",
        "    \"\"\"\n",
        "    Split data by groups (specimens) to avoid leakage.\n",
        "\n",
        "    Points from the same specimen share chemistry and microstructure.\n",
        "    Random row splits would leak this information into the test set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Source data\n",
        "    X_cols : list of str\n",
        "        Feature column names\n",
        "    y_col : str\n",
        "        Target column name\n",
        "    group_col : str\n",
        "        Column defining groups (default: 'specimen_id')\n",
        "    test_size : float\n",
        "        Fraction for test set (default: 0.25)\n",
        "    seed : int\n",
        "        Random seed\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, X_test, y_train, y_test, train_idx, test_idx\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "    groups = df[group_col].values\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    train_idx, test_idx = next(gss.split(df, groups=groups))\n",
        "\n",
        "    X = df[X_cols].values\n",
        "    y = df[y_col].values\n",
        "\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx], train_idx, test_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9ac592",
      "metadata": {
        "id": "4f9ac592"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.4 Linear Model on Multi-Specimen Data: Underfitting\n",
        "\n",
        "### Hypothesis: Linear Model with Strain Only\n",
        "\n",
        "Now that we have 30 specimens with varying material properties, we will test the simplest model:\n",
        "\n",
        "$$\\sigma = \\theta_0 + \\theta_1 \\varepsilon$$\n",
        "\n",
        "**This model intentionally ignores**:\n",
        "- Carbon content (C)\n",
        "- Grain size (d)\n",
        "- Nonlinearity (Œµ¬≤, Œµ¬≥, ...)\n",
        "\n",
        "### What to Expect\n",
        "\n",
        "**Underfitting in TWO ways**:\n",
        "1. **Linear** assumption: Stress-strain is nonlinear (elastic + plastic + necking)\n",
        "2. **Missing features**: Model ignores C and d, which we KNOW affect strength\n",
        "\n",
        "**Prediction**: Low R¬≤ because model is too simple in multiple dimensions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "btdouk0706k",
      "metadata": {
        "id": "btdouk0706k"
      },
      "source": [
        "> **‚ö†Ô∏è Common Pitfall: Row-Level Splits**\n",
        ">\n",
        "> Points from the same specimen share chemistry and microstructure. Using `train_test_split()` on individual rows can place some points from Specimen A in training and others in testing. The model then memorizes specimen-specific patterns instead of learning true material physics.\n",
        ">\n",
        "> **Solution**: Always split by `specimen_id` using grouped splits to ensure entire specimens stay together in either train or test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d09940",
      "metadata": {
        "id": "07d09940"
      },
      "outputs": [],
      "source": [
        "# Fit simple linear regression on 30-specimen data\n",
        "# Model: œÉ = Œ∏‚ÇÄ + Œ∏‚ÇÅ¬∑Œµ (ONLY strain, ignoring C and d)\n",
        "\n",
        "# Split data using grouped_split to prevent data leakage\n",
        "X_train_simple, X_test_simple, y_train_simple, y_test_simple, *_ = grouped_split(\n",
        "    df_multi, ['strain'], 'stress', test_size=0.2, seed=42\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "model_simple_multi = LinearRegression()\n",
        "model_simple_multi.fit(X_train_simple, y_train_simple)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_simple = model_simple_multi.predict(X_train_simple)\n",
        "y_test_pred_simple = model_simple_multi.predict(X_test_simple)\n",
        "\n",
        "# Metrics\n",
        "r2_train_simple = r2_score(y_train_simple, y_train_pred_simple)\n",
        "r2_test_simple = r2_score(y_test_simple, y_test_pred_simple)\n",
        "rmse_train_simple = np.sqrt(mean_squared_error(y_train_simple, y_train_pred_simple))\n",
        "rmse_test_simple = np.sqrt(mean_squared_error(y_test_simple, y_test_pred_simple))\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"LINEAR MODEL ON 30 SPECIMENS (Strain only)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Model: œÉ = {model_simple_multi.coef_[0]/1e9:.2f} GPa √ó Œµ + {model_simple_multi.intercept_/1e6:.1f} MPa\")\n",
        "print()\n",
        "print(f\"Train R¬≤: {r2_train_simple:.4f}\")\n",
        "print(f\"Test R¬≤:  {r2_test_simple:.4f}\")\n",
        "print(f\"Train RMSE: {rmse_train_simple/1e6:.2f} MPa\")\n",
        "print(f\"Test RMSE:  {rmse_test_simple/1e6:.2f} MPa\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"üî¥ SEVERE UNDERFITTING CONFIRMED\")\n",
        "print(\"Why are the R¬≤ scores so low?\")\n",
        "print(\"  ‚Ä¢ The linear model is too simple for the nonlinear stress-strain data.\")\n",
        "print(\"  ‚Ä¢ It completely ignores the significant effects of carbon and grain size,\")\n",
        "print(\"    which cause large variations in yield strength across specimens.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d805d490",
      "metadata": {
        "id": "d805d490"
      },
      "source": [
        "### What to Expect\n",
        "\n",
        "**What we will do**:\n",
        "- Apply L2 regularization to the underfitting linear model\n",
        "- Test Œª from 0 to 1000\n",
        "\n",
        "**Expected outcome**:\n",
        "- Performance will get **WORSE** as Œª increases\n",
        "- Why? Regularization reduces overfitting, not underfitting\n",
        "- Our linear model is already too simple\n",
        "\n",
        "**Key lesson**: Regularization helps complex models, not simple ones.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884a99f8",
      "metadata": {
        "id": "884a99f8"
      },
      "source": [
        "> **üí° What This Plot Shows**\n",
        ">\n",
        "> - **Left panel**: The shaded band shows the interquartile range over 30 specimens. Each colored line represents the same linear model (strain-only) with a different Œª penalty.\n",
        "> - **As Œª grows**: The slope shrinks toward zero while the intercept remains free (we use `fit_intercept=True`).\n",
        "> - **Why test R¬≤ falls**: This model is already too simple (underfitting). Regularization helps when you have *enough capacity*; here we are starving a starving model.\n",
        ">\n",
        "> **Key Insight**: Ridge regression penalizes large coefficients but will not fix a fundamentally underfit model. You need more features first."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22924915",
      "metadata": {
        "id": "22924915"
      },
      "source": [
        "### Visualizing Regularization Effect\n",
        "\n",
        "We will plot how different regularization strengths affect the fitted line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1025b9da",
      "metadata": {
        "id": "1025b9da"
      },
      "outputs": [],
      "source": [
        "# --- 1.5 Visualizing Regularization Effect ---\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# ===== 1) Choose a Œª grid that actually does something =====\n",
        "m = X_train_simple.shape[0]\n",
        "lambda_values = np.r_[0.0, np.logspace(np.log10(m*1e-3), np.log10(m*1e3), 8)]\n",
        "\n",
        "# ===== 2) Recompute metrics for strain-only model (with intercept) =====\n",
        "rows = []\n",
        "for lam in lambda_values:\n",
        "    pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"ridge\",  Ridge(alpha=float(lam), fit_intercept=True))\n",
        "    ])\n",
        "    pipe.fit(X_train_simple, y_train_simple)\n",
        "\n",
        "    ytr = pipe.predict(X_train_simple)\n",
        "    yte = pipe.predict(X_test_simple)\n",
        "\n",
        "    r2_tr = r2_score(y_train_simple, ytr)\n",
        "    r2_te = r2_score(y_test_simple,  yte)\n",
        "    rmse_te = np.sqrt(mean_squared_error(y_test_simple, yte)) / 1e6\n",
        "\n",
        "    scaler = pipe.named_steps[\"scaler\"]\n",
        "    ridge  = pipe.named_steps[\"ridge\"]\n",
        "    w_std  = float(ridge.coef_[0])\n",
        "    b_std  = float(ridge.intercept_)\n",
        "    sigmaX = float(scaler.scale_[0])\n",
        "    muX    = float(scaler.mean_[0])\n",
        "\n",
        "    slope_orig = w_std / sigmaX\n",
        "    intercept_orig = b_std - w_std * muX / sigmaX\n",
        "\n",
        "    rows.append({\"lambda\": lam, \"R¬≤ train\": r2_tr, \"R¬≤ test\": r2_te,\n",
        "                 \"RMSE test (MPa)\": rmse_te,\n",
        "                 \"slope (GPa)\": slope_orig / 1e9,\n",
        "                 \"intercept (MPa)\": intercept_orig / 1e6})\n",
        "\n",
        "df_reg_multi = pd.DataFrame(rows)\n",
        "# Add slope column in MPa for consistent plotting\n",
        "df_reg_multi['slope (MPa)'] = df_reg_multi['slope (GPa)'] * 1000\n",
        "\n",
        "\n",
        "# ===== 3) Empirical pooled reference (median + IQR) across SPECIMENS =====\n",
        "n_bins = 60\n",
        "bins = np.linspace(df_multi['strain'].min(), df_multi['strain'].max(), n_bins + 1)\n",
        "cats = pd.cut(df_multi['strain'], bins, include_lowest=True)\n",
        "\n",
        "def q25(x): return np.percentile(x, 25)\n",
        "def q75(x): return np.percentile(x, 75)\n",
        "\n",
        "ref = (df_multi.assign(bin=cats)\n",
        "       .groupby('bin', observed=False)\n",
        "       .agg(strain_med=('strain','median'),\n",
        "            stress_med=('stress','median'),\n",
        "            stress_q25=('stress', q25),\n",
        "            stress_q75=('stress', q75))\n",
        "       .dropna())\n",
        "\n",
        "# ===== 4) Plot: left = pooled \"truth\" vs ridge lines; right = metrics + slope/bias vs Œª =====\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Left: empirical pooled \"truth\"\n",
        "ax1.plot(ref['strain_med'], ref['stress_med']/1e6, 'k-', lw=2, alpha=0.9, label='Empirical median (30 specimens)')\n",
        "ax1.fill_between(ref['strain_med'], ref['stress_q25']/1e6, ref['stress_q75']/1e6,\n",
        "                 color='gray', alpha=0.15, label='IQR across specimens')\n",
        "\n",
        "# Ridge lines with intercept across calibrated Œª grid\n",
        "x_fit = np.linspace(df_multi['strain'].min(), df_multi['strain'].max(), 300).reshape(-1, 1)\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(lambda_values)))\n",
        "for lam, color in zip(lambda_values, colors):\n",
        "    pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"ridge\",  Ridge(alpha=float(lam), fit_intercept=True))\n",
        "    ])\n",
        "    pipe.fit(X_train_simple, y_train_simple)\n",
        "    y_line = pipe.predict(x_fit)\n",
        "    lab = f\"Œª={lam:.3g}\" if lam > 0 else \"Œª=0 (OLS)\"\n",
        "    ax1.plot(x_fit.ravel(), y_line/1e6, color=color, lw=2, alpha=0.9, label=lab)\n",
        "\n",
        "ax1.set_xlabel('Strain')\n",
        "ax1.set_ylabel('Stress (MPa)')\n",
        "ax1.set_title('Ridge on underfitting model (strain only, bias enabled)')\n",
        "# Legend positioned for clarity\n",
        "ax1.legend(fontsize=8, ncol=2, loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: R¬≤ on primary axis, other metrics on secondary axis\n",
        "# Plot the lambda=0 case at a small value on the log scale for visibility\n",
        "plot_lambda = df_reg_multi['lambda'].replace(0, 1e-1)\n",
        "\n",
        "# R¬≤ on left axis\n",
        "ax2.set_xlabel('Regularization Strength (Œª)')\n",
        "ax2.set_title('Performance & parameter shrinkage vs Œª (test set)')\n",
        "ax2.grid(True, which='both', alpha=0.3)\n",
        "l1 = ax2.semilogx(plot_lambda, df_reg_multi['R¬≤ test'], 'o-', lw=2, ms=7, color='tab:blue', label='R¬≤ (test)')\n",
        "ax2.set_ylabel('R¬≤ Score (Test Set)', color='tab:blue')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "# MPa metrics on right axis\n",
        "ax2b = ax2.twinx()\n",
        "l2 = ax2b.semilogx(plot_lambda, df_reg_multi['RMSE test (MPa)'], 's--', lw=2, ms=7, color='tab:orange', label='RMSE (MPa, test)')\n",
        "l3 = ax2b.semilogx(plot_lambda, df_reg_multi['slope (MPa)'], '^-.', lw=2, ms=6, color='tab:green', label='Slope (MPa)')\n",
        "l4 = ax2b.semilogx(plot_lambda, df_reg_multi['intercept (MPa)'], 'v-.', lw=2, ms=6, color='tab:red', label='Intercept (MPa)')\n",
        "ax2b.set_ylabel('Value in Megapascals (MPa)', color='tab:red')\n",
        "ax2b.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "\n",
        "# Combined legend\n",
        "lines = l1 + l2 + l3 + l4\n",
        "labels = [l.get_label() for l in lines]\n",
        "ax2.legend(lines, labels, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"REGULARIZATION ON UNDERFITTING MODEL (strain-only, bias enabled; Œª grid ~ m)\")\n",
        "print(\"=\"*80)\n",
        "print(df_reg_multi.drop(columns=['slope (GPa)']).to_string(index=False)) # Display table without GPa column\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55383150",
      "metadata": {
        "id": "55383150"
      },
      "source": [
        "### What to Expect\n",
        "\n",
        "**What we will do**:\n",
        "- Test polynomial degrees 1 to 20\n",
        "- Track train vs test R¬≤\n",
        "- Identify bias-variance tradeoff\n",
        "\n",
        "**Expected outcomes**:\n",
        "- d=1,2 ‚Üí Underfitting (low R¬≤ on both train and test)\n",
        "- d=3-5 ‚Üí Sweet spot (high R¬≤, small train-test gap)\n",
        "- d>8 ‚Üí Overfitting risk (large train-test gap)\n",
        "\n",
        "**Goal**: Find optimal complexity balancing bias and variance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd9274a",
      "metadata": {
        "id": "6fd9274a"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.6 Polynomial Features + Material Parameters Regression: Capturing Nonlinear Behavior\n",
        "\n",
        "### Why Polynomial Features?\n",
        "\n",
        "The stress-strain relationship in materials is fundamentally nonlinear:\n",
        "- **Elastic region**: œÉ ‚àù Œµ (linear)\n",
        "- **Plastic region**: œÉ ‚àù Œµ^n where n ‚âà 0.2-0.5 (power law)\n",
        "- **Necking region**: œÉ decreases (negative curvature)\n",
        "\n",
        "To capture this with linear regression, we use **polynomial features**: create new features like Œµ¬≤, Œµ¬≥, Œµ‚Å¥, and fit them linearly.\n",
        "\n",
        "**Polynomial hypothesis**:\n",
        "$$\\sigma = \\theta_0 + \\theta_1 \\varepsilon + \\theta_2 \\varepsilon^2 + \\theta_3 \\varepsilon^3 + ... + \\theta_d \\varepsilon^d$$\n",
        "\n",
        "This is still **linear regression** because the parameters Œ∏ appear linearly, even though the features are nonlinear functions of Œµ.\n",
        "\n",
        "### The Overfitting Risk\n",
        "\n",
        "With too many polynomial terms (high degree d), the model can fit the training data perfectly but fail to generalize:\n",
        "- **Low degree (d=1,2)**: Underfitting (too simple)\n",
        "- **Medium degree (d=3,4,5)**: Good balance (captures physics)\n",
        "- **High degree (d>8)**: Often but not always overfitting (fits noise, not signal)\n",
        "\n",
        "This is where **regularization becomes useful**: it prevents high-degree polynomials from overfitting by shrinking coefficients.\n",
        "\n",
        "### Our Strategy\n",
        "\n",
        "1. Fit polynomial models of increasing degree (d=1 to 20)\n",
        "2. Compare performance WITHOUT regularization\n",
        "3. Apply L2 regularization to prevent overfitting\n",
        "4. Find the optimal degree-regularization combination\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7znvtc3nr0a",
      "metadata": {
        "id": "7znvtc3nr0a"
      },
      "source": [
        "> **üìä Bias-Variance Tradeoff**\n",
        ">\n",
        "> As model complexity increases (higher polynomial degree):\n",
        "> - **Training error decreases**: More flexible model fits training data better\n",
        "> - **Test error first decreases, then increases**: The \"sweet spot\" balances bias and variance\n",
        ">\n",
        "> **Three regimes**:\n",
        "> 1. **High bias (underfitting)**: Model too simple, both train/test R¬≤ are low\n",
        "> 2. **Balanced**: Model captures true patterns, train/test R¬≤ are similar and high\n",
        "> 3. **High variance (overfitting)**: Model memorizes noise, train R¬≤ high but test R¬≤ drops\n",
        ">\n",
        "> **Materials insight**: Polynomial features on strain capture the nonlinear stress-strain relationship (elastic ‚Üí plastic ‚Üí necking), while material features (C, d) account for specimen-to-specimen variation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fa4d45",
      "metadata": {
        "id": "99fa4d45"
      },
      "outputs": [],
      "source": [
        "# Polynomial Features with Material Parameters\n",
        "# Compare strain-only vs strain+material features\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Split multi-specimen data using grouped_split to prevent specimen leakage\n",
        "X_multi = df_multi[['strain', 'carbon_content', 'grain_size_inv_sqrt']].values\n",
        "y_multi = df_multi['stress'].values\n",
        "\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi, _, _ = grouped_split(\n",
        "    df_multi,\n",
        "    ['strain', 'carbon_content', 'grain_size_inv_sqrt'],\n",
        "    'stress',\n",
        "    test_size=0.25,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Store results for both approaches\n",
        "results_poly_multi = []\n",
        "results_bias_variance = []  # For bias-variance tradeoff\n",
        "\n",
        "max_degree = 8  # reduced to avoid extreme overfitting\n",
        "\n",
        "for degree in range(1, max_degree + 1):\n",
        "    # APPROACH A: Polynomial features on strain only\n",
        "    # Use the SAME split as X_train_multi by extracting strain column\n",
        "    X_train_strain = X_train_multi[:, 0:1]  # Extract strain column from grouped split\n",
        "    X_test_strain = X_test_multi[:, 0:1]    # Extract strain column from grouped split\n",
        "\n",
        "    poly_strain = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly_strain = poly_strain.fit_transform(X_train_strain)\n",
        "    X_test_poly_strain = poly_strain.transform(X_test_strain)\n",
        "\n",
        "    model_strain = LinearRegression()\n",
        "    model_strain.fit(X_train_poly_strain, y_train_multi)\n",
        "    r2_test_strain = model_strain.score(X_test_poly_strain, y_test_multi)\n",
        "    r2_train_strain = model_strain.score(X_train_poly_strain, y_train_multi)\n",
        "\n",
        "    # APPROACH B: Polynomial features on strain + material features\n",
        "    poly_full = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_poly_full = poly_full.fit_transform(X_train_multi)\n",
        "    X_test_poly_full = poly_full.transform(X_test_multi)\n",
        "\n",
        "    model_full = LinearRegression()\n",
        "    model_full.fit(X_train_poly_full, y_train_multi)\n",
        "    r2_test_full = model_full.score(X_test_poly_full, y_test_multi)\n",
        "    r2_train_full = model_full.score(X_train_poly_full, y_train_multi)\n",
        "\n",
        "    # Calculate RMSE using full model\n",
        "    y_pred_train_full = model_full.predict(X_train_poly_full)\n",
        "    y_pred_test_full = model_full.predict(X_test_poly_full)\n",
        "    rmse_train_full = np.sqrt(np.mean((y_train_multi - y_pred_train_full)**2)) / 1e6\n",
        "    rmse_test_full = np.sqrt(np.mean((y_test_multi - y_pred_test_full)**2)) / 1e6\n",
        "\n",
        "    # Store comparison results\n",
        "    results_poly_multi.append({\n",
        "        'Degree': degree,\n",
        "        'R¬≤ Test (Strain Only)': r2_test_strain,\n",
        "        'R¬≤ Test (Strain+C+d)': r2_test_full,\n",
        "        'Improvement': r2_test_full - r2_test_strain\n",
        "    })\n",
        "\n",
        "    # Store bias-variance results (using strain+materials approach)\n",
        "    results_bias_variance.append({\n",
        "        'Degree': degree,\n",
        "        'R¬≤ Train': r2_train_full,\n",
        "        'R¬≤ Test': r2_test_full,\n",
        "        'RMSE Train (MPa)': rmse_train_full,  # Uses full model\n",
        "        'RMSE Test (MPa)': rmse_test_full      # Uses full model\n",
        "    })\n",
        "\n",
        "# Create dataframes\n",
        "df_poly_comp = pd.DataFrame(results_poly_multi)\n",
        "df_poly = pd.DataFrame(results_bias_variance)\n",
        "\n",
        "print(\"\\nüìä POLYNOMIAL COMPARISON (30 specimens)\")\n",
        "print(\"=\" * 60)\n",
        "print(df_poly_comp.to_string(index=False))\n",
        "print(f\"\\nüí° Material features improve R¬≤ by {df_poly_comp['Improvement'].mean():.3f} on average\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alVQx_28QuyD",
      "metadata": {
        "id": "alVQx_28QuyD"
      },
      "source": [
        "Visualize the comparison between strain-only and strain+material feature models across polynomial degrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d68222",
      "metadata": {
        "id": "39d68222"
      },
      "outputs": [],
      "source": [
        "# Visualize the comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "ax.plot(df_poly_comp['Degree'], df_poly_comp['R¬≤ Test (Strain Only)'],\n",
        "        'b-o', linewidth=2, markersize=8, label='Strain Only', alpha=0.7)\n",
        "ax.plot(df_poly_comp['Degree'], df_poly_comp['R¬≤ Test (Strain+C+d)'],\n",
        "        'r-s', linewidth=2, markersize=8, label='Strain + C + d', alpha=0.7)\n",
        "\n",
        "# Fill between to show improvement\n",
        "ax.fill_between(df_poly_comp['Degree'],\n",
        "                df_poly_comp['R¬≤ Test (Strain Only)'],\n",
        "                df_poly_comp['R¬≤ Test (Strain+C+d)'],\n",
        "                alpha=0.2, color='green', label='Improvement from materials')\n",
        "\n",
        "ax.set_xlabel('Polynomial Degree', fontsize=12)\n",
        "ax.set_ylabel('Test R¬≤', fontsize=12)\n",
        "ax.set_title('Impact of Material Features on Model Performance\\n30 Specimens',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11, loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim([0.5, 1.0])\n",
        "ax.axhline(0.9, color='green', linestyle='--', alpha=0.3, label='Excellent fit')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Visualization shows:\")\n",
        "print(\"   ‚Ä¢ Blue line: Strain-only model (ignores material properties)\")\n",
        "print(\"   ‚Ä¢ Red line: Complete model (includes C and d)\")\n",
        "print(\"   ‚Ä¢ Green area: Performance gain from material features\")\n",
        "print(\"\\nüí° Material parameters consistently improve predictions!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6ff855",
      "metadata": {
        "id": "da6ff855"
      },
      "source": [
        "### Analysis of Catastrophic Overfitting\n",
        "\n",
        "The result above reveals a critical insight: while adding material features to a low-degree polynomial model (d=1 to 4) significantly improves performance, the same features cause a catastrophic failure in high-degree models (d=5+), resulting in large negative R¬≤ scores.\n",
        "\n",
        "**Why does this happen? The Curse of Dimensionality.**\n",
        "\n",
        "When we apply `PolynomialFeatures` to the combined input `[strain, carbon, grain_size_inv_sqrt]`, the function generates not only powers of each feature (`strain¬≤`, `carbon¬≤`) but also all **interaction terms** (`strain*carbon`, `strain¬≤*carbon`, etc.).\n",
        "\n",
        "- **Degree 2**: 9 features\n",
        "- **Degree 5**: 55 features\n",
        "- **Degree 8**: 164 features\n",
        "\n",
        "A high-degree polynomial on all three inputs creates an explosion of features. The model gains so much flexibility that it begins to fit the random noise in the training data perfectly, a phenomenon known as **overfitting**. When this over-specialized model is shown new test data, its predictions are wildly inaccurate, leading to an R¬≤ score far below zero.\n",
        "\n",
        "**The Physics-Informed Solution**: The underlying physics does not suggest that interactions like `carbon¬≥ * strain‚Åµ` are meaningful. The primary nonlinearity is in the stress-strain relationship. A much better approach is to:\n",
        "1. Apply polynomial features **only to the strain column**.\n",
        "2. Combine these with the **linear** material property features.\n",
        "\n",
        "This targeted feature engineering adds complexity only where physics suggests it is needed, leading to more robust and accurate models. This is the approach used in the interactive tool that follows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5997cc74",
      "metadata": {
        "id": "5997cc74"
      },
      "source": [
        "### Visualizing the Bias-Variance Tradeoff\n",
        "\n",
        "We will plot how model performance changes with polynomial degree using the **strain + material features (C, d)** model. This demonstrates the classic **bias-variance tradeoff**:\n",
        "\n",
        "**What to expect:**\n",
        "- **Low degrees (1-3)**: **Underfitting** - Model too simple, both train and test R¬≤ are low\n",
        "- **Sweet spot (~3-6)**: **Good fit** - High R¬≤, small gap between train and test\n",
        "- **High degrees (10+)**: **Overfitting** - Training R¬≤ ‚Üí 1.0, but test R¬≤ plateaus or drops. The growing gap shows the model is memorizing training data rather than learning patterns.\n",
        "\n",
        "**Key insight**: Even with material features, we can still overfit by making the model too complex!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca1813c",
      "metadata": {
        "id": "2ca1813c"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "degrees = list(range(1, 16))  # Set x-axis from 1 to 15\n",
        "ax1.plot(df_poly['Degree'], df_poly['R¬≤ Train'], 'o-', linewidth=2,\n",
        "         markersize=8, label='Training R¬≤', color='blue')\n",
        "ax1.plot(df_poly['Degree'], df_poly['R¬≤ Test'], 's-', linewidth=2,\n",
        "         markersize=8, label='Test R¬≤', color='red')\n",
        "ax1.fill_between(df_poly['Degree'], df_poly['R¬≤ Test'], df_poly['R¬≤ Train'],\n",
        "                  alpha=0.2, color='orange', label='Overfitting gap')\n",
        "ax1.set_xlabel('Polynomial Degree', fontsize=12)\n",
        "ax1.set_ylabel('R¬≤ Score', fontsize=12)\n",
        "ax1.set_title('Bias-Variance Tradeoff\\nR¬≤ vs Model Complexity',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10, loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axhline(0.95, color='green', linestyle='--', alpha=0.3)\n",
        "ax1.set_xticks(degrees)\n",
        "ax2.plot(df_poly['Degree'], df_poly['RMSE Train (MPa)'], 'o-', linewidth=2,\n",
        "         markersize=8, label='Training RMSE', color='blue')\n",
        "ax2.plot(df_poly['Degree'], df_poly['RMSE Test (MPa)'], 's-', linewidth=2,\n",
        "         markersize=8, label='Test RMSE', color='red')\n",
        "ax2.set_xlabel('Polynomial Degree', fontsize=12)\n",
        "ax2.set_ylabel('RMSE (MPa)', fontsize=12)\n",
        "ax2.set_title('Prediction Error\\nRMSE vs Model Complexity',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.legend(fontsize=10, loc='best')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(degrees)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "optimal_idx = df_poly['R¬≤ Test'].idxmax()\n",
        "optimal_degree = df_poly.loc[optimal_idx, 'Degree']\n",
        "optimal_r2 = df_poly.loc[optimal_idx, 'R¬≤ Test']\n",
        "print(f\"\\nüèÜ Optimal polynomial degree: {optimal_degree}\")\n",
        "print(f\"   Test R¬≤: {optimal_r2:.4f}\")\n",
        "print(f\"   Interpretation: Degree {optimal_degree} balances bias and variance optimally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af0cebb",
      "metadata": {
        "id": "3af0cebb"
      },
      "source": [
        "### What to Expect\n",
        "\n",
        "Before we dive into ridge regularization on high‚Äëdegree polynomials, we will clarify the goal. We will train a high‚Äëdegree (degree¬†10) polynomial model on the multi‚Äëspecimen dataset using different values of the regularization strength Œª.  We will then plot training and test R¬≤ as a function of Œª, and inspect how the model parameters (slope and intercept) shrink with increasing Œª. This helps you see how regularization controls overfitting in flexible polynomial models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0641eb19",
      "metadata": {
        "id": "0641eb19"
      },
      "source": [
        "### Regularization for High-Degree Polynomials\n",
        "\n",
        "Now we will see how L2 regularization helps when we use high-degree polynomials. We will test degree 20 (which showed overfitting) with different regularization strengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10c5bea",
      "metadata": {
        "id": "e10c5bea"
      },
      "outputs": [],
      "source": [
        "# --- Regularization for High-Degree Polynomials (degree=15) on multi-specimen data ---\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "test_degree = 15  # A high degree to demonstrate overfitting\n",
        "lambda_values_ridge = [0, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0] # A more focused lambda grid\n",
        "\n",
        "# Use the same grouped split from the previous section\n",
        "X_train_poly_ridge, X_test_poly_ridge, y_train_poly_ridge, y_test_poly_ridge, _, _ = grouped_split(\n",
        "    df_multi,\n",
        "    ['strain', 'carbon_content', 'grain_size_inv_sqrt'],\n",
        "    'stress',\n",
        "    test_size=0.25,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "def pipe_template(alpha):\n",
        "    return make_poly_ridge_pipeline(test_degree, alpha)\n",
        "\n",
        "results_ridge = []\n",
        "for lam in lambda_values_ridge:\n",
        "    # Use a tiny alpha instead of 0 for the \"unregularized\" case to avoid singularity\n",
        "    alpha = 1e-9 if lam == 0 else lam\n",
        "\n",
        "    pipe = pipe_template(alpha)\n",
        "    pipe.fit(X_train_poly_ridge, y_train_poly_ridge)\n",
        "\n",
        "    ytr = pipe.predict(X_train_poly_ridge)\n",
        "    yte = pipe.predict(X_test_poly_ridge)\n",
        "\n",
        "    r2_tr = r2_score(y_train_poly_ridge, ytr)\n",
        "    r2_te = r2_score(y_test_poly_ridge,  yte)\n",
        "\n",
        "    results_ridge.append({\n",
        "        \"Œª\": lam, # Report the intended lambda (0 for OLS)\n",
        "        \"R¬≤ Train\": r2_tr,\n",
        "        \"R¬≤ Test\": r2_te,\n",
        "        \"Gap\": r2_tr - r2_te\n",
        "    })\n",
        "\n",
        "df_ridge = pd.DataFrame(results_ridge)\n",
        "print(f\"üìä Regularization Effect on Degree-{test_degree} Polynomial\")\n",
        "print(\"=\" * 70)\n",
        "print(df_ridge.to_string(index=False))\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.semilogx(df_ridge[\"Œª\"].replace(0, 1e-7), df_ridge[\"R¬≤ Train\"], 'o-', label='Train R¬≤') # Plot lambda=0 at a small value\n",
        "plt.semilogx(df_ridge[\"Œª\"].replace(0, 1e-7), df_ridge[\"R¬≤ Test\"], 's-', label='Test R¬≤')\n",
        "plt.xlabel('Regularization Strength (Œª)')\n",
        "plt.ylabel('R¬≤ Score')\n",
        "plt.title(f'Degree {test_degree}: R¬≤ vs. Regularization')\n",
        "plt.grid(True, which='both', alpha=0.3)\n",
        "plt.legend()\n",
        "plt.ylim(-0.5, 1.1) # Adjust ylim to see the negative R¬≤ for the unregularized case\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8884637",
      "metadata": {
        "id": "c8884637"
      },
      "source": [
        "### Visualizing Regularization Effect on Polynomials\n",
        "\n",
        "We will plot the fitted curves with and without regularization to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c4d229",
      "metadata": {
        "id": "79c4d229"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# SELECT ONE SPECIMEN to visualize\n",
        "specimen_id = int(df_multi['specimen_id'].median())\n",
        "specimen_data = df_multi[df_multi['specimen_id'] == specimen_id]\n",
        "specimen_C = specimen_data['carbon_content'].iloc[0]\n",
        "specimen_d_inv = specimen_data['grain_size_inv_sqrt'].iloc[0]\n",
        "\n",
        "print(f\"\\nüìç Visualizing regularization effect for Specimen {specimen_id}\")\n",
        "print(f\"   Carbon content: {specimen_C:.4f}\")\n",
        "print(f\"   Grain size (1/‚àöd): {specimen_d_inv:.2f}\")\n",
        "\n",
        "# Fine grid for smooth curves\n",
        "strain_grid = np.linspace(specimen_data['strain'].min(), specimen_data['strain'].max(), 500).reshape(-1, 1)\n",
        "\n",
        "# Create feature grid: [strain, C, 1/‚àöd] for the SAME specimen\n",
        "C_grid = np.full_like(strain_grid, specimen_C)\n",
        "d_inv_grid = np.full_like(strain_grid, specimen_d_inv)\n",
        "X_grid_full = np.hstack([strain_grid, C_grid, d_inv_grid])\n",
        "\n",
        "# Use the same degree as the previous cell\n",
        "test_degree = 15\n",
        "\n",
        "# Build and fit three models at different Œª - trained on ALL specimens\n",
        "# Use a tiny alpha for the \"no regularization\" case to avoid singularity\n",
        "pipe_no_reg = make_poly_ridge_pipeline(test_degree, 1e-9)\n",
        "pipe_mid    = make_poly_ridge_pipeline(test_degree, 0.01)   # Optimal regularization\n",
        "pipe_strong = make_poly_ridge_pipeline(test_degree, 100.0)  # Strong regularization\n",
        "\n",
        "pipe_no_reg.fit(X_train_poly_ridge, y_train_poly_ridge)\n",
        "pipe_mid.fit(X_train_poly_ridge, y_train_poly_ridge)\n",
        "pipe_strong.fit(X_train_poly_ridge, y_train_poly_ridge)\n",
        "\n",
        "# Predictions on grid\n",
        "y_no_reg  = pipe_no_reg.predict(X_grid_full)\n",
        "y_optimal = pipe_mid.predict(X_grid_full)\n",
        "y_strong  = pipe_strong.predict(X_grid_full)\n",
        "\n",
        "# Plot - show ONLY the selected specimen's data\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "plt.scatter(specimen_data['strain'], specimen_data['stress'] / 1e6,\n",
        "            color='orange', alpha=0.8, s=80, edgecolors='k',\n",
        "            label=f'Specimen {specimen_id} data', zorder=4)\n",
        "\n",
        "plt.plot(strain_grid, y_no_reg / 1e6, 'r--', linewidth=2.5,\n",
        "         label=f'Degree {test_degree}, Œª‚âà0 (Overfitting)', alpha=0.9, zorder=3)\n",
        "plt.plot(strain_grid, y_optimal / 1e6, 'g-', linewidth=3.0,\n",
        "         label=f'Degree {test_degree}, Œª=0.01 (Intermediate Fit)', alpha=0.9, zorder=3)\n",
        "plt.plot(strain_grid, y_strong / 1e6, 'b-.', linewidth=2.5,\n",
        "         label=f'Degree {test_degree}, Œª=100 (Underfitting)', alpha=0.9, zorder=3)\n",
        "\n",
        "plt.xlabel('Strain', fontsize=13)\n",
        "plt.ylabel('Stress (MPa)', fontsize=13)\n",
        "plt.title(f'Effect of L2 Regularization on Degree-{test_degree} Polynomial\\n'\n",
        "          f'Predicting for Specimen {specimen_id}',\n",
        "          fontsize=13, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(bottom=0) # Ensure y-axis starts at 0 for stress\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüí° Key insight:\")\n",
        "print(f\"   ‚Ä¢ The unregularized model (red) is wildly unstable due to overfitting.\")\n",
        "print(f\"   ‚Ä¢ A small amount of regularization (Œª=0.01, green) stabilizes the fit dramatically.\")\n",
        "print(f\"   ‚Ä¢ Too much regularization (Œª=100, blue) over-simplifies the model, causing it to underfit.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c409293f",
      "metadata": {
        "id": "c409293f"
      },
      "source": [
        "---\n",
        "\n",
        "## 1.7 Interactive Feature Selection and Modeling\n",
        "\n",
        "### Combining Materials Science with Machine Learning\n",
        "\n",
        "Now you can experiment with different feature combinations to see how they affect model performance. This interactive tool lets you:\n",
        "- **Select features**: Choose which material parameters to include (strain, carbon_content, grain_size_inv_sqrt)\n",
        "- **Choose polynomial degree**: Control the complexity of strain features (Œµ, Œµ¬≤, Œµ¬≥, ...)\n",
        "- **Apply regularization**: Prevent overfitting with L2 penalty (Ridge regression)\n",
        "- **Visualize results**: See the model fit and performance metrics\n",
        "\n",
        "**Guidance from materials science**:\n",
        "- **Strain** should always be included (fundamental œÉ-Œµ relationship)\n",
        "- **Carbon content** affects strength through solid solution strengthening\n",
        "- **Grain size (1/‚àöd)** affects strength via Hall-Petch relationship\n",
        "- Higher polynomial degrees capture plastic deformation and necking\n",
        "- Regularization becomes important for high-degree polynomials\n",
        "\n",
        "**What to explore**:\n",
        "1. Start with strain only, degree 1 ‚Üí See linear underfitting\n",
        "2. Add polynomial features (degree 3-5) ‚Üí Captures nonlinearity\n",
        "3. Include material features (C, d) ‚Üí Improves predictions across specimens\n",
        "4. Increase regularization for high degrees ‚Üí Prevents overfitting\n",
        "\n",
        "Try different combinations and observe:\n",
        "- Which features improve R¬≤?\n",
        "- When does adding complexity help vs hurt?\n",
        "- How does regularization affect the bias-variance tradeoff?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de7228e",
      "metadata": {
        "id": "8de7228e"
      },
      "outputs": [],
      "source": [
        "# --- Controls ---\n",
        "feature_options = ['strain', 'carbon_content', 'grain_size_inv_sqrt']\n",
        "feature_checkboxes = [widgets.Checkbox(value=(f == 'strain'), description=f) for f in feature_options]\n",
        "\n",
        "degree_slider = widgets.IntSlider(\n",
        "    value=3, min=1, max=15, step=1,\n",
        "    description='Polynomial degree:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "lambda_slider = widgets.FloatLogSlider(\n",
        "    value=1e-2, base=10, min=-6, max=3, step=0.1,\n",
        "    description='Regularization Œª:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "button = widgets.Button(\n",
        "    description=\"üìä Make Prediction\",\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='200px', height='40px')\n",
        ")\n",
        "out = widgets.Output()\n",
        "\n",
        "# -- nuke stacked handlers if the cell is re-run --\n",
        "try:\n",
        "    button._click_handlers.callbacks = []\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def interactive_multi_specimen_model(_=None):\n",
        "    # reentrancy guard: ignore accidental duplicate clicks/buggy double events\n",
        "    if getattr(button, \"_locked\", False):\n",
        "        return\n",
        "    button._locked = True\n",
        "    button.disabled = True\n",
        "    try:\n",
        "        with out:\n",
        "            out.clear_output(wait=True)\n",
        "\n",
        "            selected_features = [cb.description for cb in feature_checkboxes if cb.value]\n",
        "            if not selected_features:\n",
        "                print(\"‚ö†Ô∏è  Please select at least one feature!\")\n",
        "                return\n",
        "\n",
        "            degree = int(degree_slider.value)\n",
        "\n",
        "            # Degree clamping when strain is not selected\n",
        "            if 'strain' not in selected_features:\n",
        "                degree = 1\n",
        "\n",
        "            reg_lambda = float(lambda_slider.value)\n",
        "\n",
        "            # Use grouped_split to prevent specimen leakage\n",
        "            X_train, X_test, y_train, y_test, _, _ = grouped_split(\n",
        "                df_multi, selected_features, 'stress', test_size=0.25, seed=42\n",
        "            )\n",
        "\n",
        "            pipe = Pipeline([\n",
        "                ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('ridge', Ridge(alpha=reg_lambda, fit_intercept=True))\n",
        "            ])\n",
        "            pipe.fit(X_train, y_train)\n",
        "\n",
        "            y_train_pred = pipe.predict(X_train)\n",
        "            y_test_pred  = pipe.predict(X_test)\n",
        "\n",
        "            r2_train = r2_score(y_train, y_train_pred)\n",
        "            r2_test  = r2_score(y_test,  y_test_pred)\n",
        "            rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred)) / 1e6\n",
        "            gap = r2_train - r2_test\n",
        "            n_features_expanded = pipe.named_steps['poly'].n_output_features_\n",
        "\n",
        "            print(f\"Features: {', '.join(selected_features)} | Degree: {degree} | Œª: {reg_lambda:.1e} | Total features: {n_features_expanded}\")\n",
        "            print(f\"R¬≤ Train: {r2_train:.4f} | R¬≤ Test: {r2_test:.4f} | Gap: {gap:.4f} | RMSE: {rmse_test:.1f} MPa\")\n",
        "            if gap > 0.1:\n",
        "                print(\"‚ö†Ô∏è Overfitting ‚Äî increase Œª or decrease degree\")\n",
        "            elif r2_test < 0.7:\n",
        "                print(\"‚ö†Ô∏è Underfitting ‚Äî add features or increase degree\")\n",
        "            else:\n",
        "                print(\"‚úÖ Good balance\")\n",
        "\n",
        "            # Create figure with a unique approach to prevent double display\n",
        "            from matplotlib.figure import Figure\n",
        "            fig = Figure(figsize=(15, 6))\n",
        "            axes = fig.subplots(1, 2)\n",
        "\n",
        "            # Use the Figure directly instead of plt.subplots to avoid pyplot global state\n",
        "            ax1, ax2 = axes\n",
        "\n",
        "            # Left: y_true vs y_pred\n",
        "            ax1.scatter(y_test/1e6, y_test_pred/1e6, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)\n",
        "            lims = [min(y_test.min(), y_test_pred.min())/1e6, max(y_test.max(), y_test_pred.max())/1e6]\n",
        "            ax1.plot(lims, lims, 'r--', linewidth=2, alpha=0.7, label='Perfect')\n",
        "            ax1.set_xlabel('True Stress (MPa)', fontsize=12)\n",
        "            ax1.set_ylabel('Predicted Stress (MPa)', fontsize=12)\n",
        "            ax1.set_title(f'Test Set: R¬≤ = {r2_test:.4f}', fontsize=13, fontweight='bold')\n",
        "            ax1.legend(fontsize=10)\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            ax1.set_aspect('equal', adjustable='box')\n",
        "\n",
        "            # Right: specimen curve if strain present, otherwise residuals\n",
        "            if 'strain' in selected_features:\n",
        "                specimen_id = int(df_multi[\"specimen_id\"].median())\n",
        "                d = df_multi[df_multi['specimen_id'] == specimen_id]\n",
        "                fixed = {f: d[f].iloc[0] for f in selected_features}\n",
        "                strain_grid = np.linspace(d['strain'].min(), d['strain'].max(), 500)\n",
        "                if len(selected_features) == 1:\n",
        "                    Xg = strain_grid.reshape(-1, 1)\n",
        "                else:\n",
        "                    cols = [strain_grid.reshape(-1, 1) if f == 'strain'\n",
        "                            else np.full((500, 1), fixed[f]) for f in selected_features]\n",
        "                    Xg = np.hstack(cols)\n",
        "                yg = pipe.predict(Xg)\n",
        "                ax2.scatter(d['strain'], d['stress']/1e6, color='orange', alpha=0.7,\n",
        "                            s=60, edgecolors='k', label=f'Specimen {specimen_id}', zorder=4)\n",
        "                ax2.plot(strain_grid, yg/1e6, linewidth=2.5, label='Model', zorder=3)\n",
        "                ax2.set_xlabel('Strain', fontsize=12)\n",
        "                ax2.set_ylabel('Stress (MPa)', fontsize=12)\n",
        "                ax2.set_title(f'Specimen {specimen_id} Fit', fontsize=13, fontweight='bold')\n",
        "                ax2.legend(fontsize=10, loc='lower right')\n",
        "            else:\n",
        "                residuals = (y_test - y_test_pred)/1e6\n",
        "                ax2.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "                ax2.axvline(0, linestyle='--', linewidth=2)\n",
        "                ax2.set_xlabel('Residuals (MPa)', fontsize=12)\n",
        "                ax2.set_ylabel('Frequency', fontsize=12)\n",
        "                ax2.set_title('Residual Distribution', fontsize=13, fontweight='bold')\n",
        "\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            fig.tight_layout()\n",
        "\n",
        "            # Display the figure - this will now only show once\n",
        "            display(fig)\n",
        "\n",
        "    finally:\n",
        "        button._locked = False\n",
        "        button.disabled = False\n",
        "\n",
        "button.on_click(interactive_multi_specimen_model)\n",
        "\n",
        "# UI\n",
        "feature_box = widgets.VBox([widgets.HTML(\"<b>Select Features:</b>\"),\n",
        "                            widgets.HBox(feature_checkboxes)])\n",
        "controls_box = widgets.VBox([feature_box, degree_slider, lambda_slider, button, out])\n",
        "display(controls_box)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13ea8b8",
      "metadata": {
        "id": "c13ea8b8"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusions: Part 1\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways:\n",
        "\n",
        "1. **Physics-aware features provide persistent value**: Adding `carbon_content` and `grain_size_inv_sqrt` gave a consistent R¬≤ lift across all polynomial degrees, because they encode specimen-to-specimen differences that strain alone cannot capture.\n",
        "\n",
        "2. **Regularization helps capacity-rich models, not underfit ones**: Ridge regression is powerful when you have enough features to overfit. Applying it to a too-simple linear model (strain-only) does not improve performance - it just shrinks an already-inadequate slope.\n",
        "\n",
        "3. **Group-aware validation is non-negotiable**: Points from the same specimen share chemistry and microstructure. Random row splits leak this information and inflate test metrics. Always use `GroupShuffleSplit` or similar grouped cross-validation for multi-curve materials data.\n",
        "\n",
        "These principles generalize beyond stress-strain curves to any materials dataset with specimen-level heterogeneity.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13f64d0",
      "metadata": {
        "id": "e13f64d0"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Logistic Regression for Multi-Axial Yield Prediction\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Multi-Axial Loading and Failure Criteria\n",
        "\n",
        "### Engineering Problem\n",
        "\n",
        "Structural components experience complex stress states (œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÉ) from combined loading. Predicting yield initiation requires:\n",
        "1. A scalar effective stress œÉ_eff from the stress tensor\n",
        "2. A failure criterion: œÉ_eff ‚â• œÉ_y\n",
        "\n",
        "### Binary Classification Formulation\n",
        "\n",
        "**Input**: Principal stresses (œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÉ)\n",
        "**Output**: Material state ‚àà {Elastic (0), Plastic (1)}\n",
        "\n",
        "### Physics-Informed Approach\n",
        "\n",
        "The von Mises criterion provides a theoretical decision boundary. Our goal: determine if logistic regression can learn this nonlinear boundary from experimental data alone.\n",
        "\n",
        "**Key Questions**:\n",
        "1. Can logistic regression discover the quadratic von Mises relationship?\n",
        "2. How well does the learned boundary match theoretical predictions?\n",
        "3. How should decision thresholds be selected for safety-critical applications?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fba98e",
      "metadata": {
        "id": "05fba98e"
      },
      "source": [
        "## 2.2 Von Mises Yield Criterion\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For a general 3D stress state with **principal stresses** œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÉ (stresses in the directions where shear stress is zero), the von Mises equivalent stress is:\n",
        "\n",
        "$$\\sigma_{VM} = \\sqrt{\\frac{1}{2}[(\\sigma_1 - \\sigma_2)^2 + (\\sigma_2 - \\sigma_3)^2 + (\\sigma_3 - \\sigma_1)^2]}$$\n",
        "\n",
        "For **2D plane stress** (thin plates where œÉ‚ÇÉ = 0), this simplifies to:\n",
        "\n",
        "$$\\sigma_{VM} = \\sqrt{\\sigma_1^2 - \\sigma_1 \\sigma_2 + \\sigma_2^2}$$\n",
        "\n",
        "**Physical interpretation**:\n",
        "- œÉ_VM represents the \"effective\" stress\n",
        "- It measures the magnitude of distortion energy in the material\n",
        "- Materials yield when this effective stress exceeds the yield strength\n",
        "\n",
        "### Yield Condition\n",
        "\n",
        "The material yields when:\n",
        "$$\\sigma_{VM} \\geq \\sigma_y$$\n",
        "\n",
        "where œÉ_y is the uniaxial yield strength (from a simple tensile test).\n",
        "\n",
        "### Decision Boundary Formulation\n",
        "\n",
        "Rearranging the yield condition as a decision boundary:\n",
        "$$f(\\sigma_1, \\sigma_2) = \\sigma_1^2 - \\sigma_1 \\sigma_2 + \\sigma_2^2 - \\sigma_y^2$$\n",
        "\n",
        "**Interpretation**:\n",
        "- **f < 0**: Inside yield surface ‚Üí Elastic (Class 0)\n",
        "- **f = 0**: On yield surface ‚Üí At yield point\n",
        "- **f > 0**: Outside yield surface ‚Üí Plastic (Class 1)\n",
        "\n",
        "### Geometric Shape\n",
        "\n",
        "In the œÉ‚ÇÅ-œÉ‚ÇÇ stress space, this equation defines an **ellipse** centered at the origin. This is called the **yield surface** or **yield locus**.\n",
        "\n",
        "**Key properties**:\n",
        "- Symmetric about both axes (material yields equally in tension and compression)\n",
        "- Passes through (œÉ_y, 0) and (0, œÉ_y) points\n",
        "- Maximum stress œÉ_y occurs in uniaxial loading\n",
        "- Smaller yield stress required when both stresses are applied simultaneously\n",
        "\n",
        "### Before We Code: Predictions\n",
        "\n",
        "What should we expect when we generate experimental data and fit a machine learning model?\n",
        "\n",
        "1. **Data distribution**: Points should cluster inside (elastic) and outside (plastic) the theoretical ellipse\n",
        "2. **Model learning**: Logistic regression should discover the quadratic relationship (œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤)\n",
        "3. **Decision boundary**: Learned boundary should approximate the von Mises ellipse\n",
        "4. **Feature importance**: œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤ should have the strongest coefficients\n",
        "\n",
        "We will test these predictions empirically!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c6178c",
      "metadata": {
        "id": "14c6178c"
      },
      "source": [
        "## 2.3 Multi-Axial Loading Data Generation\n",
        "\n",
        "### Experimental Setup\n",
        "\n",
        "We simulate a series of biaxial tension tests where specimens are loaded with different combinations of principal stresses œÉ‚ÇÅ and œÉ‚ÇÇ. For each test, we record whether the specimen yielded.\n",
        "\n",
        "**Data generation strategy**:\n",
        "1. Sample stress combinations uniformly in œÉ‚ÇÅ-œÉ‚ÇÇ space\n",
        "2. Calculate von Mises equivalent stress for each combination\n",
        "3. Label as yield (1) if œÉ_VM > œÉ_y, elastic (0) otherwise\n",
        "4. Add measurement noise to simulate real experimental variability\n",
        "\n",
        "This creates a realistic dataset that mirrors actual multi-axial testing campaigns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd60d3f",
      "metadata": {
        "id": "8cd60d3f"
      },
      "outputs": [],
      "source": [
        "# Generate biaxial loading data (using sigma_y_failure from config)\n",
        "n_samples = 500\n",
        "np.random.seed(42)\n",
        "\n",
        "# Sample principal stresses uniformly\n",
        "sigma1_data = np.random.uniform(-1.5 * sigma_y_failure, 1.5 * sigma_y_failure, n_samples)\n",
        "sigma2_data = np.random.uniform(-1.5 * sigma_y_failure, 1.5 * sigma_y_failure, n_samples)\n",
        "\n",
        "# Calculate von Mises stress\n",
        "def von_mises_stress(s1, s2):\n",
        "    \"\"\"Calculate von Mises equivalent stress for plane stress (sigma_3=0).\"\"\"\n",
        "    return np.sqrt(s1**2 - s1*s2 + s2**2)\n",
        "\n",
        "sigma_vm_data = von_mises_stress(sigma1_data, sigma2_data)\n",
        "\n",
        "# Label data: 1 if yielded (sigma_VM > sigma_y), 0 otherwise\n",
        "yield_labels = (sigma_vm_data > sigma_y_failure).astype(int)\n",
        "\n",
        "# Add some experimental noise to the boundary\n",
        "# In real experiments, there is variability in when yielding is detected\n",
        "noise_factor = 0.05  # 5% uncertainty in yield detection\n",
        "for i in range(n_samples):\n",
        "    # For points near the boundary, add some classification noise\n",
        "    distance_to_boundary = abs(sigma_vm_data[i] - sigma_y_failure) / sigma_y_failure\n",
        "\n",
        "    if distance_to_boundary < noise_factor:\n",
        "        # Flip label with some probability for boundary points\n",
        "        if np.random.rand() < 0.2:\n",
        "            yield_labels[i] = 1 - yield_labels[i]\n",
        "\n",
        "# Create DataFrame\n",
        "df_failure = pd.DataFrame({\n",
        "    'sigma1': sigma1_data,\n",
        "    'sigma2': sigma2_data,\n",
        "    'sigma_vm': sigma_vm_data,\n",
        "    'yielded': yield_labels\n",
        "})\n",
        "\n",
        "# Summary statistics\n",
        "n_elastic = (yield_labels == 0).sum()\n",
        "n_plastic = (yield_labels == 1).sum()\n",
        "\n",
        "print(\"‚úÖ Multi-axial loading data generated\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   Total experiments: {n_samples}\")\n",
        "print(f\"   Elastic (Class 0): {n_elastic} ({n_elastic/n_samples*100:.1f}%)\")\n",
        "print(f\"   Yielded (Class 1): {n_plastic} ({n_plastic/n_samples*100:.1f}%)\")\n",
        "print(f\"   Material yield strength: {sigma_y_failure/1e6:.0f} MPa\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df_failure.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50a53e",
      "metadata": {
        "id": "0c50a53e"
      },
      "source": [
        "### Visualizing the Experimental Data\n",
        "\n",
        "We will plot the experimental data along with the theoretical von Mises yield surface. This gives us a visual sense of how well real data aligns with theory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1165f2",
      "metadata": {
        "id": "1e1165f2"
      },
      "outputs": [],
      "source": [
        "# Create theoretical von Mises ellipse\n",
        "theta = np.linspace(0, 2*np.pi, 200)\n",
        "\n",
        "# Parametric equation for von Mises ellipse in 2D plane stress\n",
        "# sigma1^2 - sigma1*sigma2 + sigma2^2 = sigma_y^2\n",
        "# This can be parameterized as:\n",
        "\n",
        "# Rotation angle for von Mises ellipse\n",
        "\n",
        "# Generate ellipse points (more accurate von Mises representation)\n",
        "t = np.linspace(0, 2*np.pi, 300)\n",
        "r = sigma_y_failure / np.sqrt(1 - 0.5*np.sin(2*t))  # von Mises in polar form\n",
        "sigma1_ellipse = r * np.cos(t)\n",
        "sigma2_ellipse = r * np.sin(t)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "# Separate elastic and plastic points\n",
        "elastic_mask = df_failure['yielded'] == 0\n",
        "plastic_mask = df_failure['yielded'] == 1\n",
        "\n",
        "# Plot data points\n",
        "ax.scatter(df_failure.loc[elastic_mask, 'sigma1'] / 1e6,\n",
        "           df_failure.loc[elastic_mask, 'sigma2'] / 1e6,\n",
        "           c='blue', alpha=0.6, s=40, edgecolors='k', linewidth=0.5,\n",
        "           label='Elastic (Class 0)', marker='o')\n",
        "\n",
        "ax.scatter(df_failure.loc[plastic_mask, 'sigma1'] / 1e6,\n",
        "           df_failure.loc[plastic_mask, 'sigma2'] / 1e6,\n",
        "           c='red', alpha=0.6, s=40, linewidth=0.5,\n",
        "           label='Yielded (Class 1)', marker='x')\n",
        "\n",
        "# Plot theoretical von Mises ellipse\n",
        "ax.plot(sigma1_ellipse / 1e6, sigma2_ellipse / 1e6,\n",
        "        'k--', linewidth=3, alpha=0.8, label='Theoretical von Mises boundary')\n",
        "\n",
        "# Mark axes\n",
        "ax.axhline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "ax.axvline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "# Mark uniaxial yield points\n",
        "ax.plot([sigma_y_failure/1e6, sigma_y_failure/1e6],\n",
        "        [0, 0], 'go', markersize=10, label='Uniaxial yield points')\n",
        "ax.plot([0, 0], [sigma_y_failure/1e6, sigma_y_failure/1e6], 'go', markersize=10)\n",
        "ax.plot([-sigma_y_failure/1e6, -sigma_y_failure/1e6], [0, 0], 'go', markersize=10)\n",
        "ax.plot([0, 0], [-sigma_y_failure/1e6, -sigma_y_failure/1e6], 'go', markersize=10)\n",
        "\n",
        "ax.set_xlabel('Principal Stress œÉ‚ÇÅ (MPa)', fontsize=13)\n",
        "ax.set_ylabel('Principal Stress œÉ‚ÇÇ (MPa)', fontsize=13)\n",
        "ax.set_title('Experimental Data with Theoretical von Mises Yield Surface\\n' +\n",
        "             f'Yield Strength = {sigma_y_failure/1e6:.0f} MPa',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11, loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "# Set reasonable limits\n",
        "limit = 1.5 * sigma_y_failure / 1e6\n",
        "ax.set_xlim([-limit, limit])\n",
        "ax.set_ylim([-limit, limit])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Visual Interpretation:\")\n",
        "print(\"   - Blue circles: Material remains elastic (no permanent deformation)\")\n",
        "print(\"   - Red crosses: Material yielded (permanent deformation)\")\n",
        "print(\"   - Dashed ellipse: Theoretical prediction from von Mises criterion\")\n",
        "print(\"   - Green dots: Uniaxial yield points (¬±œÉ_y on each axis)\")\n",
        "print(\"\\nüí° Observations:\")\n",
        "print(\"   - Data clusters inside (elastic) and outside (plastic) the ellipse\")\n",
        "print(\"   - Some scatter near boundary due to experimental noise\")\n",
        "print(\"   - Shape is symmetric: material yields equally in all directions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114e01d8",
      "metadata": {
        "id": "114e01d8"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.4 Interactive Von Mises Stress Calculator\n",
        "\n",
        "### Understanding the Yield Surface\n",
        "\n",
        "Before we build a machine learning model, we will develop intuition for how the von Mises criterion works. Use this interactive tool to:\n",
        "- Move a point in œÉ‚ÇÅ-œÉ‚ÇÇ stress space\n",
        "- See the calculated von Mises equivalent stress\n",
        "- Observe whether the material would yield at that stress state\n",
        "- Visualize distance from the yield surface\n",
        "\n",
        "**Instructions**: Drag the sliders to set œÉ‚ÇÅ and œÉ‚ÇÇ. Watch how œÉ_VM changes and whether the point is inside or outside the yield ellipse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1b8185",
      "metadata": {
        "id": "7e1b8185"
      },
      "outputs": [],
      "source": [
        "def interactive_von_mises(sigma1_mpa, sigma2_mpa):\n",
        "    \"\"\"Interactive von Mises stress calculator with visualization.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Convert to Pa\n",
        "    sigma1 = sigma1_mpa * 1e6\n",
        "    sigma2 = sigma2_mpa * 1e6\n",
        "\n",
        "    # Calculate von Mises stress\n",
        "    sigma_vm = von_mises_stress(sigma1, sigma2)\n",
        "\n",
        "    # Check yield condition\n",
        "    yielded = sigma_vm > sigma_y_failure\n",
        "    safety_factor = sigma_y_failure / sigma_vm\n",
        "\n",
        "    # Decision boundary value\n",
        "    f_boundary = sigma1**2 - sigma1*sigma2 + sigma2**2 - sigma_y_failure**2\n",
        "    boundary_status = \"(INSIDE - Elastic)\" if f_boundary < 0 else \"(OUTSIDE - Plastic)\"\n",
        "\n",
        "    # Yield prediction label\n",
        "    yield_prediction = \"    MATERIAL YIELDS (Class 1)\" if yielded else \"    MATERIAL IS ELASTIC (Class 0)\"\n",
        "\n",
        "    # Visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Left plot: Stress space with yield surface\n",
        "    ax1.plot(sigma1_ellipse / 1e6, sigma2_ellipse / 1e6,\n",
        "             'k--', linewidth=2, alpha=0.8, label='von Mises yield surface')\n",
        "\n",
        "    # Current point\n",
        "    color = 'red' if yielded else 'blue'\n",
        "    marker = 'x' if yielded else 'o'\n",
        "    state_label = \"YIELDED\" if yielded else \"ELASTIC\"\n",
        "    ax1.plot(sigma1_mpa, sigma2_mpa, marker, markersize=20,\n",
        "             markeredgewidth=3, color=color,\n",
        "             label=f\"Current state: {state_label}\")\n",
        "\n",
        "    # Line from origin to point\n",
        "    ax1.plot([0, sigma1_mpa], [0, sigma2_mpa], 'g-', linewidth=2, alpha=0.5)\n",
        "\n",
        "    # Radial line to yield surface\n",
        "    if sigma1**2 + sigma2**2 > 0:\n",
        "        scale = sigma_y_failure / von_mises_stress(sigma1, sigma2)\n",
        "        sigma1_boundary = sigma1 * scale / 1e6\n",
        "        sigma2_boundary = sigma2 * scale / 1e6\n",
        "        ax1.plot([sigma1_mpa, sigma1_boundary],\n",
        "                 [sigma2_mpa, sigma2_boundary],\n",
        "                 'r--', linewidth=2, alpha=0.7, label='Distance to boundary')\n",
        "\n",
        "    ax1.axhline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "    ax1.axvline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    ax1.set_xlabel('œÉ‚ÇÅ (MPa)', fontsize=12)\n",
        "    ax1.set_ylabel('œÉ‚ÇÇ (MPa)', fontsize=12)\n",
        "    ax1.set_title('Principal Stress Space', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_aspect('equal', adjustable='box')\n",
        "    limit = 500\n",
        "    ax1.set_xlim([-limit, limit])\n",
        "    ax1.set_ylim([-limit, limit])\n",
        "\n",
        "    # Right plot: Information panel\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # Build clean info text\n",
        "    info_text = \"\\n\" + \"=\" * 60 + \"\\n\"\n",
        "    info_text += \" VON MISES STRESS ANALYSIS\\n\"\n",
        "    info_text += \"=\" * 60 + \"\\n\\n\"\n",
        "\n",
        "    info_text += \" INPUT STRESS STATE:\\n\"\n",
        "    info_text += f\"   œÉ‚ÇÅ = {sigma1_mpa:.1f} MPa\\n\"\n",
        "    info_text += f\"   œÉ‚ÇÇ = {sigma2_mpa:.1f} MPa\\n\\n\"\n",
        "\n",
        "    info_text += \" CALCULATED VON MISES STRESS:\\n\"\n",
        "    info_text += f\"   œÉ_VM = {sigma_vm/1e6:.1f} MPa\\n\\n\"\n",
        "\n",
        "    info_text += \" MATERIAL YIELD STRENGTH:\\n\"\n",
        "    info_text += f\"   œÉ_y = {sigma_y_failure/1e6:.0f} MPa\\n\\n\"\n",
        "\n",
        "    info_text += \" DECISION BOUNDARY VALUE:\\n\"\n",
        "    info_text += f\"   f(œÉ‚ÇÅ,œÉ‚ÇÇ) = {f_boundary/1e12:.2f} √ó 10¬π¬≤ Pa¬≤\\n\"\n",
        "    info_text += f\"   {boundary_status}\\n\\n\"\n",
        "\n",
        "    info_text += \" SAFETY FACTOR:\\n\"\n",
        "    info_text += f\"   SF = {safety_factor:.2f}\\n\\n\"\n",
        "\n",
        "    info_text += \" YIELD PREDICTION:\\n\"\n",
        "    info_text += f\" {yield_prediction}\\n\\n\"\n",
        "\n",
        "    info_text += \"-\" * 60 + \"\\n\"\n",
        "    info_text += \" INTERPRETATION\\n\"\n",
        "    info_text += \"-\" * 60 + \"\\n\"\n",
        "\n",
        "    if yielded:\n",
        "        info_text += f\" Equivalent stress ({sigma_vm/1e6:.1f} MPa) EXCEEDS\\n\"\n",
        "        info_text += f\" yield strength ({sigma_y_failure/1e6:.0f} MPa).\\n\"\n",
        "        info_text += \" ‚Üí PERMANENT DEFORMATION WILL OCCUR\\n\"\n",
        "    else:\n",
        "        info_text += f\" Equivalent stress ({sigma_vm/1e6:.1f} MPa) is BELOW\\n\"\n",
        "        info_text += f\" yield strength ({sigma_y_failure/1e6:.0f} MPa).\\n\"\n",
        "        info_text += \" ‚Üí Material remains ELASTIC\\n\"\n",
        "\n",
        "    info_text += f\"\\n Safety margin: {abs(safety_factor - 1)*100:.1f}%\\n\"\n",
        "    info_text += \"=\" * 60 + \"\\n\"\n",
        "\n",
        "    ax2.text(0.05, 0.5, info_text, fontsize=10, family='monospace',\n",
        "             verticalalignment='center', transform=ax2.transAxes,\n",
        "             bbox=dict(boxstyle='round,pad=1', facecolor='lightblue' if not yielded else 'lightcoral',\n",
        "                      edgecolor='black', linewidth=2))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive sliders\n",
        "sigma1_slider = widgets.FloatSlider(\n",
        "    value=200, min=-450, max=450, step=10,\n",
        "    description='œÉ‚ÇÅ (MPa):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "sigma2_slider = widgets.FloatSlider(\n",
        "    value=150, min=-450, max=450, step=10,\n",
        "    description='œÉ‚ÇÇ (MPa):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "print(\"üéÆ Interactive von Mises Stress Calculator\")\n",
        "print(\"   Move the sliders to explore different stress states!\")\n",
        "print()\n",
        "interact(interactive_von_mises, sigma1_mpa=sigma1_slider, sigma2_mpa=sigma2_slider)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2863e61",
      "metadata": {
        "id": "c2863e61"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.5 Manual Decision Boundary Optimization\n",
        "\n",
        "### Understanding the Learning Problem\n",
        "\n",
        "In machine learning, we do not manually position the decision boundary. Instead, the algorithm learns it from data by minimizing a **cost function**. But before we let the algorithm do this automatically, we will manually try to find a good boundary ourselves.\n",
        "\n",
        "**Your challenge**: Move and rotate the ellipse to minimize misclassification cost. This hands-on experience will help you understand:\n",
        "- What makes one decision boundary better than another\n",
        "- How cost functions quantify model performance\n",
        "- Why optimization algorithms are needed for complex problems\n",
        "\n",
        "**Cost function preview**: For each point, the cost increases if:\n",
        "- Elastic point (blue) is classified as yielded (outside ellipse)\n",
        "- Yielded point (red) is classified as elastic (inside ellipse)\n",
        "\n",
        "Try to minimize the total cost displayed in the title!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b9c9af",
      "metadata": {
        "id": "23b9c9af"
      },
      "outputs": [],
      "source": [
        "def manual_boundary_optimizer(scale_factor, rotation_deg):\n",
        "    \"\"\"Manual optimization of decision boundary by moving/rotating ellipse.\"\"\"\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Transform ellipse with current parameters\n",
        "    rotation_rad = np.deg2rad(rotation_deg)\n",
        "\n",
        "    # Original von Mises ellipse points\n",
        "    t = np.linspace(0, 2*np.pi, 300)\n",
        "    r_original = sigma_y_failure / np.sqrt(1 - 0.5*np.sin(2*t))\n",
        "\n",
        "    # Scale the ellipse\n",
        "    r_scaled = r_original * scale_factor\n",
        "\n",
        "    # Apply rotation\n",
        "    sigma1_transformed = r_scaled * np.cos(t + rotation_rad)\n",
        "    sigma2_transformed = r_scaled * np.sin(t + rotation_rad)\n",
        "\n",
        "    # Calculate cost (misclassification count)\n",
        "    # For each data point, check if it is correctly classified\n",
        "    cost = 0\n",
        "    correct_classifications = 0\n",
        "\n",
        "    for _, row in df_failure.iterrows():\n",
        "        s1, s2 = row['sigma1'], row['sigma2']\n",
        "        true_label = row['yielded']\n",
        "\n",
        "        # Rotate point back to ellipse frame\n",
        "        s1_rot = s1 * np.cos(-rotation_rad) - s2 * np.sin(-rotation_rad)\n",
        "        s2_rot = s1 * np.sin(-rotation_rad) + s2 * np.cos(-rotation_rad)\n",
        "\n",
        "        # Check if inside transformed ellipse\n",
        "        vm_transformed = von_mises_stress(s1_rot, s2_rot) / scale_factor\n",
        "        predicted_label = 1 if vm_transformed > sigma_y_failure else 0\n",
        "\n",
        "        # Calculate cost (using log-loss style)\n",
        "        if predicted_label == true_label:\n",
        "            correct_classifications += 1\n",
        "        else:\n",
        "            cost += 1  # Simple misclassification count\n",
        "\n",
        "    accuracy = correct_classifications / len(df_failure)\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(11, 11))\n",
        "\n",
        "    # Plot data\n",
        "    elastic_mask = df_failure['yielded'] == 0\n",
        "    plastic_mask = df_failure['yielded'] == 1\n",
        "\n",
        "    ax.scatter(df_failure.loc[elastic_mask, 'sigma1'] / 1e6,\n",
        "               df_failure.loc[elastic_mask, 'sigma2'] / 1e6,\n",
        "               c='blue', alpha=0.6, s=40, edgecolors='k', linewidth=0.5,\n",
        "               label='Elastic (Class 0)', marker='o', zorder=3)\n",
        "\n",
        "    ax.scatter(df_failure.loc[plastic_mask, 'sigma1'] / 1e6,\n",
        "               df_failure.loc[plastic_mask, 'sigma2'] / 1e6,\n",
        "               c='red', alpha=0.6, s=40, linewidth=0.5,\n",
        "               label='Yielded (Class 1)', marker='x', zorder=3)\n",
        "\n",
        "    # Plot original von Mises ellipse (gray, dashed)\n",
        "    ax.plot(sigma1_ellipse / 1e6, sigma2_ellipse / 1e6,\n",
        "            'gray', linestyle=':', linewidth=2, alpha=0.5,\n",
        "            label='Original von Mises', zorder=1)\n",
        "\n",
        "    # Plot transformed ellipse (green, solid)\n",
        "    ax.plot(sigma1_transformed / 1e6, sigma2_transformed / 1e6,\n",
        "            'g-', linewidth=3, alpha=0.8,\n",
        "            label=f'Adjusted boundary (Scale={scale_factor:.2f}, Rot={rotation_deg:.0f}¬∞)',\n",
        "            zorder=2)\n",
        "\n",
        "    ax.axhline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "    ax.axvline(0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "    ax.set_xlabel('œÉ‚ÇÅ (MPa)', fontsize=12)\n",
        "    ax.set_ylabel('œÉ‚ÇÇ (MPa)', fontsize=12)\n",
        "    ax.set_title(f'Manual Boundary Optimization\\n' +\n",
        "                 f'Cost (Misclassifications): {cost}   |   Accuracy: {accuracy*100:.1f}%   |   ' +\n",
        "                 f'Goal: Minimize Cost!',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=10, loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    limit = 500\n",
        "    ax.set_xlim([-limit, limit])\n",
        "    ax.set_ylim([-limit, limit])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feedback\n",
        "    if cost < 20:\n",
        "        print(\"üèÜ Excellent! Cost < 20: You've found a nearly optimal boundary!\")\n",
        "    elif cost < 50:\n",
        "        print(\"‚úÖ Good work! Cost < 50: Boundary fits data reasonably well.\")\n",
        "    elif cost < 100:\n",
        "        print(\"üü° Moderate fit. Cost < 100: Try adjusting scale or rotation.\")\n",
        "    else:\n",
        "        print(\"üî¥ High cost. Try: scale ‚âà 1.0, rotation ‚âà 0¬∞ for von Mises data.\")\n",
        "\n",
        "    print(f\"\\nüìä Current configuration:\")\n",
        "    print(f\"   Scale factor: {scale_factor:.2f}\")\n",
        "    print(f\"   Rotation: {rotation_deg:.0f}¬∞\")\n",
        "    print(f\"   Misclassifications: {cost} / {len(df_failure)}\")\n",
        "    print(f\"   Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "# Create sliders\n",
        "scale_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.5, max=1.5, step=0.05,\n",
        "    description='Scale factor:', continuous_update=False,\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "rotation_slider = widgets.FloatSlider(\n",
        "    value=0, min=-45, max=45, step=5,\n",
        "    description='Rotation (deg):', continuous_update=False,\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "print(\"üéÆ Manual Decision Boundary Optimization\")\n",
        "print(\"   Move and rotate the ellipse to minimize misclassification cost!\")\n",
        "print(\"   The green line is your adjusted boundary.\")\n",
        "print()\n",
        "interact(manual_boundary_optimizer, scale_factor=scale_slider, rotation_deg=rotation_slider)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f0cf3a",
      "metadata": {
        "id": "a9f0cf3a"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.6 Logistic Regression for Binary Classification\n",
        "\n",
        "### From Linear to Probabilistic Prediction\n",
        "\n",
        "Logistic regression maps linear combinations to probabilities via the sigmoid function:\n",
        "\n",
        "$$h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-(\\theta^T x)}}$$\n",
        "\n",
        "**Properties**:\n",
        "- Output ‚àà (0, 1): Interpretable as P(y=1|x)\n",
        "- Decision boundary: {x : Œ∏·µÄx = 0}\n",
        "- Smooth gradient everywhere (unlike step function)\n",
        "\n",
        "### Feature Engineering for Von Mises\n",
        "\n",
        "Linear logistic regression cannot learn the nonlinear von Mises boundary. We use polynomial features:\n",
        "\n",
        "**Original**: x = [œÉ‚ÇÅ, œÉ‚ÇÇ]\n",
        "**Augmented**: x' = [1, œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤]\n",
        "\n",
        "This allows the model to learn:\n",
        "$$\\theta_0 + \\theta_1\\sigma_1 + \\theta_2\\sigma_2 + \\theta_3\\sigma_1^2 + \\theta_4\\sigma_1\\sigma_2 + \\theta_5\\sigma_2^2$$\n",
        "\n",
        "**Expected coefficients** (from von Mises: œÉ‚ÇÅ¬≤ - œÉ‚ÇÅœÉ‚ÇÇ + œÉ‚ÇÇ¬≤ = œÉ_y¬≤):\n",
        "- Œ∏‚ÇÉ ‚âà Œ∏‚ÇÖ > 0 (quadratic terms)\n",
        "- Œ∏‚ÇÑ < 0 (interaction term)\n",
        "- Œ∏‚ÇÅ ‚âà Œ∏‚ÇÇ ‚âà 0 (material isotropy)\n",
        "\n",
        "### Cross-Entropy Loss\n",
        "\n",
        "For binary classification, use log-loss instead of MSE:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$$\n",
        "\n",
        "**Why cross-entropy?**\n",
        "- MSE is non-convex for logistic regression\n",
        "- Cross-entropy is convex ‚Üí guaranteed global minimum\n",
        "- Penalizes confident wrong predictions exponentially\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Update rule has identical form to linear regression:\n",
        "\n",
        "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
        "\n",
        "but h_Œ∏(x) is now sigmoid-transformed.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ffb92e",
      "metadata": {
        "id": "c0ffb92e"
      },
      "source": [
        "### Visualizing Sigmoid and Cost Functions\n",
        "\n",
        "Before training the model, we will visualize these key mathematical components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84909f48",
      "metadata": {
        "id": "84909f48"
      },
      "outputs": [],
      "source": [
        "# Create visualization of sigmoid and cost functions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Left plot: Sigmoid function\n",
        "z_vals = np.linspace(-10, 10, 200)\n",
        "sigmoid_vals = 1 / (1 + np.exp(-z_vals))\n",
        "\n",
        "ax1.plot(z_vals, sigmoid_vals, 'b-', linewidth=3, label='œÉ(z) = 1/(1+e^(-z))')\n",
        "ax1.axhline(0.5, color='red', linestyle='--', alpha=0.7, label='Decision threshold')\n",
        "ax1.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Mark key points\n",
        "ax1.plot(0, 0.5, 'ro', markersize=10, label='œÉ(0) = 0.5')\n",
        "ax1.plot([-5, 5], [sigmoid_vals[20], sigmoid_vals[180]], 'go', markersize=8)\n",
        "\n",
        "ax1.set_xlabel('z = Œ∏·µÄx (linear combination)', fontsize=12)\n",
        "ax1.set_ylabel('œÉ(z) = P(y=1|x)', fontsize=12)\n",
        "ax1.set_title('Sigmoid Function\\nMaps linear output to probability [0,1]',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([-0.1, 1.1])\n",
        "\n",
        "# Right plot: Cost function components\n",
        "h_vals = np.linspace(0.01, 0.99, 100)  # Predicted probabilities\n",
        "\n",
        "cost_y1 = -np.log(h_vals)  # Cost when y=1\n",
        "cost_y0 = -np.log(1 - h_vals)  # Cost when y=0\n",
        "\n",
        "ax2.plot(h_vals, cost_y1, 'r-', linewidth=3, label='Cost when y=1: -log(h)')\n",
        "ax2.plot(h_vals, cost_y0, 'b-', linewidth=3, label='Cost when y=0: -log(1-h)')\n",
        "\n",
        "# Mark key points\n",
        "ax2.axvline(0.5, color='gray', linestyle='--', alpha=0.5, label='Decision threshold')\n",
        "\n",
        "ax2.set_xlabel('Predicted Probability h(x)', fontsize=12)\n",
        "ax2.set_ylabel('Cost', fontsize=12)\n",
        "ax2.set_title('Log-Loss Cost Function Components\\nPenalizes confident wrong predictions',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim([0, 5])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Interpretation:\")\n",
        "print(\"\\nLeft plot (Sigmoid):\")\n",
        "print(\"   - Maps linear combination z to probability [0, 1]\")\n",
        "print(\"   - z = 0 gives 50% probability (decision boundary)\")\n",
        "print(\"   - Large positive z ‚Üí high confidence in Class 1\")\n",
        "print(\"   - Large negative z ‚Üí high confidence in Class 0\")\n",
        "print(\"\\nRight plot (Cost function):\")\n",
        "print(\"   - Red: Cost when true label is 1 (yielded)\")\n",
        "print(\"     ‚Üí Low cost if we predict high probability (correct)\")\n",
        "print(\"     ‚Üí High cost if we predict low probability (wrong)\")\n",
        "print(\"   - Blue: Cost when true label is 0 (elastic)\")\n",
        "print(\"     ‚Üí Low cost if we predict low probability (correct)\")\n",
        "print(\"     ‚Üí High cost if we predict high probability (wrong)\")\n",
        "print(\"\\nüí° Key insight: Cost increases exponentially for confident wrong predictions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de9d7bc2",
      "metadata": {
        "id": "de9d7bc2"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.7 Model Training and Evaluation\n",
        "\n",
        "### Feature Engineering for von Mises\n",
        "\n",
        "To help logistic regression discover the quadratic von Mises relationship, we create polynomial features:\n",
        "\n",
        "**Original features**: [œÉ‚ÇÅ, œÉ‚ÇÇ]\n",
        "**Engineered features**: [1, œÉ‚ÇÅ, œÉ‚ÇÇ, œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤]\n",
        "\n",
        "This allows the model to learn:\n",
        "$$\\theta_0 + \\theta_1 \\sigma_1 + \\theta_2 \\sigma_2 + \\theta_3 \\sigma_1^2 + \\theta_4 \\sigma_1\\sigma_2 + \\theta_5 \\sigma_2^2$$\n",
        "\n",
        "**Expected result**: The learned coefficients should approximate the von Mises formula:\n",
        "$$\\sigma_1^2 - \\sigma_1\\sigma_2 + \\sigma_2^2 - \\sigma_y^2 = 0$$\n",
        "\n",
        "We will train the model and see!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fceac1",
      "metadata": {
        "id": "00fceac1"
      },
      "outputs": [],
      "source": [
        "# Prepare features and labels\n",
        "X_failure = df_failure[['sigma1', 'sigma2']].values\n",
        "y_failure = df_failure['yielded'].values\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
        "    X_failure, y_failure, test_size=0.2, random_state=42, stratify=y_failure\n",
        ")\n",
        "\n",
        "# Standardize features (important for logistic regression)\n",
        "scaler_failure = StandardScaler()\n",
        "X_train_f_scaled = scaler_failure.fit_transform(X_train_f)\n",
        "X_test_f_scaled = scaler_failure.transform(X_test_f)\n",
        "\n",
        "# Create polynomial features (degree 2)\n",
        "poly_failure = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_f_poly = poly_failure.fit_transform(X_train_f_scaled)\n",
        "X_test_f_poly = poly_failure.transform(X_test_f_scaled)\n",
        "\n",
        "print(\"üìä Feature engineering:\")\n",
        "print(f\"   Original features: {X_train_f.shape[1]}\")\n",
        "print(f\"   Polynomial features: {X_train_f_poly.shape[1]}\")\n",
        "print(f\"   Feature names: {poly_failure.get_feature_names_out(['œÉ1', 'œÉ2'])}\")\n",
        "print(f\"\\nüì¶ Data split:\")\n",
        "print(f\"   Training samples: {len(X_train_f)} ({len(y_train_f[y_train_f==1])} yielded, {len(y_train_f[y_train_f==0])} elastic)\")\n",
        "print(f\"   Test samples: {len(X_test_f)} ({len(y_test_f[y_test_f==1])} yielded, {len(y_test_f[y_test_f==0])} elastic)\")\n",
        "\n",
        "# Train logistic regression model\n",
        "print(\"\\nüîÑ Training logistic regression model...\")\n",
        "model_failure = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_failure.fit(X_train_f_poly, y_train_f)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_f = model_failure.predict(X_train_f_poly)\n",
        "y_test_pred_f = model_failure.predict(X_test_f_poly)\n",
        "\n",
        "y_train_prob_f = model_failure.predict_proba(X_train_f_poly)[:, 1]\n",
        "y_test_prob_f = model_failure.predict_proba(X_test_f_poly)[:, 1]\n",
        "\n",
        "# Evaluate performance\n",
        "train_acc_f = accuracy_score(y_train_f, y_train_pred_f)\n",
        "test_acc_f = accuracy_score(y_test_f, y_test_pred_f)\n",
        "\n",
        "train_loss_f = log_loss(y_train_f, y_train_prob_f)\n",
        "test_loss_f = log_loss(y_test_f, y_test_prob_f)\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä Model Performance:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"   Training accuracy: {train_acc_f*100:.2f}%\")\n",
        "print(f\"   Test accuracy: {test_acc_f*100:.2f}%\")\n",
        "print(f\"   Training log-loss: {train_loss_f:.4f}\")\n",
        "print(f\"   Test log-loss: {test_loss_f:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Display learned coefficients\n",
        "print(\"\\nüîç Learned Coefficients:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get feature names from polynomial features\n",
        "feature_names = list(poly_failure.get_feature_names_out(['œÉ1', 'œÉ2']))\n",
        "\n",
        "# Get all coefficients (intercept is stored separately in sklearn)\n",
        "# model_failure.coef_[0] contains coefficients for all features INCLUDING the bias term from PolynomialFeatures\n",
        "coefficients = model_failure.coef_[0]\n",
        "\n",
        "# Print in a simple, clean format\n",
        "print(f\"{'Feature':<15} {'Coefficient':>15}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Intercept':<15} {model_failure.intercept_[0]:>15.6f}\")\n",
        "for fname, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{fname:<15} {coef:>15.6f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(\"   - Intercept: Shifts the decision boundary\")\n",
        "print(\"   - œÉ1, œÉ2: Linear terms (usually small for von Mises)\")\n",
        "print(\"   - œÉ1¬≤, œÉ2¬≤: Quadratic terms (should be positive, ~equal)\")\n",
        "print(\"   - œÉ1¬∑œÉ2: Interaction term (should be negative for ellipse)\")\n",
        "print(\"\\nüéØ Expected pattern for von Mises:\")\n",
        "print(\"   œÉ1¬≤ - œÉ1¬∑œÉ2 + œÉ2¬≤ - constant = 0\")\n",
        "print(\"   Our model should have œÉ1¬≤ ‚âà œÉ2¬≤ > 0 and œÉ1¬∑œÉ2 < 0\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nüìä Confusion Matrix (Test Set):\")\n",
        "cm = confusion_matrix(y_test_f, y_test_pred_f)\n",
        "print(\"                 Predicted\")\n",
        "print(\"                 0 (Elastic)  1 (Yielded)\")\n",
        "print(f\"Actual  0 (Elastic)    {cm[0,0]:4d}         {cm[0,1]:4d}\")\n",
        "print(f\"        1 (Yielded)    {cm[1,0]:4d}         {cm[1,1]:4d}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nüìã Detailed Classification Report:\")\n",
        "print(classification_report(y_test_f, y_test_pred_f,\n",
        "                          target_names=['Elastic (0)', 'Yielded (1)']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e33091",
      "metadata": {
        "id": "67e33091"
      },
      "outputs": [],
      "source": [
        "# Degree-2 features on raw stresses for coefficient interpretability\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COEFFICIENT INTERPRETABILITY: Raw Units (No Scaling)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train degree-2 logistic regression on RAW (unscaled) stresses for physical interpretation.\n",
        "# Coefficients will differ from scaled model due to different feature scales,\n",
        "# but the PATTERN (positive quadratic, negative cross-term) reveals the von Mises structure.\n",
        "poly2_raw = PolynomialFeatures(degree=2, include_bias=True)\n",
        "X_train_poly_raw = poly2_raw.fit_transform(X_train_f)\n",
        "X_test_poly_raw = poly2_raw.transform(X_test_f)\n",
        "\n",
        "logit_raw = LogisticRegression(max_iter=1000, penalty=\"l2\", C=1.0, solver=\"lbfgs\")\n",
        "logit_raw.fit(X_train_poly_raw, y_train_f)\n",
        "\n",
        "names_raw = poly2_raw.get_feature_names_out([\"œÉ1\",\"œÉ2\"])\n",
        "print(\"\\nCoefficients (raw units, degree-2):\")\n",
        "for name, coef in zip([\"Intercept\"] + list(names_raw),\n",
        "                      np.r_[logit_raw.intercept_[0], logit_raw.coef_[0]]):\n",
        "    print(f\"  {name:>10}: {coef: .3e}\")\n",
        "\n",
        "print(\"\\nüí° Notice:\")\n",
        "print(\"  - Quadratic terms (œÉ1¬≤, œÉ2¬≤) have similar positive coefficients\")\n",
        "print(\"  - Cross term (œÉ1¬∑œÉ2) has negative coefficient\")\n",
        "print(\"  - This matches von Mises pattern: œÉ1¬≤ - œÉ1¬∑œÉ2 + œÉ2¬≤ ‚âà const\")\n",
        "print(\"\\nWe still use the scaled model for predictions (better numerics).\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82330cc1",
      "metadata": {
        "id": "82330cc1"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.8 Comparing Learned vs Theoretical Boundaries\n",
        "\n",
        "### Visualizing the Decision Boundary\n",
        "\n",
        "Now we will visualize how well the logistic regression model learned the von Mises yield criterion. We will compare:\n",
        "- **Theoretical boundary**: von Mises ellipse from materials science\n",
        "- **Learned boundary**: Decision boundary from logistic regression\n",
        "- **Confidence regions**: Probability contours showing model uncertainty\n",
        "\n",
        "If the model successfully learned the physics, the learned boundary should closely match the theoretical von Mises ellipse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192291f6",
      "metadata": {
        "id": "192291f6"
      },
      "outputs": [],
      "source": [
        "# Create meshgrid for decision boundary visualization\n",
        "x1_min, x1_max = X_failure[:, 0].min() - 50e6, X_failure[:, 0].max() + 50e6\n",
        "x2_min, x2_max = X_failure[:, 1].min() - 50e6, X_failure[:, 1].max() + 50e6\n",
        "\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                        np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "# Prepare grid points for prediction\n",
        "grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "grid_points_scaled = scaler_failure.transform(grid_points)\n",
        "grid_points_poly = poly_failure.transform(grid_points_scaled)\n",
        "\n",
        "# Predict probabilities\n",
        "Z = model_failure.predict_proba(grid_points_poly)[:, 1]\n",
        "Z = Z.reshape(xx1.shape)\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Left plot: Decision boundary with data\n",
        "contour_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "contour = ax1.contourf(xx1/1e6, xx2/1e6, Z, levels=contour_levels,\n",
        "                       cmap='RdYlBu_r', alpha=0.6)\n",
        "plt.colorbar(contour, ax=ax1, label='P(Yield)')\n",
        "\n",
        "# Decision boundary at 0.5 probability\n",
        "ax1.contour(xx1/1e6, xx2/1e6, Z, levels=[0.5], colors='green',\n",
        "            linewidths=3, linestyles='solid')\n",
        "\n",
        "# Theoretical von Mises ellipse\n",
        "ax1.plot(sigma1_ellipse / 1e6, sigma2_ellipse / 1e6,\n",
        "         'k--', linewidth=3, alpha=0.8, label='Theoretical von Mises')\n",
        "\n",
        "# Data points\n",
        "elastic_mask = df_failure['yielded'] == 0\n",
        "plastic_mask = df_failure['yielded'] == 1\n",
        "\n",
        "ax1.scatter(df_failure.loc[elastic_mask, 'sigma1'] / 1e6,\n",
        "            df_failure.loc[elastic_mask, 'sigma2'] / 1e6,\n",
        "            c='blue', alpha=0.7, s=50, edgecolors='k', linewidth=0.5,\n",
        "            label='Elastic (Class 0)', marker='o', zorder=5)\n",
        "\n",
        "ax1.scatter(df_failure.loc[plastic_mask, 'sigma1'] / 1e6,\n",
        "            df_failure.loc[plastic_mask, 'sigma2'] / 1e6,\n",
        "            c='red', alpha=0.7, s=50, linewidth=0.5,\n",
        "            label='Yielded (Class 1)', marker='x', zorder=5)\n",
        "\n",
        "# Add green line to legend\n",
        "ax1.plot([], [], 'g-', linewidth=3, label='Learned boundary (P=0.5)')\n",
        "\n",
        "ax1.set_xlabel('Principal Stress œÉ‚ÇÅ (MPa)', fontsize=12)\n",
        "ax1.set_ylabel('Principal Stress œÉ‚ÇÇ (MPa)', fontsize=12)\n",
        "ax1.set_title('Learned Decision Boundary vs Theoretical\\nContours show yield probability',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10, loc='upper left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_aspect('equal', adjustable='box')\n",
        "\n",
        "# Right plot: Probability histogram\n",
        "ax2.hist(y_test_prob_f[y_test_f == 0], bins=20, alpha=0.7, color='blue',\n",
        "         edgecolor='black', label='Elastic (Class 0)', density=True)\n",
        "ax2.hist(y_test_prob_f[y_test_f == 1], bins=20, alpha=0.7, color='red',\n",
        "         edgecolor='black', label='Yielded (Class 1)', density=True)\n",
        "\n",
        "ax2.axvline(0.5, color='green', linestyle='--', linewidth=2, label='Decision threshold')\n",
        "\n",
        "ax2.set_xlabel('Predicted Probability P(Yield)', fontsize=12)\n",
        "ax2.set_ylabel('Density', fontsize=12)\n",
        "ax2.set_title('Prediction Confidence Distribution\\nTest Set',\n",
        "              fontsize=13, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Visual Interpretation:\")\n",
        "print(\"\\nLeft plot:\")\n",
        "print(\"   - Green solid line: Learned decision boundary (P = 0.5)\")\n",
        "print(\"   - Black dashed ellipse: Theoretical von Mises criterion\")\n",
        "print(\"   - Color contours: Probability of yielding (red = high, blue = low)\")\n",
        "print(\"   - Close alignment = model learned the physics!\")\n",
        "print(\"\\nRight plot:\")\n",
        "print(\"   - Blue histogram: Predicted probabilities for elastic points\")\n",
        "print(\"   - Red histogram: Predicted probabilities for yielded points\")\n",
        "print(\"   - Good separation = model is confident in its predictions\")\n",
        "print(\"   - Overlap near 0.5 = model uncertain about boundary cases\")\n",
        "\n",
        "# Calculate how well boundaries match\n",
        "# Sample points on theoretical boundary and check prediction\n",
        "theta_sample = np.linspace(0, 2*np.pi, 100)\n",
        "r_sample = sigma_y_failure / np.sqrt(1 - 0.5*np.sin(2*theta_sample))\n",
        "boundary_points = np.column_stack([r_sample * np.cos(theta_sample),\n",
        "                                   r_sample * np.sin(theta_sample)])\n",
        "\n",
        "boundary_scaled = scaler_failure.transform(boundary_points)\n",
        "boundary_poly = poly_failure.transform(boundary_scaled)\n",
        "boundary_probs = model_failure.predict_proba(boundary_poly)[:, 1]\n",
        "\n",
        "mean_prob = np.mean(boundary_probs)\n",
        "std_prob = np.std(boundary_probs)\n",
        "\n",
        "print(f\"\\nüéØ Boundary Alignment Analysis:\")\n",
        "print(f\"   Mean probability on von Mises ellipse: {mean_prob:.3f}\")\n",
        "print(f\"   Standard deviation: {std_prob:.3f}\")\n",
        "print(f\"\\nüí° Ideal result: Mean ‚âà 0.5 (model boundary matches theory)\")\n",
        "\n",
        "if abs(mean_prob - 0.5) < 0.05:\n",
        "    print(\"   ‚úÖ Excellent alignment! Model learned von Mises criterion accurately.\")\n",
        "elif abs(mean_prob - 0.5) < 0.1:\n",
        "    print(\"   üü¢ Good alignment! Model captures the general yield behavior.\")\n",
        "else:\n",
        "    print(\"   üü° Moderate alignment. Model may need more training or features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9096379",
      "metadata": {
        "id": "c9096379"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.9 Interactive Threshold Selection\n",
        "\n",
        "### Beyond the Default 0.5 Threshold\n",
        "\n",
        "By default, logistic regression uses P = 0.5 as the decision threshold. But in engineering applications, we often want to adjust this threshold based on the consequences of different errors:\n",
        "\n",
        "**Conservative design** (safety-critical):\n",
        "- Set threshold < 0.5 (e.g., 0.3)\n",
        "- More likely to predict yield ‚Üí larger safety margins\n",
        "- Minimizes false negatives (predicting elastic when it actually yields)\n",
        "\n",
        "**Aggressive design** (cost-sensitive):\n",
        "- Set threshold > 0.5 (e.g., 0.7)\n",
        "- Less likely to predict yield ‚Üí smaller safety margins\n",
        "- Minimizes false positives (predicting yield when it actually stays elastic)\n",
        "\n",
        "### The Precision-Recall Tradeoff\n",
        "\n",
        "**Precision**: Of all predicted yields, how many actually yielded?\n",
        "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$\n",
        "\n",
        "**Recall**: Of all actual yields, how many did we predict?\n",
        "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n",
        "\n",
        "Use the interactive tool below to explore how threshold selection affects model behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Recall Is Critical for Imbalanced Classes\n",
        "\n",
        "**The problem:** When yield events are rare (e.g., 5% of samples), a naive model can achieve 95% accuracy by simply predicting \"no yield\" every time, yet catch **zero** actual failures.\n",
        "\n",
        "**Why standard metrics mislead:**\n",
        "- **Accuracy** looks high even when all positives are missed\n",
        "- **ROC-AUC** remains optimistic because true negatives dominate\n",
        "- **Precision** becomes unstable with few positive predictions\n",
        "\n",
        "**Why recall matters for safety:**\n",
        "In safety-critical applications, we must catch rare failures. **Recall** directly measures this:\n",
        "\n",
        "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} = \\frac{\\text{True Yields Caught}}{\\text{All Actual Yields}}$$\n",
        "\n",
        "The complementary metric is **False Negative Rate (FNR)**, the fraction of failures we miss:\n",
        "\n",
        "$$\\text{FNR} = 1 - \\text{Recall} = \\frac{\\text{FN}}{\\text{TP + FN}}$$\n",
        "\n",
        "**Practical strategy for imbalanced data:**\n",
        "\n",
        "1. **Set a recall target** (e.g., 95% recall = 5% FNR) to ensure you catch most failures\n",
        "2. **Use the Precision-Recall curve** instead of ROC to select thresholds‚Äîit focuses on positive class performance\n",
        "3. **Report AUPRC** (Area Under PR Curve) alongside ROC-AUC for imbalanced datasets\n",
        "4. **Use `class_weight=\"balanced\"`** in training to penalize missed positives more heavily\n",
        "5. **Choose cautious thresholds** (typically < 0.5) to boost recall when positives are scarce\n",
        "\n",
        "**Example:** For rare yield detection, you might set threshold = 0.2 to achieve 95% recall, accepting lower precision (more false alarms) as the cost of not missing actual failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1435418d",
      "metadata": {
        "id": "1435418d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "\n",
        "def interactive_threshold_selection(threshold):\n",
        "    y_pred = (y_test_prob_f >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_test_f, y_pred)\n",
        "    prec = precision_score(y_test_f, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test_f, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test_f, y_pred, zero_division=0)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test_f, y_pred).ravel()\n",
        "\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(16, 7))\n",
        "    gs = fig.add_gridspec(2, 3)\n",
        "    ax1 = fig.add_subplot(gs[:, 0])\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax4 = fig.add_subplot(gs[1, 1:])\n",
        "\n",
        "    # Decision regions\n",
        "    ax1.contourf(xx1/1e6, xx2/1e6, Z, levels=[0, threshold, 1],\n",
        "                 colors=['#a6cee3', '#fb9a99'], alpha=0.6)\n",
        "    ax1.contour(xx1/1e6, xx2/1e6, Z, levels=[threshold], colors='green', linewidths=3)\n",
        "    ax1.plot(sigma1_ellipse/1e6, sigma2_ellipse/1e6, 'k--', lw=2.5, label='von Mises (theory)')\n",
        "\n",
        "    # Test points\n",
        "    mask_correct = (y_pred == y_test_f)\n",
        "    ax1.scatter(X_test_f[mask_correct & (y_test_f==0), 0]/1e6,\n",
        "                X_test_f[mask_correct & (y_test_f==0), 1]/1e6,\n",
        "                c='blue', s=50, edgecolors='k', linewidth=0.5, marker='o', label='TN')\n",
        "    ax1.scatter(X_test_f[mask_correct & (y_test_f==1), 0]/1e6,\n",
        "                X_test_f[mask_correct & (y_test_f==1), 1]/1e6,\n",
        "                c='red', s=60, linewidth=1.0, marker='x', label='TP')\n",
        "    ax1.scatter(X_test_f[~mask_correct & (y_test_f==0), 0]/1e6,\n",
        "                X_test_f[~mask_correct & (y_test_f==0), 1]/1e6,\n",
        "                c='orange', s=70, edgecolors='k', linewidth=1.0, marker='s', label='FP')\n",
        "    ax1.scatter(X_test_f[~mask_correct & (y_test_f==1), 0]/1e6,\n",
        "                X_test_f[~mask_correct & (y_test_f==1), 1]/1e6,\n",
        "                c='purple', s=70, edgecolors='k', linewidth=1.0, marker='s', label='FN')\n",
        "    ax1.set_aspect('equal', adjustable='box')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlabel('œÉ‚ÇÅ (MPa)')\n",
        "    ax1.set_ylabel('œÉ‚ÇÇ (MPa)')\n",
        "    ax1.set_title(f'Threshold = {threshold:.2f}')\n",
        "    ax1.legend(fontsize=8, loc='upper right')\n",
        "\n",
        "    # Confusion matrix (same style, but with white text on dark)\n",
        "    cm = np.array([[tn, fp], [fn, tp]])\n",
        "    im = ax2.imshow(cm, cmap='Blues')\n",
        "    vmax = cm.max()\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            val = cm[i, j]\n",
        "            # compute normalized intensity to decide text color\n",
        "            c = im.cmap(im.norm(val))\n",
        "            luminance = 0.299*c[0] + 0.587*c[1] + 0.114*c[2]\n",
        "            text_color = 'white' if luminance < 0.5 else 'black'\n",
        "            ax2.text(j, i, f'{val}', ha='center', va='center',\n",
        "                     fontsize=14, color=text_color, fontweight='bold')\n",
        "\n",
        "    ax2.set_xticks([0, 1]); ax2.set_yticks([0, 1])\n",
        "    ax2.set_xticklabels(['Pred 0', 'Pred 1'])\n",
        "    ax2.set_yticklabels(['True 0', 'True 1'])\n",
        "    ax2.set_title(f'Acc {acc:.2f}  Prec {prec:.2f}  Rec {rec:.2f}  F1 {f1:.2f}')\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    pr_prec, pr_rec, _ = precision_recall_curve(y_test_f, y_test_prob_f)\n",
        "    ax3.plot(pr_rec, pr_prec, lw=2)\n",
        "    ax3.scatter([rec], [prec], s=60)\n",
        "    ax3.set_xlabel('Recall'); ax3.set_ylabel('Precision')\n",
        "    ax3.set_title('Precision-Recall')\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, roc_thr = roc_curve(y_test_f, y_test_prob_f)\n",
        "    ax4.plot(fpr, tpr, lw=2, label=f'AUC = {auc(fpr, tpr):.3f}')\n",
        "    idx = np.argmin(np.abs(roc_thr - threshold))\n",
        "    ax4.scatter([fpr[idx]], [tpr[idx]], s=60)\n",
        "    ax4.plot([0,1],[0,1],'k--', alpha=0.3)\n",
        "    ax4.set_xlabel('FPR'); ax4.set_ylabel('TPR')\n",
        "    ax4.set_title('ROC')\n",
        "    ax4.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "threshold_slider = widgets.FloatSlider(\n",
        "    value=0.5, min=0.05, max=0.95, step=0.05,\n",
        "    description='Decision threshold:', continuous_update=False,\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='500px')\n",
        ")\n",
        "print(\"Threshold selection - move the slider to trade precision and recall.\")\n",
        "interact(interactive_threshold_selection, threshold=threshold_slider)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sfwl0RCxhwh7",
      "metadata": {
        "id": "Sfwl0RCxhwh7"
      },
      "source": [
        "### How to read the ROC panel (what it shows here)\n",
        "\n",
        "- **What it plots:** the **Receiver Operating Characteristic (ROC)** curve traces model performance as the decision **threshold sweeps from 1 ‚Üí 0**.  \n",
        "  Each point is a pair **(FPR, TPR)** at a particular threshold.\n",
        "- **The dashed diagonal** is random guessing. Curves closer to the **top-left** are better (high TPR at low FPR).  \n",
        "- **AUC (area under the curve)** summarizes performance over **all thresholds**:  \n",
        "  1.0 = perfect, 0.5 = random.\n",
        "- **The dot on the curve** marks the **current threshold** from the slider. Moving the slider slides this dot, trading **missed yields (FN)** against **false alarms (FP)**.\n",
        "\n",
        "**How to use it to pick a threshold**\n",
        "- If you must **limit false alarms**, pick the smallest threshold that keeps **FPR ‚â§ target** (e.g., 5%) while giving acceptable TPR.  \n",
        "- If you want a single default choice, a common rule is to maximize **Youden‚Äôs J = TPR ‚àí FPR** (point farthest above the diagonal)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3de0a5",
      "metadata": {
        "id": "4b3de0a5"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusions: Key Insights and Engineering Applications\n",
        "\n",
        "---\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "\n",
        "### Part 2: Logistic Regression for Binary Classification\n",
        "\n",
        "**Core principle**: Model probability of binary outcomes using sigmoid function and optimize with log-loss cost function.\n",
        "\n",
        "**Key insights**:\n",
        "1. **Sigmoid maps linear output to probabilities**: Smooth, differentiable function suitable for optimization\n",
        "2. **Log-loss penalizes confident wrong predictions**: Convex cost function with single global minimum\n",
        "3. **Polynomial features enable learning complex boundaries**: Model discovered quadratic von Mises relationship from data\n",
        "4. **Decision boundary approximates physics**: Learned ellipse closely matches theoretical von Mises criterion\n",
        "5. **Threshold selection affects error types**: Can tune for conservative (safety) vs aggressive (cost) design\n",
        "\n",
        "**Practical lesson**: Machine learning can discover physical relationships from data, but domain knowledge (von Mises theory) helps interpret and validate results.\n",
        "\n",
        "---\n",
        "\n",
        "## The Most Important Lesson\n",
        "\n",
        "### Machine Learning + Domain Knowledge = Powerful Engineering Tool\n",
        "\n",
        "Throughout this notebook, we saw this synergy in action:\n",
        "\n",
        "**Physics guided feature selection**:\n",
        "- Hall-Petch relationship (1/‚àögrain_size) based on dislocation theory\n",
        "- Polynomial strain terms (Œµ¬≤, Œµ¬≥) matching plastic deformation physics\n",
        "- Quadratic stress terms (œÉ‚ÇÅ¬≤, œÉ‚ÇÅœÉ‚ÇÇ, œÉ‚ÇÇ¬≤) from von Mises distortion energy\n",
        "\n",
        "**Materials science informed model architecture**:\n",
        "- Degree 3-5 polynomials match typical stress-strain curvature\n",
        "- Quadratic features sufficient for yield surface (no need for higher orders)\n",
        "- Symmetric ellipse shape consistent with isotropic material behavior\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways for Engineering Practice\n",
        "\n",
        "### 1. Start with Physical Understanding\n",
        "Before applying ML, understand:\n",
        "- What physical processes govern the system?\n",
        "- What are the relevant length/time scales?\n",
        "- What constraints must the solution satisfy?\n",
        "\n",
        "This prevents \"garbage in, garbage out\" and enables intelligent feature engineering.\n",
        "\n",
        "### 2. Validate Against Known Physics\n",
        "ML models should:\n",
        "- Reproduce theoretical results in idealized conditions\n",
        "- Respect conservation laws and physical constraints\n",
        "- Provide interpretable coefficients matching physical expectations\n",
        "- Fail gracefully when extrapolating beyond training data\n",
        "\n",
        "### 3. Quantify Uncertainty\n",
        "Engineering decisions require uncertainty quantification:\n",
        "- Train-test split assesses generalization\n",
        "- Regularization prevents overconfidence\n",
        "- Probability outputs (logistic regression) enable risk analysis\n",
        "- Confidence intervals for predictions inform safety factors\n",
        "\n",
        "### 4. Iterate Based on Physical Insights\n",
        "If model performance is poor:\n",
        "- Check if underfitting (too simple) or overfitting (too complex)\n",
        "- Add physics-based features before arbitrary transformations\n",
        "- Use error patterns to diagnose systematic issues\n",
        "- Collect more data in regions where model struggles\n",
        "\n",
        "**Remember**:\n",
        "- Start simple, add complexity only when needed\n",
        "- Regularization prevents overfitting, not underfitting\n",
        "- Feature engineering often matters more than algorithm choice\n",
        "- Validation against physical principles catches spurious correlations\n",
        "- Uncertainty quantification enables responsible engineering decisions\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}