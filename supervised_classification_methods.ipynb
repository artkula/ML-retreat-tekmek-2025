{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustafbjurstam/ML-retreat-tekmek-2025/blob/main/supervised_classification_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "1. **Supervised Classification Methods**    \n",
        "   1.1 What does Supervised Classification mean?  \n",
        "   1.2 Roadmap  \n",
        "2. **Motivating Example: Fluid Flow Classification**  \n",
        "   2.1 Problem setup  \n",
        "   2.2 Definitions  \n",
        "3. **Logistic Regression**  \n",
        "   3.1 Quick refresher  \n",
        "   3.2 Fluid velocity profile application\n",
        "4. **k-Nearest Neighbors (k-NN)**  \n",
        "   4.1 Quick refresher  \n",
        "   4.2 Fluid velocity profile application  \n",
        "5. **Support Vector Machine (SVM)**  \n",
        "   5.1 Hard and Soft Margins  \n",
        "   5.2 Non-linear data  \n",
        "6. **Multi-Layer Perceptron (MLP)**  \n",
        "\n",
        "---\n",
        "\n",
        "## What does \"Supervised Classification\" mean?\n",
        "\n",
        "In **supervised learning**, each training example is a pair $(\\mathbf{x}_i, y_i)$ where $\\mathbf{x}_i$ is a feature vector and $y_i$ is a **known label**. The goal is to learn a function $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that **generalizes**: it predicts accurate labels for **new, unseen** inputs drawn from the same data-generating process.\n",
        "\n",
        "In **classification**, $y_i$ takes values in a **finite set** of categories (e.g., laminar-like / transitional / turbulent-like).\n",
        "\n",
        "**Our setting.** Each sample is a 1D velocity profile $u(y)$ sampled on a fixed grid (a ~50-D feature vector). We construct a **continuum of regimes** by mixing laminar and turbulent baselines with a parameter $\\alpha \\in [0,1]$ and then discretize into three labels: laminar-like, transitional, turbulent-like. This gives an interpretable **difficulty knob**: predictions near $\\alpha \\approx 0.5$ are inherently harder.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Roadmap\n",
        "\n",
        "For each classification method, we'll follow a consistent structure:\n",
        "1. **Intuition and how it works** — Understand or recall the basic idea behind the method, key mechanics and parameters.\n",
        "2. **Fluid velocity profile application** — We will test the performance of different methods using this example.\n",
        "\n",
        "By the end of this notebook, you'll have an intuitive and practical understanding of:\n",
        "- Logistic Regression  \n",
        "- k-Nearest Neighbors (k-NN)\n",
        "- Support Vector Machine (SVM)\n",
        "- Multi-Layer Perceptron (MLP)\n",
        "- Classification methods comparison\n"
      ],
      "metadata": {
        "id": "hbRiB0s1bDXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup: Import all required libraries\n",
        "\n",
        "# Core scientific computing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Interactive widgets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, FloatSlider, FloatLogSlider\n",
        "from IPython.display import display\n",
        "\n",
        "# Machine learning - datasets\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Machine learning - preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Machine learning - pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Machine learning - models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Machine learning - model selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Machine learning - metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "C-Cd_CoNyUXo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivating Example: Fluid Flow Classification\n",
        "\n",
        "To make things concrete, we will classify **velocity profiles** in pipe flow.\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "**Physical system.** Fluid in a circular pipe. Across a diameter, the speed is highest at the center and drops to zero at the walls (no-slip).  \n",
        "**Data.** We sample the velocity at 51 equally spaced points across the diameter, giving a **51-dimensional feature vector** for each observation.\n",
        "\n",
        "**Three regimes (our labels):**\n",
        "- **Laminar-like**: sharply curved, parabolic shape  \n",
        "- **Transitional**: in between, neither fully parabolic nor fully flat  \n",
        "- **Turbulent-like**: flatter core, fuller profile\n",
        "\n",
        "We will generate these profiles synthetically so we can control difficulty and noise.\n",
        "\n",
        "### Geometry and Normalization\n",
        "\n",
        "**Radial coordinate \\( y \\).** Normalize the diameter so $y \\in [-1, 1]$:\n",
        "- $y=-1$: left wall\n",
        "- $y=0$: centerline\n",
        "- $y=+1$: right wall\n",
        "\n",
        "**Velocity $v(y)$.** We also normalize by the centerline speed $u_\\infty$ so curves are comparable across settings.\n",
        "\n",
        "This makes the setup **scale-independent** (millimeter capillary or meter-scale pipe have same treatment).\n",
        "\n",
        "## Definitions\n",
        "\n",
        "We start from two simple baseline shapes:\n",
        "\n",
        "- **Laminar (parabolic):**  \n",
        "  $$\n",
        "  v_{\\text{lam}}(y) = u_\\infty - y^2 + \\varepsilon\n",
        "  $$\n",
        "- **Turbulent-like (power law):**  \n",
        "  $$\n",
        "  v_{\\text{turb}}(y) = u_\\infty - C\\,|y|^{7} + \\varepsilon\n",
        "  $$\n",
        "Here $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is small measurement noise, and $C$ controls how flat the turbulent-like core is.\n",
        "\n",
        "### Transitional regime via convex combination ($\\alpha$-mixing)\n",
        "\n",
        "To model intermediate cases, we blend the two baselines:\n",
        "$$\n",
        "v_\\alpha(y) = (1-\\alpha)\\,v_{\\text{lam}}(y) + \\alpha\\,v_{\\text{turb}}(y) + \\varepsilon, \\quad \\alpha \\in [0,1].\n",
        "$$\n",
        "\n",
        "We then assign **class labels** directly from $\\alpha$:\n",
        "- **Laminar-like:** $\\alpha < 0.25$  \n",
        "- **Transitional:** $0.25 \\le \\alpha \\le 0.75$  \n",
        "- **Turbulent-like:** $\\alpha > 0.75$\n"
      ],
      "metadata": {
        "id": "py62r0WIuUF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing laminar, transitional, and turbulent velocity profiles\n",
        "#MARCO\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_velocity_profile(flow_type='laminar', u_infty=1.0, C=1.0, sigma=0.01, n_points=51, seed=None):\n",
        "    \"\"\"\n",
        "    Generate a velocity profile across a pipe diameter.\n",
        "\n",
        "    Parameters:\n",
        "    - flow_type (str): 'laminar' or 'turbulent'\n",
        "    - u_infty (float): centerline velocity (maximum speed)\n",
        "    - C (float): turbulent profile shape constant\n",
        "    - sigma (float): measurement noise level\n",
        "    - n_points (int): measurement points across diameter\n",
        "    - seed (int or None): random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    - y (np.ndarray): normalized radial positions [-1, 1]\n",
        "    - v (np.ndarray): velocity at each position\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Normalized positions: -1 (left wall) to +1 (right wall)\n",
        "    y = np.linspace(-1, 1, n_points)\n",
        "\n",
        "    # Measurement noise\n",
        "    noise = np.random.normal(loc=0.0, scale=sigma, size=n_points)\n",
        "\n",
        "    if flow_type == 'laminar':\n",
        "        v = u_infty - y**2 + noise\n",
        "    elif flow_type == 'turbulent':\n",
        "        v = u_infty - C * (np.abs(y))**7 + noise\n",
        "    else:\n",
        "        raise ValueError(\"flow_type must be either 'laminar' or 'turbulent'\")\n",
        "\n",
        "    return y, v\n",
        "\n",
        "def generate_hybrid_velocity_profile(alpha, u_infty=1.0, C=1.0, sigma=0.0, n_points=51, seed=None):\n",
        "    \"\"\"\n",
        "    Hybrid profile via convex combination:\n",
        "        v_alpha = (1 - alpha)*v_laminar + alpha*v_turbulent + noise\n",
        "    \"\"\"\n",
        "    if not (0.0 <= alpha <= 1.0):\n",
        "        raise ValueError(\"alpha must be in [0, 1].\")\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Get noise-free baselines for a clean mix\n",
        "    y, v_lam = generate_velocity_profile('laminar', u_infty=u_infty, C=C, sigma=0.0, n_points=n_points)\n",
        "    _, v_tur = generate_velocity_profile('turbulent', u_infty=u_infty, C=C, sigma=0.0, n_points=n_points)\n",
        "\n",
        "    v = (1 - alpha) * v_lam + alpha * v_tur\n",
        "    # Single noise injection post-mix (kept 0.0 here for clarity in the demo)\n",
        "    v = v + np.random.normal(0.0, sigma, size=n_points)\n",
        "    return y, v\n",
        "\n",
        "# --- Plot the three illustrative cases (noise-free) ---\n",
        "alphas = [0.0, 0.5, 1.0]  # laminar-like, transitional, turbulent-like\n",
        "labels = [\"Laminar-like (α=0.00)\", \"Transitional (α=0.50)\", \"Turbulent-like (α=1.00)\"]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "for a, lbl in zip(alphas, labels):\n",
        "    y, v = generate_hybrid_velocity_profile(alpha=a, sigma=0.0, n_points=51, C=1.0, u_infty=1.0, seed=42)\n",
        "    plt.plot(y, v, linewidth=2, label=lbl)\n",
        "\n",
        "plt.xlabel(\"Normalized radial position y (pipe wall to wall)\")\n",
        "plt.ylabel(\"Normalized velocity v(y) / U∞\")\n",
        "plt.title(\"Laminar, Transitional, and Turbulent-like Velocity Profiles\")\n",
        "plt.axvline(x=-1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "plt.axvline(x=1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "plt.text(-1, 0.05, 'Left\\nwall', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.text(1, 0.05, 'Right\\nwall', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.text(0, 1.05, 'Center', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jIT9C59Vmhnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "We have treated logistic regression already, so we just recall the main ideas here.\n",
        "\n",
        "## Quick refresher\n",
        "Logistic regression is a linear classifier: it learns a weighted sum of the input features and passes it through a squashing function to produce probabilities. In the binary case, the sigmoid turns the score into\n",
        "$P(y=1|x)$; the decision boundary is a hyperplane (linear in the features). We fit the weights by maximizing the likelihood (equivalently, minimizing cross-entropy), typically with a bit of regularization to avoid overfitting.\n",
        "\n",
        "**Multiclass classification**. We use the multinomial/softmax extension: the model learns one linear score per class, converts all scores into a probability distribution over {laminar-like, transitional, turbulent-like} via softmax (they sum to 1), and predicts the class with the highest probability. Intuitively, this is just the binary idea applied jointly to all classes rather than training separate one-vs-rest models.\n"
      ],
      "metadata": {
        "id": "TTfAZ7Wh5Wb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fluid velocity profile application\n",
        "\n",
        "We will now test the performance of the logistic regression method on the velocity profile dataset. We will first generate 1500 samples (adding noise).\n",
        "\n",
        "A test dataset (10%) is kept aside to fairly check on the model performance."
      ],
      "metadata": {
        "id": "QOq0RhGSE3dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Logistic regression performance on fluid velocity profile dataset.\n",
        "# MARCO\n",
        "# --- Multiclass Logistic Regression on 3-class velocity profiles (no interactive knob) ---\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "# Assumes you already defined generate_velocity_profile(flow_type=..., ...)\n",
        "# We add a tiny helper to create hybrid profiles with alpha in [0,1].\n",
        "\n",
        "def generate_hybrid_profile(alpha, n_points=51, sigma=0.03, C=1.0, u_infty=1.0, seed=None):\n",
        "    \"\"\"Hybrid profile: v_alpha = (1-alpha)*v_lam + alpha*v_turb + noise.\"\"\"\n",
        "    if seed is not None:\n",
        "        rng = np.random.default_rng(seed)\n",
        "    else:\n",
        "        rng = np.random.default_rng()\n",
        "    y, v_lam = generate_velocity_profile('laminar', u_infty=u_infty, C=C, sigma=0.0, n_points=n_points)\n",
        "    _, v_turb = generate_velocity_profile('turbulent', u_infty=u_infty, C=C, sigma=0.0, n_points=n_points)\n",
        "    v = (1 - alpha) * v_lam + alpha * v_turb\n",
        "    v = v + rng.normal(0.0, sigma, size=n_points)  # single noise injection\n",
        "    return y, v\n",
        "\n",
        "def label_from_alpha(alpha):\n",
        "    \"\"\"3-class labeling based on alpha.\"\"\"\n",
        "    if alpha < 0.25:\n",
        "        return 0  # laminar-like\n",
        "    elif alpha > 0.75:\n",
        "        return 2  # turbulent-like\n",
        "    else:\n",
        "        return 1  # transitional\n",
        "\n",
        "# --- Generate dataset ---\n",
        "n_samples = 1500\n",
        "n_points = 51\n",
        "sigma = 0.05\n",
        "C = 1.0\n",
        "u_infty = 1.0\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "alphas = rng.uniform(0.0, 1.0, size=n_samples)\n",
        "X = np.empty((n_samples, n_points))\n",
        "y = np.empty(n_samples, dtype=int)\n",
        "\n",
        "for i, a in enumerate(alphas):\n",
        "    _, v = generate_hybrid_profile(a, n_points=n_points, sigma=sigma, C=C, u_infty=u_infty, seed=rng.integers(1e9))\n",
        "    X[i] = v\n",
        "    y[i] = label_from_alpha(a)\n",
        "\n",
        "# --- Stratified split: use 10% as test ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.10, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# --- Multinomial Logistic Regression with scaling ---\n",
        "logreg = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(\n",
        "        multi_class=\"multinomial\",\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=2000,\n",
        "        random_state=42\n",
        "    )\n",
        ")\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# --- Metrics (simple and relevant) ---\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, y_pred, average=\"macro\", zero_division=0\n",
        ")\n",
        "\n",
        "print(f\"Accuracy:       {acc:.3f}\")\n",
        "print(f\"Precision (µ):  {prec_macro:.3f}\")\n",
        "print(f\"Recall (µ):     {recall_macro:.3f}\")\n",
        "print(f\"F1 (µ):         {f1_macro:.3f}\\n\")\n",
        "\n",
        "target_names = [\"Laminar-like (0)\", \"Transitional (1)\", \"Turbulent-like (2)\"]\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-j7TEpdorBk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new velocity profile by changing $\\alpha$ (pure laminar=0, pure turbulent=1) and $\\sigma$ for the noise the noise level.\n",
        "\n",
        "Recall that the class labels are:\n",
        "\n",
        "- **Laminar-like:** $\\alpha < 0.25$  \n",
        "- **Transitional:** $0.25 \\le \\alpha \\le 0.75$  \n",
        "- **Turbulent-like:** $\\alpha > 0.75$"
      ],
      "metadata": {
        "id": "qAy8dYxQs4jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Alpha_log = 0.77 # 0: pure laminar; 1: pure turbulent\n",
        "Sigma_log = 0.05 # originally 0.05"
      ],
      "metadata": {
        "id": "McRp-6_QtPOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to evaluate logistic regression on a new velocity profile\n",
        "# MARCO\n",
        "# --- Predict a new profile by choosing alpha and visualize it ---\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class names (consistent with earlier labeling)\n",
        "CLASS_NAMES = [\"Laminar-like (0)\", \"Transitional (1)\", \"Turbulent-like (2)\"]\n",
        "\n",
        "def classify_and_plot(alpha=0.50, sigma=0.05, C=1.0, u_infty=1.0, n_points=51, seed=12345):\n",
        "    \"\"\"\n",
        "    Generate a single hybrid profile for given alpha, classify with the trained model,\n",
        "    and plot the velocity profile.\n",
        "    \"\"\"\n",
        "    # Generate profile (single noise injection; fixed seed for reproducibility)\n",
        "    y, v = generate_hybrid_profile(alpha, n_points=n_points, sigma=sigma,\n",
        "                                   C=C, u_infty=u_infty, seed=seed)\n",
        "\n",
        "    # Predict with the trained pipeline\n",
        "    pred_class = int(logreg.predict(v.reshape(1, -1))[0])\n",
        "    proba = logreg.predict_proba(v.reshape(1, -1))[0]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(y, v, linewidth=2)\n",
        "    plt.xlabel(\"Normalized radial position y (pipe wall to wall)\")\n",
        "    plt.ylabel(\"Normalized velocity v(y) / U∞\")\n",
        "    plt.title(f\"α = {alpha:.2f}  →  Predicted: {CLASS_NAMES[pred_class]}\")\n",
        "    plt.axvline(x=-1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.axvline(x=1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add probabilities as a small textbox on the plot\n",
        "    prob_text = \"\\n\".join(f\"{name}: {p:.2f}\" for name, p in zip(CLASS_NAMES, proba))\n",
        "    plt.text(0.02, 0.98, prob_text, transform=plt.gca().transAxes,\n",
        "             va='top', ha='left', fontsize=9,\n",
        "             bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.85))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: change alpha in this call to test different profiles\n",
        "classify_and_plot(alpha=Alpha_log, sigma=Sigma_log)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "raXHuV0nslO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors (k-NN)\n",
        "\n",
        "## Quick refresher\n",
        "The $k$-nearest‑neighbour (kNN) was used in anomaly detection: if you have to travel far to reach your $k$th neighbour, you must be in a sparse part of the space and are therefore a candidate outlier.  Formally, for a point $\\mathbf{x}$ let\n",
        "\n",
        "$$\n",
        "s_{k}(\\mathbf{x})=\\frac1k\\sum_{j=1}^{k} \\lVert\\mathbf{x}-\\mathbf{x}_{(j)}\\rVert ,\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x}_{(j)}$ is the $j$th nearest neighbour of $\\mathbf{x}$ in the training set.  Large $s_{k}$ means low local density; ranking all samples by this score and thresholding the largest few per cent yields the anomaly set.\n",
        "\n",
        "The only hyper‑parameter is $k$; small values make the score sensitive to fine local structure, while larger $k$ smooth over noise at the risk of diluting rare but genuine anomalies.\n",
        "\n",
        "Once the scores are generated, we need to determine where to put the cut-off. Scores above the cut-off are flagged as anomalies.\n",
        "\n",
        "---\n",
        "### How kNN does supervised classification\n",
        "k-Nearest Neighbors (k-NN) has a very intuitive principle of work. In fact, its name already explains it. Unlike logistic regression, k-NN can be used for multi-class classification \"out of the box\". Here is the way k-NN works:\n",
        "\n",
        "1. Pick a data point you want to classify.\n",
        "2. Find the k nearest neighbors.\n",
        "3. Assign the class to the point that corresponds to the majority class within the k neighbors.\n",
        "\n",
        "And that is all — nice and simple! The only parameters we need to decide on are **k** and the way that the **distance between points is calculated**.\n",
        "\n",
        "Choosing the right distance metric helps k-NN reflect the true \"closeness\" between data points in a meaningful way for your specific problem.\n",
        "\n",
        "### Advantages and disadvantages\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Very simple and intuitive.\n",
        "- Naturally handles multi-class classification.\n",
        "- No training phase — the model just stores the data.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- Computationally expensive at prediction time (especially for large datasets).\n",
        "- Performance can degrade in high-dimensional spaces (\"curse of dimensionality\").\n",
        "- Sensitive to irrelevant or redundant features and scaling of data.\n"
      ],
      "metadata": {
        "id": "MqtC4JK2v_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive k-NN demo\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# === Generate dataset ===\n",
        "centers = [(-2, -2), (2, 0), (-1, 4), (3, 7)]\n",
        "cluster_std = [1.0, 1.5, 0.5, 1.0]\n",
        "X, y_labels = make_blobs(n_samples=[100, 200, 5, 100], centers=centers, cluster_std=cluster_std, random_state=42)\n",
        "\n",
        "# === Fit once globally ===\n",
        "knn_global = KNeighborsClassifier()\n",
        "knn_global.fit(X, y_labels)\n",
        "\n",
        "# === Define plotting function ===\n",
        "def plot_knn_decision_boundary(x=0.0, y=0.0, k=5, show_lines=True):\n",
        "    # Convert slider inputs to native float (in case they are numpy scalars)\n",
        "    x = float(x)\n",
        "    y = float(y)\n",
        "\n",
        "    # Refit classifier with selected k\n",
        "    knn_global.set_params(n_neighbors=k)\n",
        "    knn_global.fit(X, y_labels)\n",
        "\n",
        "    # Mesh grid for decision boundary\n",
        "    h = 0.1\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = knn_global.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Query point and neighbors\n",
        "    query_point = np.array([[x, y]])\n",
        "    query_class = knn_global.predict(query_point)[0]\n",
        "    print(f\"color is: {query_class}\")\n",
        "    distances, indices = knn_global.kneighbors(query_point)\n",
        "    neighbor_pts = X[indices[0]]\n",
        "\n",
        "    # Define class colors\n",
        "    color_map = {\n",
        "    0: 'blue',\n",
        "    1: 'red',\n",
        "    2: 'pink',\n",
        "    3: 'cyan',\n",
        "    }\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(8, 7))\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.tab10)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y_labels, cmap=plt.cm.tab10, edgecolor='k', s=50)\n",
        "\n",
        "    if show_lines:\n",
        "        for pt in neighbor_pts:\n",
        "            ax.plot([x, pt[0]], [y, pt[1]], 'k--', linewidth=1)\n",
        "\n",
        "    ax.scatter(neighbor_pts[:, 0], neighbor_pts[:, 1],\n",
        "               facecolors='none', edgecolors='black', s=150, linewidths=2, label='Neighbors')\n",
        "\n",
        "    ax.scatter(x, y, c=color_map[query_class], edgecolor='k', s=200, marker='o', label='Query Point')\n",
        "\n",
        "    ax.set_title(f\"Interactive k-NN Classification (k={k})\", fontsize=14)\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# === Sliders ===\n",
        "x_slider = widgets.FloatSlider(value=2.0, min=-4.0, max=6.0, step=0.1, description='x:')\n",
        "y_slider = widgets.FloatSlider(value=6.0, min=-4.0, max=9.0, step=0.1, description='y:')\n",
        "k_slider = widgets.IntSlider(value=5, min=1, max=20, step=1, description='k:')\n",
        "lines_toggle = widgets.Checkbox(value=True, description='Show lines to neighbors')\n",
        "\n",
        "interactive_plot = widgets.interactive_output(\n",
        "    plot_knn_decision_boundary,\n",
        "    {'x': x_slider, 'y': y_slider, 'k': k_slider, 'show_lines': lines_toggle}\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([x_slider, y_slider, k_slider, lines_toggle])\n",
        "display(ui, interactive_plot)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Gbv7BAd9_jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fluid velocity profile application\n",
        "#MARCO\n",
        "# --- k-NN on 3-class velocity profiles generated via alpha mixing ---\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "# If these helpers already exist in your notebook, you can skip redefining them.\n",
        "def generate_hybrid_profile(alpha, n_points=51, sigma=0.03, C=1.0, u_infty=1.0, seed=None):\n",
        "    \"\"\"Hybrid profile: v_alpha = (1 - alpha)*v_lam + alpha*v_turb + noise.\"\"\"\n",
        "    if seed is not None:\n",
        "        rng = np.random.default_rng(seed)\n",
        "    else:\n",
        "        rng = np.random.default_rng()\n",
        "    # Baselines are noise-free for a clean mix\n",
        "    y = np.linspace(-1, 1, n_points)\n",
        "    v_lam = u_infty - y**2\n",
        "    v_turb = u_infty - C * (np.abs(y))**7\n",
        "    v = (1 - alpha) * v_lam + alpha * v_turb\n",
        "    v = v + rng.normal(0.0, sigma, size=n_points)  # single noise injection\n",
        "    return y, v\n",
        "\n",
        "def label_from_alpha(alpha):\n",
        "    \"\"\"3-class labeling based on alpha.\"\"\"\n",
        "    if alpha < 0.25:\n",
        "        return 0  # laminar-like\n",
        "    elif alpha > 0.75:\n",
        "        return 2  # turbulent-like\n",
        "    else:\n",
        "        return 1  # transitional\n",
        "\n",
        "# --- Generate dataset ---\n",
        "n_samples = 1500\n",
        "n_points = 51\n",
        "sigma = 0.05\n",
        "C = 1.0\n",
        "u_infty = 1.0\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "alphas = rng.uniform(0.0, 1.0, size=n_samples)\n",
        "X = np.empty((n_samples, n_points))\n",
        "y = np.empty(n_samples, dtype=int)\n",
        "\n",
        "for i, a in enumerate(alphas):\n",
        "    _, v = generate_hybrid_profile(a, n_points=n_points, sigma=sigma, C=C, u_infty=u_infty, seed=rng.integers(1e9))\n",
        "    X[i] = v\n",
        "    y[i] = label_from_alpha(a)\n",
        "\n",
        "# --- Stratified split: 10% test ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.10, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# --- k-NN with scaling ---\n",
        "k = 5  # simple, readable default (you can try 3/7/11 later)\n",
        "knn = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KNeighborsClassifier(n_neighbors=k, weights=\"uniform\", metric=\"minkowski\")\n",
        ")\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# --- Metrics ---\n",
        "y_pred = knn.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "    y_test, y_pred, average=\"macro\", zero_division=0\n",
        ")\n",
        "\n",
        "print(f\"Accuracy:        {acc:.3f}\")\n",
        "print(f\"Precision (macro){prec_macro:.3f}\")\n",
        "print(f\"Recall (macro):  {recall_macro:.3f}\")\n",
        "print(f\"F1 (macro):      {f1_macro:.3f}\\n\")\n",
        "\n",
        "target_names = [\"Laminar-like (0)\", \"Transitional (1)\", \"Turbulent-like (2)\"]\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1S7NxdFEztld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new velocity profile by changing $\\alpha$ (pure laminar=0, pure turbulent=1) and $\\sigma$ for the noise the noise level.\n",
        "\n",
        "Recall that the class labels are:\n",
        "\n",
        "- **Laminar-like:** $\\alpha < 0.25$  \n",
        "- **Transitional:** $0.25 \\le \\alpha \\le 0.75$  \n",
        "- **Turbulent-like:** $\\alpha > 0.75$\n"
      ],
      "metadata": {
        "id": "qx_PILfk1eIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Alpha_knn = 0.70 # 0: pure laminar; 1: pure turbulent\n",
        "Sigma_knn = 0.05 # originally 0.05"
      ],
      "metadata": {
        "id": "Yq4nx6wH1kT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to evaluate k-NN classification on a new velocity profile\n",
        "# MARCO\n",
        "# --- Classify a single α-generated velocity profile with the trained k-NN model ---\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class names consistent with earlier labeling\n",
        "CLASS_NAMES = [\"Laminar-like (0)\", \"Transitional (1)\", \"Turbulent-like (2)\"]\n",
        "\n",
        "def classify_and_plot_knn(alpha=0.50, sigma=0.05, C=1.0, u_infty=1.0, n_points=51, seed=12345):\n",
        "    \"\"\"\n",
        "    Generate one hybrid profile at a chosen alpha, classify it with the trained k-NN pipeline `knn`,\n",
        "    and plot the velocity profile with predicted class and class probabilities.\n",
        "\n",
        "    Assumes:\n",
        "      - `generate_hybrid_profile(...)` is defined (as in previous cells)\n",
        "      - `knn` is the trained Pipeline(StandardScaler -> KNeighborsClassifier)\n",
        "    \"\"\"\n",
        "    # Generate profile (single noise injection; fixed seed for reproducibility)\n",
        "    y, v = generate_hybrid_profile(alpha, n_points=n_points, sigma=sigma,\n",
        "                                   C=C, u_infty=u_infty, seed=seed)\n",
        "\n",
        "    # Predict with trained k-NN pipeline\n",
        "    pred_class = int(knn.predict(v.reshape(1, -1))[0])\n",
        "    proba = knn.predict_proba(v.reshape(1, -1))[0]  # neighbor vote fractions\n",
        "\n",
        "    # Plot the profile\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(y, v, linewidth=2)\n",
        "    plt.xlabel(\"Normalized radial position y (pipe wall to wall)\")\n",
        "    plt.ylabel(\"Normalized velocity v(y) / U∞\")\n",
        "    plt.title(f\"α = {alpha:.2f}  →  Predicted: {CLASS_NAMES[pred_class]}\")\n",
        "    plt.axvline(x=-1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.axvline(x=1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show probabilities on the plot\n",
        "    prob_text = \"\\n\".join(f\"{name}: {p:.2f}\" for name, p in zip(CLASS_NAMES, proba))\n",
        "    plt.text(0.02, 0.98, prob_text, transform=plt.gca().transAxes,\n",
        "             va='top', ha='left', fontsize=9,\n",
        "             bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.85))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example call (adjust alpha as you wish):\n",
        "classify_and_plot_knn(alpha=Alpha_knn, sigma=Sigma_knn)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t9dMZ7vG1krU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "**Core idea** SVM looks for a decision boundary that separates classes with the largest possible margin. Think of two clouds of points: SVM places a hyperplane between them so that the closest points to the plane are as far as possible. Those closest points are the support vectors and they define the boundary."
      ],
      "metadata": {
        "id": "SOBcKqQVWaHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple demo below presents the core idea behind SVM. Try to manually find a line that separates the classes while maximizing the margin."
      ],
      "metadata": {
        "id": "AhLjuMb2iRO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SVM margin demo\n",
        "\n",
        "# --- Generate two clusters ---\n",
        "np.random.seed(1)\n",
        "n_points = 10\n",
        "cluster_1 = np.random.randn(n_points, 2) + np.array([-2, 4])\n",
        "cluster_2 = np.random.randn(n_points, 2) + np.array([2, -4])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1]*n_points + [-1]*n_points)\n",
        "\n",
        "def svm_margin_plot(slope):\n",
        "    w = -np.array([slope, -1])  # Normal vector to decision boundary\n",
        "    w = w / np.linalg.norm(w)  # Normalize\n",
        "\n",
        "    # Project points onto normal vector\n",
        "    projections = X @ w\n",
        "    margin_pos = projections[y == 1].min()\n",
        "    margin_neg = projections[y == -1].max()\n",
        "    margin = 0.5 * (margin_pos - margin_neg)\n",
        "    decision_offset = 0.5 * (margin_pos + margin_neg)\n",
        "\n",
        "    def plot_margin_line(ax, offset, style='k--', label=None):\n",
        "        midpoint = offset * w\n",
        "        direction = np.array([-w[1], w[0]])  # Perpendicular direction\n",
        "        line_points = np.array([midpoint + t * direction for t in [-10, 10]])\n",
        "        ax.plot(line_points[:, 0], line_points[:, 1], style, label=label)\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.scatter(cluster_1[:, 0], cluster_1[:, 1], c='red', label='Class +1')\n",
        "    ax.scatter(cluster_2[:, 0], cluster_2[:, 1], c='blue', label='Class -1')\n",
        "\n",
        "    plot_margin_line(ax, decision_offset, style='k-', label='Decision boundary')\n",
        "    plot_margin_line(ax, margin_pos, style='k--', label='Margin boundaries')\n",
        "    plot_margin_line(ax, margin_neg, style='k--')\n",
        "\n",
        "    # Highlight support vectors\n",
        "    sv_pos = X[y == 1][np.argmin(projections[y == 1])]\n",
        "    sv_neg = X[y == -1][np.argmax(projections[y == -1])]\n",
        "    ax.scatter(*sv_pos, s=150, facecolors='none', edgecolors='red', linewidths=2, label='Support vectors')\n",
        "    ax.scatter(*sv_neg, s=150, facecolors='none', edgecolors='blue', linewidths=2)\n",
        "\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 15)\n",
        "    ax.set_aspect('equal', adjustable='box')  # Changed this line\n",
        "    ax.set_title(f\"SVM Margin and Decision Boundary (slope = {slope:.2f})\")\n",
        "    ax.set_xlabel(\"x₁\")\n",
        "    ax.set_ylabel(\"x₂\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive slider\n",
        "interact(svm_margin_plot, slope=FloatSlider(value=-0.5, min=-1, max=20, step=0.1));"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PZo1NaCFtRcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hard and Soft Margins\n",
        "\n",
        "So far, we have looked at the case where two clusters can be perfectly separated by a line. However, in many real-world datasets, misclassifications are inevitable, and a perfect separation is not possible. In such cases, the standard SVM cannot find a valid decision boundary.\n",
        "\n",
        "What we need is some flexibility in the margin — a way to tolerate a small number of misclassified points when determining the separating hyperplane. This approach is known as the **soft margin**, as opposed to the **hard margin**, which assumes that the data is linearly separable with no exceptions.\n",
        "\n",
        "The **soft margin** SVM introduces a mechanism to allow certain violations of the margin — that is, to allow some points to fall on the wrong side of the boundary or within the margin. This makes it more robust and better suited for real-world data.\n",
        "\n",
        "The trade-off between maximizing the margin and minimizing classification errors is controlled by the parameter $C$. This parameter determines how much **penalty** is assigned to misclassified points:\n",
        "\n",
        "- A **large $C$** forces the model to classify all training examples correctly (hard margin behavior), possibly at the cost of a smaller margin.\n",
        "- A **small $C$** allows for more margin violations (soft margin), leading to a wider margin and potentially better generalization.\n",
        "\n",
        "Soft-margin SVM is the **default** behavior in most machine learning libraries and toolboxes.\n",
        "\n",
        "Below is a plot that shows the effects of different $C$ values. A hard margin can be approximated by using a very large $C$.\n"
      ],
      "metadata": {
        "id": "BD1YyaMj4COH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hard vs soft margin plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Generate linearly separable data ---\n",
        "np.random.seed(0)\n",
        "n_samples = 20\n",
        "cluster_1 = np.random.randn(n_samples, 2) + np.array([2, 2])\n",
        "cluster_2 = np.random.randn(n_samples, 2) + np.array([-2, -2])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1]*n_samples + [-1]*n_samples)\n",
        "\n",
        "# --- Fit hard margin and soft margin SVM ---\n",
        "svm_hard = SVC(kernel='linear', C=1e10)\n",
        "svm_soft = SVC(kernel='linear', C=0.1)\n",
        "svm_hard.fit(X, y)\n",
        "svm_soft.fit(X, y)\n",
        "\n",
        "# --- Create mesh for decision boundary visualization ---\n",
        "h = 0.05\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "Z_hard = svm_hard.decision_function(grid).reshape(xx.shape)\n",
        "Z_soft = svm_soft.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "titles = ['Hard Margin SVM (C=1e10)', 'Soft Margin SVM (C=0.1)']\n",
        "svms = [svm_hard, svm_soft]\n",
        "Zs = [Z_hard, Z_soft]\n",
        "\n",
        "for ax, title, svm, Z in zip(axs, titles, svms, Zs):\n",
        "    ax.contourf(xx, yy, Z > 0, alpha=0.2, cmap=plt.cm.coolwarm)\n",
        "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
        "\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    ax.scatter(svm.support_vectors_[:, 0],\n",
        "               svm.support_vectors_[:, 1],\n",
        "               s=150, facecolors='none', edgecolors='k', linewidths=1.5, label='Support Vectors')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"x₁\")\n",
        "    ax.set_ylabel(\"x₂\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D-lGQfn85N7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Left figure** High C (hard margin):\n",
        "Tries to get every training point right. The margin (the buffer around the line) becomes narrow. The line is pulled toward any difficult or noisy points. Usually fewer points sit on/inside the margin, so fewer support vectors determine the boundary-but it can also latch onto outliers.\n",
        "\n",
        "** Right figure** Low C (soft margin):\n",
        "Allows a few mistakes on the training set in exchange for a wider margin. More points end up on/inside that wider buffer, so more support vectors help set the line. This tends to be more robust to noise and generalizes better."
      ],
      "metadata": {
        "id": "XGdFWO8Fzvuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Non-linearly separable data with soft margin SVM demo\n",
        "\n",
        "# --- Generate non-linearly separable data ---\n",
        "np.random.seed(1)\n",
        "n_samples = 20\n",
        "cluster_1 = 1.5 * np.random.randn(n_samples, 2) + np.array([1, 1])\n",
        "cluster_2 = np.random.randn(n_samples, 2) + np.array([-1, -1])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1] * n_samples + [-1] * n_samples)\n",
        "\n",
        "# --- Create mesh grid for decision boundaries ---\n",
        "h = 0.05\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "def plot_svm(C):\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X, y)\n",
        "    Z = clf.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.2, cmap=plt.cm.coolwarm)\n",
        "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.scatter(clf.support_vectors_[:, 0],\n",
        "                clf.support_vectors_[:, 1],\n",
        "                s=150, facecolors='none', edgecolors='k', linewidths=1.5, label='Support Vectors')\n",
        "\n",
        "    plt.title(f\"Soft Margin SVM (C={C:.3f})\")\n",
        "    plt.xlabel(\"x₁\")\n",
        "    plt.ylabel(\"x₂\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive slider (log scale for better exploration)\n",
        "interact(plot_svm, C=FloatLogSlider(value=1.0, base=10, min=-2, max=3, step=0.1, description='C'))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v-ZUk_rJ5qqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-linear data and multi-class cases\n",
        "\n",
        "At this point, it might seem like we’ve put a lot of effort into a method that can only separate data with a straight line. Fortunately, SVM has a powerful technique to handle more complex scenarios. When the raw data is not linearly separable, SVM can map it into a higher-dimensional space where a hyperplane *can* separate the clusters. This idea is often called the \"kernel trick\" and is one of the key strengths of SVM.\n",
        "\n",
        "SVM is naturally a binary classifier. For 3+ classes, we combine several binary SVMs.\n",
        "\n",
        "The usual approach (what SVC uses under the hood) is one-vs-one: for $K$\n",
        "classes it trains a small SVM for each pair of classes. At prediction time, every pair \"votes\" and the class with the most votes wins.\n",
        "Conceptually, you can also think one-vs-rest: one SVM per class vs all others.\n",
        "\n"
      ],
      "metadata": {
        "id": "DepbbtQ9_sDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new velocity profile by changing $\\alpha$ (pure laminar=0, pure turbulent=1) and $\\sigma$ for the noise the noise level.\n",
        "\n",
        "Recall that the class labels are:\n",
        "\n",
        "- **Laminar-like:** $\\alpha < 0.25$  \n",
        "- **Transitional:** $0.25 \\le \\alpha \\le 0.75$  \n",
        "- **Turbulent-like:** $\\alpha > 0.75$"
      ],
      "metadata": {
        "id": "mj11aBY0_7_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Alpha_svm = 0.9 # 0: pure laminar; 1: pure turbulent\n",
        "Sigma_svm = 0.03 # originally 0.05"
      ],
      "metadata": {
        "id": "16HlhDOl_9IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to evaluate SVM classification on a new velocity profile\n",
        "#MARCO\n",
        "# --- Classify a single α-generated velocity profile with the trained SVM model ---\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class names (consistent with earlier labeling)\n",
        "CLASS_NAMES = [\"Laminar-like (0)\", \"Transitional (1)\", \"Turbulent-like (2)\"]\n",
        "\n",
        "def classify_and_plot_svm(alpha=0.50, sigma=0.05, C_shape=1.0, u_infty=1.0, n_points=51, seed=12345):\n",
        "    \"\"\"\n",
        "    Generate one hybrid profile at a chosen alpha, classify it with the trained SVM pipeline `svm`,\n",
        "    and plot the velocity profile with the predicted class.\n",
        "\n",
        "    Assumes:\n",
        "      - `generate_hybrid_profile(...)` is defined\n",
        "      - `svm` is a trained Pipeline(StandardScaler -> SVC)\n",
        "    \"\"\"\n",
        "    # Generate profile (single noise injection; fixed seed for reproducibility)\n",
        "    y, v = generate_hybrid_profile(alpha, n_points=n_points, sigma=sigma,\n",
        "                                   C=C_shape, u_infty=u_infty, seed=seed)\n",
        "\n",
        "    # Predict class\n",
        "    pred_class = int(svm.predict(v.reshape(1, -1))[0])\n",
        "\n",
        "    # Try to get probabilities if available; else use softmax of decision scores (for display only)\n",
        "    proba_text = None\n",
        "    try:\n",
        "        proba = svm.predict_proba(v.reshape(1, -1))[0]  # works only if SVC(probability=True)\n",
        "        proba_text = \"\\n\".join(f\"{name}: {p:.2f}\" for name, p in zip(CLASS_NAMES, proba))\n",
        "    except Exception:\n",
        "        # Fall back to decision_function scores -> softmax (NOT calibrated probabilities)\n",
        "        scores = np.ravel(svm.decision_function(v.reshape(1, -1)))\n",
        "        scores = scores - np.max(scores)\n",
        "        exp_s = np.exp(scores)\n",
        "        soft = exp_s / np.sum(exp_s)\n",
        "        proba_text = \"\\n\".join(f\"{name}: {p:.2f}\" for name, p in zip(CLASS_NAMES, soft))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(y, v, linewidth=2)\n",
        "    plt.xlabel(\"Normalized radial position y (pipe wall to wall)\")\n",
        "    plt.ylabel(\"Normalized velocity v(y) / U∞\")\n",
        "    plt.title(f\"α = {alpha:.2f}  →  Predicted: {CLASS_NAMES[pred_class]}\")\n",
        "    plt.axvline(x=-1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.axvline(x=1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show probability/score text box\n",
        "    plt.text(0.02, 0.98, proba_text, transform=plt.gca().transAxes,\n",
        "             va='top', ha='left', fontsize=9,\n",
        "             bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.85))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "classify_and_plot_svm(alpha=Alpha_svm, sigma=Sigma_svm)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZLXHFah3AFdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron (MLP)\n",
        "\n",
        "**Multi-Layer Perceptron (MLP)** is by far the most versatile and complex technique covered in this session. What makes MLPs so useful is the fact that they consist of many interconnected nodes (neurons), which can be organized into multiple layers — an input layer, one or more hidden layers, and an output layer. Thanks to this layered structure, MLPs can be used for a wide range of problems **including regression**, **classification**, and even more advanced tasks like **time series prediction** or **image recognition**. Most importantly, MLPs allow for learning complex non-linear functions that traditional linear models cannot capture.\n",
        "\n",
        "However, this flexibility comes with the challenge of selecting appropriate hyperparameters, which significantly affect performance:\n",
        "\n",
        "- **Hidden Layer Size**: The number of neurons in each hidden layer (e.g. one layer `(100,)` vs. two layers `(100, 50)`). More neurons and more layers increase model capacity but also the risk of overfitting.\n",
        "\n",
        "- **Number of Hidden Layers**: Increasing the depth (i.e. number of layers) enables learning more complex patterns, but also makes training harder.\n",
        "\n",
        "- **Activation Function**: Determines the non-linearity at each neuron. Common choices include 'relu', 'tanh', and 'logistic'.\n",
        "\n",
        "- **Learning Rate**: Controls how fast the model updates its weights during training. Too high can make learning unstable; too low can slow convergence.\n",
        "\n",
        "- **Regularization (alpha)**: Prevents overfitting by penalizing large weights. A higher alpha applies stronger regularization.\n",
        "\n",
        "- **Max Iterations**: The maximum number of training epochs. If the model doesn't converge, you might need to increase this.\n",
        "\n",
        "- **Solver**: The optimization algorithm used (e.g., `'adam'`, `'sgd'`, `'lbfgs'`). `'adam'` is often a good default for most tasks.\n",
        "\n",
        "Choosing good hyperparameters often requires experimentation and sometimes cross-validation. In practice, using tools like `GridSearchCV` or `RandomizedSearchCV` can help automate this process. We will cover MLPs in more detail in the upcoming sessions.\n",
        "\n",
        "In the following example, we see how an MLP can classify a highly non-linear dataset — something that simpler models cannot do."
      ],
      "metadata": {
        "id": "c0uRhpI1SCP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MLP example\n",
        "\n",
        "# --- Generate two spiral dataset ---\n",
        "def generate_spiral(n_points, noise=0.2):\n",
        "    theta = np.sqrt(np.random.rand(n_points)) * 4 * np.pi  # Angle\n",
        "    r = theta\n",
        "    x1 = r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y1 = r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    x2 = -r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y2 = -r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    X = np.vstack((np.column_stack((x1, y1)), np.column_stack((x2, y2))))\n",
        "    y = np.array([0]*n_points + [1]*n_points)\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_spiral(500, noise=0.15)\n",
        "\n",
        "# --- Split data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# --- Train MLP ---\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', max_iter=8000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# --- Accuracy ---\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# --- Plot decision boundary ---\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    h = 0.05\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.4)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
        "    plt.title(\"MLP Classification of Two Spirals\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(mlp, X, y)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9Mgy0Cj6VNmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flow velocity profile classification using MLP\n",
        "\n",
        "# --- Generate labeled dataset ---\n",
        "n_samples_per_class = 200\n",
        "n_points = 51\n",
        "sigma = 0.05  # same noise level\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for _ in range(n_samples_per_class):\n",
        "    _, v_laminar = generate_velocity_profile('laminar', sigma=sigma)\n",
        "    _, v_turbulent = generate_velocity_profile('turbulent', sigma=sigma)\n",
        "    X.append(v_laminar)\n",
        "    y.append(0)\n",
        "    X.append(v_turbulent)\n",
        "    y.append(1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Train-test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Fit MLP classifier ---\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=3000, random_state=42)\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate ---\n",
        "y_pred = mlp_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"MLP Accuracy: {acc:.2f}\")\n",
        "\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Visualize a few test samples ---\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "y_vals = np.linspace(-1, 1, n_points)\n",
        "\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    idx = i\n",
        "    ax.plot(y_vals, X_test[idx], label=\"Velocity profile\")\n",
        "    pred = mlp_clf.predict(X_test[idx].reshape(1, -1))[0]\n",
        "    true = y_test[idx]\n",
        "    ax.set_title(f\"True: {'Laminar' if true == 0 else 'Turbulent'} | Pred: {'Laminar' if pred == 0 else 'Turbulent'}\")\n",
        "    ax.set_xlabel(\"y\")\n",
        "    ax.set_ylabel(\"v(y)\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u3VrPfgO28Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, MLP also achives 100% accuracy on the fluid flow example."
      ],
      "metadata": {
        "id": "DFgh7DwULFKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification methods comparison\n",
        "\n",
        "When looking at the velocity profiles classification shown throughout this notebook, we see that all of the methods achieved 100% accuracy. Often times, it is good to start with the simplest and most efficient method. If the results are not satisfactory for a given application, you should try out another method. Often times, some hyperparamter tinkering will be required before seeing better results. That is often the case with MLPs. Hence, give it some time when testing new methods as they tend to yield better results with some tuning.\n",
        "\n",
        "\n",
        "Below, you can find decision boundaries derived using logistic regression, k-NN, SVM and MLP. Take a moment to look at the plots and draw conclusions on their classification capacity and overall quality of decision boundary."
      ],
      "metadata": {
        "id": "YrP1X51bYhnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classification methods visualization with Accuracy\n",
        "\n",
        "# --- Generate two spiral dataset ---\n",
        "def generate_spiral(n_points, noise=0.2):\n",
        "    theta = np.sqrt(np.random.rand(n_points)) * 4 * np.pi  # Angle\n",
        "    r = theta\n",
        "    x1 = r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y1 = r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    x2 = -r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y2 = -r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    X = np.vstack((np.column_stack((x1, y1)), np.column_stack((x2, y2))))\n",
        "    y = np.array([0]*n_points + [1]*n_points)\n",
        "    return X, y\n",
        "\n",
        "# --- Generate dataset ---\n",
        "np.random.seed(42)\n",
        "X, y = generate_spiral(300, noise=0.25)\n",
        "\n",
        "# --- Create mesh grid for plotting ---\n",
        "h = 0.1\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# --- Logistic Regression with Polynomial Features ---\n",
        "logreg = make_pipeline(PolynomialFeatures(degree=5), LogisticRegression(max_iter=15000))\n",
        "logreg.fit(X, y)\n",
        "Z_logreg = logreg.predict(grid).reshape(xx.shape)\n",
        "acc_logreg = accuracy_score(y, logreg.predict(X))\n",
        "\n",
        "# --- k-Nearest Neighbors ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X, y)\n",
        "Z_knn = knn.predict(grid).reshape(xx.shape)\n",
        "acc_knn = accuracy_score(y, knn.predict(X))\n",
        "\n",
        "# --- Multi-Layer Perceptron ---\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', max_iter=8000, random_state=42)\n",
        "mlp.fit(X, y)\n",
        "Z_mlp = mlp.predict(grid).reshape(xx.shape)\n",
        "acc_mlp = accuracy_score(y, mlp.predict(X))\n",
        "\n",
        "# --- SVM with RBF kernel ---\n",
        "svm = SVC(kernel='rbf', gamma='scale', C=100000.0)\n",
        "svm.fit(X, y)\n",
        "Z_svm = svm.predict(grid).reshape(xx.shape)\n",
        "acc_svm = accuracy_score(y, svm.predict(X))\n",
        "\n",
        "# --- Plot all models ---\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Logistic Regression\n",
        "axs[0, 0].contourf(xx, yy, Z_logreg, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[0, 0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[0, 0].set_title(f\"Logistic Regression\\nAcc: {acc_logreg:.2f}\")\n",
        "axs[0, 0].set_xlabel(\"x\")\n",
        "axs[0, 0].set_ylabel(\"y\")\n",
        "axs[0, 0].grid(True)\n",
        "axs[0, 0].set_aspect('equal')\n",
        "\n",
        "# k-NN\n",
        "axs[0, 1].contourf(xx, yy, Z_knn, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[0, 1].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[0, 1].set_title(f\"k-NN (k=5)\\nAcc: {acc_knn:.2f}\")\n",
        "axs[0, 1].set_xlabel(\"x\")\n",
        "axs[0, 1].set_ylabel(\"y\")\n",
        "axs[0, 1].grid(True)\n",
        "axs[0, 1].set_aspect('equal')\n",
        "\n",
        "# SVM\n",
        "axs[1, 0].contourf(xx, yy, Z_svm, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[1, 0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[1, 0].set_title(f\"SVM (RBF kernel)\\nAcc: {acc_svm:.2f}\")\n",
        "axs[1, 0].set_xlabel(\"x\")\n",
        "axs[1, 0].set_ylabel(\"y\")\n",
        "axs[1, 0].grid(True)\n",
        "axs[1, 0].set_aspect('equal')\n",
        "\n",
        "# MLP\n",
        "axs[1, 1].contourf(xx, yy, Z_mlp, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[1, 1].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[1, 1].set_title(f\"MLP (100,100) relu\\nAcc: {acc_mlp:.2f}\")\n",
        "axs[1, 1].set_xlabel(\"x\")\n",
        "axs[1, 1].set_ylabel(\"y\")\n",
        "axs[1, 1].grid(True)\n",
        "axs[1, 1].set_aspect('equal')\n",
        "\n",
        "plt.suptitle(\"Classification Comparison on Two-Spiral Dataset\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kX-oXAB1OZ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first glance, we can see that logistic regression has the lowest accuracy score, which is reflected in a boundary that poorly fits the training data. This is due to the simplicity of this technique. It struggles when dealing with more complicated distributions. On the other hand, k-NN, SVM and MLP both achieve 100% accuracy. However, the two differ in the way that new points get classified. Now, if we wanted to sample a new point and check which class it belongs to, k-NN would need to check that point against every point in the data set in order to find the k nearest neighbors. On the other hand, SVM relies on a model derived from the data allowing for low computational overhead when classifying new data. Similarly,  MLP uses its network of weights to define the decision boundary. This approach makes it a lot easier and faster to classify data after the network is trained."
      ],
      "metadata": {
        "id": "w1wFo83wDeF5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCxYwB1OPRbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}