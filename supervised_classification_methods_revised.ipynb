{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustafbjurstam/ML-retreat-tekmek-2025/blob/main/supervised_classification_methods_revised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "1. Supervised Classification Methods  \n",
        "    1.1 Introduction  \n",
        "    1.2 What does \"Supervised Classification\" mean?  \n",
        "    1.3 Roadmap  \n",
        "2. Motivating Example: Fluid Flow Classification  \n",
        "    2.1 Problem setup  \n",
        "    2.2 Mathematical models  \n",
        "3. Logistic Regression  \n",
        "    3.1 Intuition and how it works  \n",
        "    3.2 Worked-out example  \n",
        "    3.3 Fluid velocity profile application  \n",
        "4. k-Nearest Neighbors (k-NN)  \n",
        "    4.1 Intuition and how it works  \n",
        "    4.2 Distance metrics  \n",
        "    4.3 Advantages and disadvantages  \n",
        "5. Support Vector Machine (SVM)  \n",
        "    5.1 Hard and Soft Margins  \n",
        "    5.2 Non-linear data  \n",
        "6. Multi-Layer Perceptron (MLP)  \n",
        "7. Classification methods comparison\n",
        "\n",
        "---\n",
        "\n",
        "# Supervised Classification Methods\n",
        "\n",
        "Our goal is to understand what a \"supervised\" machine learning method is and explore how different machine learning techniques can be used to classify data when the class labels are known in advance.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook introduces the concepts of **linear regression** and **logistic regression**, two fundamental techniques in supervised machine learning. Using practical engineering-inspired examples, we will explore how these models work, how to fit them to data, and how to interpret their results. We will also discuss their assumptions, limitations, and how they can be extended to more complex problems.\n",
        "\n",
        "---\n",
        "\n",
        "## What does \"Supervised Classification\" mean?\n",
        "\n",
        "In **supervised learning**, we train models on data where each example comes with a known **label**. The model's job is to learn a function that maps inputs to their correct outputs. Once trained, the model can be used to predict labels for new, unseen data.\n",
        "\n",
        "In **classification**, the labels are **discrete categories**. For example:\n",
        "- Email: *spam* or *not spam*\n",
        "- Medical image: *benign* or *malignant*\n",
        "- Fluid flow: *laminar* or *turbulent*\n",
        "\n",
        "In this notebook, we focus on **binary classification**, where there are only two possible classes. However, there are methods that can handle multiple categories.\n",
        "\n",
        "---\n",
        "\n",
        "## Roadmap\n",
        "\n",
        "For each classification method, we'll follow a consistent structure:\n",
        "1. **Intuition and how it works** — Understand the basic idea behind the method, key mechanics and parameters.\n",
        "2. **Visual example or demonstration** — Visual illustration in low dimensions.\n",
        "3. **Fluid velocity profile application** — The setup for this example will be defined in the next section. We will test the performance of different methods using this example.\n",
        "\n",
        "By the end of this notebook, you'll have an intuitive and practical understanding of:\n",
        "- Logistic Regression  \n",
        "- k-Nearest Neighbors (k-NN)\n",
        "- Support Vector Machine (SVM)\n",
        "- Multi-Layer Perceptron (MLP)\n",
        "- Classification methods comparison\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "hbRiB0s1bDXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup: Import all required libraries\n",
        "\n",
        "# Core scientific computing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Interactive widgets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, FloatSlider, FloatLogSlider\n",
        "from IPython.display import display\n",
        "\n",
        "# Machine learning - datasets\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Machine learning - preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Machine learning - pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Machine learning - models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Machine learning - model selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Machine learning - metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "C-Cd_CoNyUXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivating Example: Fluid Flow Classification\n",
        "\n",
        "To make our exploration concrete, we'll use a **fluid mechanics** example: classifying velocity profiles in pipe flow.\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "**Physical system**: Fluid flowing through a circular pipe (like water through a garden hose).\n",
        "\n",
        "When we look at a **cross-section** of the pipe, the velocity varies from the center (fastest) to the walls (zero due to friction). We measure the velocity at 51 points across the diameter, creating a **51-dimensional feature vector** for each flow observation.\n",
        "\n",
        "**Two flow regimes**:\n",
        "- **Laminar flow**: Smooth, parabolic velocity profile (low flow rates)\n",
        "- **Turbulent flow**: Flatter, more irregular profile (high flow rates)\n",
        "\n",
        "## Geometry and Normalization\n",
        "\n",
        "**Radial position $ y $**: We normalize the pipe radius so $ y \\in [-1, 1] $, where:\n",
        "- $ y = -1 $: left pipe wall\n",
        "- $ y = 0 $: pipe centerline\n",
        "- $ y = +1 $: right pipe wall\n",
        "\n",
        "This normalization makes the problem **scale-independent** — the same model works whether we're analyzing a millimeter capillary or a meter-wide industrial pipe.\n",
        "\n",
        "**Velocity $ v(y) $**: Normalized by the centerline velocity $ u_{\\infty} $ (the maximum speed at the pipe center).\n",
        "\n",
        "## Mathematical Models\n",
        "\n",
        "- **Laminar flow** (parabolic profile):  \n",
        "  $$v(y) = u_{\\infty} - y^2 + \\varepsilon$$\n",
        "\n",
        "- **Turbulent flow** (power-law profile):  \n",
        "  $$v(y) = u_{\\infty} - C |y|^{7} + \\varepsilon$$\n",
        "\n",
        "Where $ \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2) $ represents measurement noise.\n",
        "\n",
        "The plot below shows example profiles. Notice how laminar flow has a sharper curvature near the walls, while turbulent flow is flatter in the center due to enhanced mixing."
      ],
      "metadata": {
        "id": "py62r0WIuUF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWNb2S4faSPB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generating laminar and turbulent velocity profiles\n",
        "\n",
        "def generate_velocity_profile(flow_type='laminar', u_infty=1.0, C=1.0, sigma=0.01, n_points=51, seed=None):\n",
        "    \"\"\"\n",
        "    Generate a velocity profile across a pipe diameter.\n",
        "\n",
        "    Parameters:\n",
        "    - flow_type (str): 'laminar' or 'turbulent'\n",
        "    - u_infty (float): centerline velocity (maximum speed)\n",
        "    - C (float): turbulent profile shape constant\n",
        "    - sigma (float): measurement noise level\n",
        "    - n_points (int): measurement points across diameter\n",
        "    - seed (int or None): random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    - y (np.ndarray): normalized radial positions [-1, 1]\n",
        "    - v (np.ndarray): velocity at each position\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Normalized positions: -1 (left wall) to +1 (right wall)\n",
        "    y = np.linspace(-1, 1, n_points)\n",
        "\n",
        "    # Measurement noise\n",
        "    noise = np.random.normal(loc=0.0, scale=sigma, size=n_points)\n",
        "\n",
        "    if flow_type == 'laminar':\n",
        "        v = u_infty - y**2 + noise\n",
        "    elif flow_type == 'turbulent':\n",
        "        v = u_infty - C * (np.abs(y))**(7) + noise\n",
        "    else:\n",
        "        raise ValueError(\"flow_type must be either 'laminar' or 'turbulent'\")\n",
        "\n",
        "    return y, v\n",
        "\n",
        "# Generate both profiles\n",
        "y, v_laminar = generate_velocity_profile('laminar', sigma=0.02, seed=42)\n",
        "_, v_turbulent = generate_velocity_profile('turbulent', C=1, sigma=0.02, seed=42)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y, v_laminar, label='Laminar', linewidth=2)\n",
        "plt.plot(y, v_turbulent, label='Turbulent', linewidth=2)\n",
        "plt.xlabel(\"Normalized radial position y (pipe wall to wall)\")\n",
        "plt.ylabel(\"Normalized velocity v(y) / u∞\")\n",
        "plt.title(\"Velocity Profiles Across Pipe Diameter\")\n",
        "plt.axvline(x=-1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "plt.axvline(x=1, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
        "plt.text(-1, 0.05, 'Left\\nwall', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.text(1, 0.05, 'Right\\nwall', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.text(0, 1.05, 'Center', ha='center', fontsize=9, alpha=0.6)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "## Intuition and how it works\n",
        "\n",
        "Logistic regression is a method that was introduced in the previous workshop. It is one of the most basic types of classification techniques. It relies on trying to fit a sigmoidal function in a way to maximize the likelihood of all data points. Data points can be multi-dimensional. However, any point put through the sigmoidal function will be a scalar. For example, let's assume there is a 4 dimensional data point $(x_1, x_2, x_3, x_4)$. The boundary condition used for the classification problem is defined as $d(\\vec{x}) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4$. The scalar value of $d(\\vec{x})$ is then used as input to the sigmoidal function which produces the final likelihood value $h_{\\theta}(\\vec{x}) = \\sigma(d(\\vec{x})) \\in [-1, 1]$. In our example, logistic regression relies on a mapping from $R^4$ to $R$ when determining the optimal decision boundary (i.e. the coefficients of $\\theta_i, i \\in \\{1, 2, 3, 4\\}$).\n",
        "\n",
        "## Worked-out example\n",
        "\n",
        "**Example 1** uses logistic regression to fit a decision boundary in the form of:\n",
        "\n",
        "$$d(x_1, x_2) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
        "\n",
        "However, logistic regression can be used in combination with other kinds of decision boundaries as long as they are linear with respect to their coefficients. In our case, **Example 2** is able to fit a circural decision boundary because it is defined as:\n",
        "\n",
        "$$d(x_1, x_2) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1 x_2 + \\theta_4 x_1^2 + \\theta_5 x_2^2$$\n",
        "\n",
        "Such $d(x_1, x_2)$ gives more flexibility as the decision boundary can be defined as a polynomial. Here of degree $n=2$."
      ],
      "metadata": {
        "id": "TTfAZ7Wh5Wb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Logistic regression Example 1\n",
        "\n",
        "# Generate linear data\n",
        "n_samples = 200\n",
        "\n",
        "# Class 0\n",
        "x0 = np.random.normal(loc=[1, 1], scale=0.5, size=(n_samples // 2, 2))\n",
        "# Class 1\n",
        "x1 = np.random.normal(loc=[3, 3], scale=0.5, size=(n_samples // 2, 2))\n",
        "\n",
        "X_linear = np.vstack((x0, x1))\n",
        "y_linear = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
        "\n",
        "# Fit logistic regression\n",
        "model_linear = LogisticRegression()\n",
        "model_linear.fit(X_linear, y_linear)\n",
        "\n",
        "# Plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(-1, 4.5, 300), np.linspace(-1, 4.5, 300))\n",
        "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model_linear.predict_proba(grid_points)[:, 1].reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.2, colors=[\"blue\", \"red\"])\n",
        "plt.scatter(X_linear[y_linear == 0, 0], X_linear[y_linear == 0, 1], label=\"Class 0\", alpha=0.6)\n",
        "plt.scatter(X_linear[y_linear == 1, 0], X_linear[y_linear == 1, 1], label=\"Class 1\", alpha=0.6)\n",
        "plt.title(\"Linear Boundary: Logistic Regression\")\n",
        "plt.xlabel(\"x0\")\n",
        "plt.ylabel(\"x1\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "efcgyfWfCJJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Logistic regression Example 2\n",
        "# Generate circular data\n",
        "np.random.seed(0)\n",
        "n_samples = 200\n",
        "radius = 1.0\n",
        "\n",
        "# Inner circle (class 0)\n",
        "r0 = radius * np.sqrt(np.random.rand(n_samples // 2))\n",
        "theta0 = 2 * np.pi * np.random.rand(n_samples // 2)\n",
        "x0 = np.stack((r0 * np.cos(theta0), r0 * np.sin(theta0)), axis=1)\n",
        "\n",
        "# Outer ring (class 1)\n",
        "r1 = radius + 0.5 * np.random.rand(n_samples // 2)\n",
        "theta1 = 2 * np.pi * np.random.rand(n_samples // 2)\n",
        "x1 = np.stack((r1 * np.cos(theta1), r1 * np.sin(theta1)), axis=1)\n",
        "\n",
        "# Combine and label\n",
        "X_circle = np.vstack((x0, x1))\n",
        "y_circle = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
        "\n",
        "# Fit logistic regression with polynomial features\n",
        "model_circle = make_pipeline(PolynomialFeatures(degree=2), LogisticRegression())\n",
        "model_circle.fit(X_circle, y_circle)\n",
        "\n",
        "# Plotting\n",
        "xx, yy = np.meshgrid(np.linspace(-2, 2, 300), np.linspace(-2, 2, 300))\n",
        "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model_circle.predict_proba(grid_points)[:, 1].reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.2, colors=[\"blue\", \"red\"])\n",
        "plt.scatter(X_circle[y_circle == 0, 0], X_circle[y_circle == 0, 1], label=\"Class 0\", alpha=0.6)\n",
        "plt.scatter(X_circle[y_circle == 1, 0], X_circle[y_circle == 1, 1], label=\"Class 1\", alpha=0.6)\n",
        "plt.title(\"Nonlinear Boundary: Logistic Regression with Polynomial Features\")\n",
        "plt.xlabel(\"x0\")\n",
        "plt.ylabel(\"x1\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GDoIhkWb-Hb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fluid velocity profile application\n",
        "\n",
        "As mentioned before, logistic regression can go beyond the visual confinements of 2D and 3D. The question is, how well can it distinguish between the laminar and turbulent velocity profile?"
      ],
      "metadata": {
        "id": "QOq0RhGSE3dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flow velocity profile application\n",
        "\n",
        "# --- Generate labeled dataset ---\n",
        "n_samples_per_class = 200\n",
        "n_points = 51\n",
        "sigma = 0.05  # increased noise to make the task more realistic\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(n_samples_per_class):\n",
        "    _, v_laminar = generate_velocity_profile('laminar', sigma=sigma)\n",
        "    _, v_turbulent = generate_velocity_profile('turbulent', sigma=sigma)\n",
        "    X.append(v_laminar)\n",
        "    y.append(0)\n",
        "    X.append(v_turbulent)\n",
        "    y.append(1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Split into train/test sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Fit logistic regression model ---\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate the model ---\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# --- Optional: show confusion matrix ---\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Visualize a few examples ---\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "y_vals = np.linspace(-1, 1, n_points)\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    idx = i\n",
        "    ax.plot(y_vals, X_test[idx], label=\"Velocity profile\")\n",
        "    pred = clf.predict(X_test[idx].reshape(1, -1))[0]\n",
        "    true = y_test[idx]\n",
        "    ax.set_title(f\"True: {'Laminar' if true == 0 else 'Turbulent'} | Pred: {'Laminar' if pred == 0 else 'Turbulent'}\")\n",
        "    ax.set_xlabel(\"y\")\n",
        "    ax.set_ylabel(\"v(y)\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dF7k4lC4F2Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, logistic regression is just fine for the task. It manages to classify every flow profile with 100% accuracy."
      ],
      "metadata": {
        "id": "gV2DstYRJ2gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors (k-NN)\n",
        "\n",
        "## Intuition and how it works\n",
        "\n",
        "k-Nearest Neighbors (k-NN) has a very intuitive principle of work. In fact, it's name already explains it. Unlike logistic regression, k-NN can be used for multi-class classification \"out of the box\". Here is the way k-NN works:\n",
        "\n",
        "1. Pick a data point you want to classify.\n",
        "2. Find k nearest neighbors. A neighbor is one of k data points closest to the point we want to classify.\n",
        "3. Assign the class to the point that corresponds to the majority class within the k neighbors.\n",
        "\n",
        "And that is all, nice and simple! The only parameters we need to decide on are k and the way that the distance between points is calculated. If we choose too low k then the results can be noisy since few neighbors decide on the classification. On the other hand, picking too high of k can lead to outright rejection of classes that do not have many points in the data set. Play around with the demo below and try to recreate those two scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "v_0RMmPKYHZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors (k-NN)\n",
        "\n",
        "## Intuition and how it works\n",
        "\n",
        "k-Nearest Neighbors (k-NN) has a very intuitive principle of work. In fact, its name already explains it. Unlike logistic regression, k-NN can be used for multi-class classification \"out of the box\". Here is the way k-NN works:\n",
        "\n",
        "1. Pick a data point you want to classify.\n",
        "2. Find the k nearest neighbors.\n",
        "3. Assign the class to the point that corresponds to the majority class within the k neighbors.\n",
        "\n",
        "And that is all — nice and simple! The only parameters we need to decide on are **k** and the way that the **distance between points is calculated**.\n",
        "\n",
        "## Distance metrics\n",
        "\n",
        "Different datasets and problem domains can benefit from different distance metrics. Here are some common options:\n",
        "\n",
        "- **Euclidean distance** (default): Good for continuous, geometrically well-behaved data.\n",
        "- **Manhattan distance**: More robust when the data contains outliers or if dimensions are not directly comparable.\n",
        "- **Minkowski distance**: A generalization of both Euclidean and Manhattan (you can tune its power parameter).\n",
        "- **Cosine similarity**: Often used for text or high-dimensional sparse data, where direction matters more than magnitude.\n",
        "\n",
        "Choosing the right distance metric helps k-NN reflect the true \"closeness\" between data points in a meaningful way for your specific problem.\n",
        "\n",
        "## Advantages and disadvantages\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Very simple and intuitive.\n",
        "- Naturally handles multi-class classification.\n",
        "- No training phase — the model just stores the data.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- Computationally expensive at prediction time (especially for large datasets).\n",
        "- Performance can degrade in high-dimensional spaces (\"curse of dimensionality\").\n",
        "- Sensitive to irrelevant or redundant features and scaling of data.\n",
        "\n",
        "Play around with the demo below and try to recreate different behaviors by changing **k** and experimenting with how points are distributed. Try small values of k to see overfitting in action, or high values to observe how the model smooths over finer details.\n"
      ],
      "metadata": {
        "id": "MqtC4JK2v_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive k-NN demo\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# === Generate dataset ===\n",
        "centers = [(-2, -2), (2, 0), (-1, 4), (3, 7)]\n",
        "cluster_std = [1.0, 1.5, 0.5, 1.0]\n",
        "X, y_labels = make_blobs(n_samples=[100, 200, 5, 100], centers=centers, cluster_std=cluster_std, random_state=42)\n",
        "\n",
        "# === Fit once globally ===\n",
        "knn_global = KNeighborsClassifier()\n",
        "knn_global.fit(X, y_labels)\n",
        "\n",
        "# === Define plotting function ===\n",
        "def plot_knn_decision_boundary(x=0.0, y=0.0, k=5, show_lines=True):\n",
        "    # Convert slider inputs to native float (in case they are numpy scalars)\n",
        "    x = float(x)\n",
        "    y = float(y)\n",
        "\n",
        "    # Refit classifier with selected k\n",
        "    knn_global.set_params(n_neighbors=k)\n",
        "    knn_global.fit(X, y_labels)\n",
        "\n",
        "    # Mesh grid for decision boundary\n",
        "    h = 0.1\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = knn_global.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Query point and neighbors\n",
        "    query_point = np.array([[x, y]])\n",
        "    query_class = knn_global.predict(query_point)[0]\n",
        "    print(f\"color is: {query_class}\")\n",
        "    distances, indices = knn_global.kneighbors(query_point)\n",
        "    neighbor_pts = X[indices[0]]\n",
        "\n",
        "    # Define class colors\n",
        "    color_map = {\n",
        "    0: 'blue',\n",
        "    1: 'red',\n",
        "    2: 'pink',\n",
        "    3: 'cyan',\n",
        "    }\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(8, 7))\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.tab10)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y_labels, cmap=plt.cm.tab10, edgecolor='k', s=50)\n",
        "\n",
        "    if show_lines:\n",
        "        for pt in neighbor_pts:\n",
        "            ax.plot([x, pt[0]], [y, pt[1]], 'k--', linewidth=1)\n",
        "\n",
        "    ax.scatter(neighbor_pts[:, 0], neighbor_pts[:, 1],\n",
        "               facecolors='none', edgecolors='black', s=150, linewidths=2, label='Neighbors')\n",
        "\n",
        "    ax.scatter(x, y, c=color_map[query_class], edgecolor='k', s=200, marker='o', label='Query Point')\n",
        "\n",
        "    ax.set_title(f\"Interactive k-NN Classification (k={k})\", fontsize=14)\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# === Sliders ===\n",
        "x_slider = widgets.FloatSlider(value=2.0, min=-4.0, max=6.0, step=0.1, description='x:')\n",
        "y_slider = widgets.FloatSlider(value=6.0, min=-4.0, max=9.0, step=0.1, description='y:')\n",
        "k_slider = widgets.IntSlider(value=5, min=1, max=20, step=1, description='k:')\n",
        "lines_toggle = widgets.Checkbox(value=True, description='Show lines to neighbors')\n",
        "\n",
        "interactive_plot = widgets.interactive_output(\n",
        "    plot_knn_decision_boundary,\n",
        "    {'x': x_slider, 'y': y_slider, 'k': k_slider, 'show_lines': lines_toggle}\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([x_slider, y_slider, k_slider, lines_toggle])\n",
        "display(ui, interactive_plot)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Gbv7BAd9_jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flow velocity profile application\n",
        "\n",
        "# --- Generate labeled dataset ---\n",
        "n_samples_per_class = 200\n",
        "n_points = 51\n",
        "sigma = 0.05  # noise level\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(n_samples_per_class):\n",
        "    _, v_laminar = generate_velocity_profile('laminar', sigma=sigma)\n",
        "    _, v_turbulent = generate_velocity_profile('turbulent', sigma=sigma)\n",
        "    X.append(v_laminar)\n",
        "    y.append(0)\n",
        "    X.append(v_turbulent)\n",
        "    y.append(1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Split into train/test sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Fit k-NN model ---\n",
        "k = 5\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate the model ---\n",
        "y_pred = knn.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# --- Optional: show confusion matrix ---\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Visualize a few examples ---\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "y_vals = np.linspace(-1, 1, n_points)\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    idx = i\n",
        "    ax.plot(y_vals, X_test[idx], label=\"Velocity profile\")\n",
        "    pred = knn.predict(X_test[idx].reshape(1, -1))[0]\n",
        "    true = y_test[idx]\n",
        "    ax.set_title(f\"True: {'Laminar' if true == 0 else 'Turbulent'} | Pred: {'Laminar' if pred == 0 else 'Turbulent'}\")\n",
        "    ax.set_xlabel(\"y\")\n",
        "    ax.set_ylabel(\"v(y)\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NWVaQjWODxQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, k-NN manages to classify the flows just fine."
      ],
      "metadata": {
        "id": "MkXCRCvaKqU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine (SVM)\n",
        "\n",
        "Another supervised machine learning classification method is the Support Vector Machine (SVM). The main idea behind it is to separate two classes using a hyperplane. Examples of hyperplanes are:\n",
        "\n",
        "- 1D: point $h_{\\theta}(x_1) = \\theta_0 + \\theta_1 x_1$\n",
        "- 2D: line $h_{\\theta}(x_1, x_2) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2$\n",
        "- 3D: plane $h_{\\theta}(x_1, x_2, x_3) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$\n",
        "- ... and so on\n",
        "\n",
        "The main advantage of SVM is that the optimization problem it poses can be solved using a method called **quadratic programming**, which is computationally efficient. Contrast this with iterative methods such as gradient descent, which require multiple updates to converge to a solution. Because of this, SVM is very efficient for classifying **linearly separable** data.\n",
        "\n",
        "Even though this seems like a limitation, we will later show how this approach can still be applied to data that is not linearly separable, through kernel methods. However, we will not cover the mathematical details of SVMs, as that is beyond the scope of this workshop.\n",
        "\n",
        "---\n",
        "\n",
        "Below is an example of two clusters that need to be separated with a line. What makes one line better than another?"
      ],
      "metadata": {
        "id": "SOBcKqQVWaHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure 1\n",
        "from IPython.display import display, Image, SVG, Video\n",
        "display(SVG(filename='/content/decision-boundaries.svg'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5U-4XYbEZdp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, the line on the left in Figure 1 appears to divide the clusters in the most natural way. This is also how SVM attempts to separate classes: by finding the hyperplane that maximizes the **margin** between the two classes (see Figure 2).\n",
        "\n",
        "SVM’s objective is to find a decision boundary that:\n",
        "\n",
        "- maximizes the margin between the nearest points of the two classes,\n",
        "- and still correctly classifies the training examples.\n",
        "\n",
        "The points that lie closest to the hyperplane and define the margin are called **support vectors** (see Figure 2). They are critical because the margin is \"pushed\" up against them from both sides."
      ],
      "metadata": {
        "id": "CWn4_nFsabuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure 2\n",
        "display(SVG(filename='/content/terminology.svg'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jB91gt1ndxjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple demo below presents the core idea behind SVM. Try to manually find a line that separates the classes while maximizing the margin."
      ],
      "metadata": {
        "id": "AhLjuMb2iRO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SVM margin demo\n",
        "\n",
        "# --- Generate two clusters ---\n",
        "np.random.seed(1)\n",
        "n_points = 10\n",
        "cluster_1 = np.random.randn(n_points, 2) + np.array([-2, 4])\n",
        "cluster_2 = np.random.randn(n_points, 2) + np.array([2, -4])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1]*n_points + [-1]*n_points)\n",
        "\n",
        "def svm_margin_plot(slope):\n",
        "    w = -np.array([slope, -1])  # Normal vector to decision boundary\n",
        "    w = w / np.linalg.norm(w)  # Normalize\n",
        "\n",
        "    # Project points onto normal vector\n",
        "    projections = X @ w\n",
        "    margin_pos = projections[y == 1].min()\n",
        "    margin_neg = projections[y == -1].max()\n",
        "    margin = 0.5 * (margin_pos - margin_neg)\n",
        "    decision_offset = 0.5 * (margin_pos + margin_neg)\n",
        "\n",
        "    def plot_margin_line(ax, offset, style='k--', label=None):\n",
        "        midpoint = offset * w\n",
        "        direction = np.array([-w[1], w[0]])  # Perpendicular direction\n",
        "        line_points = np.array([midpoint + t * direction for t in [-10, 10]])\n",
        "        ax.plot(line_points[:, 0], line_points[:, 1], style, label=label)\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.scatter(cluster_1[:, 0], cluster_1[:, 1], c='red', label='Class +1')\n",
        "    ax.scatter(cluster_2[:, 0], cluster_2[:, 1], c='blue', label='Class -1')\n",
        "\n",
        "    plot_margin_line(ax, decision_offset, style='k-', label='Decision boundary')\n",
        "    plot_margin_line(ax, margin_pos, style='k--', label='Margin boundaries')\n",
        "    plot_margin_line(ax, margin_neg, style='k--')\n",
        "\n",
        "    # Highlight support vectors\n",
        "    sv_pos = X[y == 1][np.argmin(projections[y == 1])]\n",
        "    sv_neg = X[y == -1][np.argmax(projections[y == -1])]\n",
        "    ax.scatter(*sv_pos, s=150, facecolors='none', edgecolors='red', linewidths=2, label='Support vectors')\n",
        "    ax.scatter(*sv_neg, s=150, facecolors='none', edgecolors='blue', linewidths=2)\n",
        "\n",
        "    ax.set_xlim(-10, 10)\n",
        "    ax.set_ylim(-10, 15)\n",
        "    ax.set_aspect('equal', adjustable='box')  # Changed this line\n",
        "    ax.set_title(f\"SVM Margin and Decision Boundary (slope = {slope:.2f})\")\n",
        "    ax.set_xlabel(\"x₁\")\n",
        "    ax.set_ylabel(\"x₂\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive slider\n",
        "interact(svm_margin_plot, slope=FloatSlider(value=-0.5, min=-1, max=20, step=0.1));"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PZo1NaCFtRcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hard and Soft Margins\n",
        "\n",
        "So far, we have looked at the case where two clusters can be perfectly separated by a line. However, in many real-world datasets, misclassifications are inevitable, and a perfect separation is not possible. In such cases, the standard SVM cannot find a valid decision boundary.\n",
        "\n",
        "What we need is some flexibility in the margin — a way to tolerate a small number of misclassified points when determining the separating hyperplane. This approach is known as the **soft margin**, as opposed to the **hard margin**, which assumes that the data is linearly separable with no exceptions.\n",
        "\n",
        "The **soft margin** SVM introduces a mechanism to allow certain violations of the margin — that is, to allow some points to fall on the wrong side of the boundary or within the margin. This makes it more robust and better suited for real-world data.\n",
        "\n",
        "The trade-off between maximizing the margin and minimizing classification errors is controlled by the parameter $C$. This parameter determines how much **penalty** is assigned to misclassified points:\n",
        "\n",
        "- A **large $C$** forces the model to classify all training examples correctly (hard margin behavior), possibly at the cost of a smaller margin.\n",
        "- A **small $C$** allows for more margin violations (soft margin), leading to a wider margin and potentially better generalization.\n",
        "\n",
        "Soft-margin SVM is the **default** behavior in most machine learning libraries and toolboxes.\n",
        "\n",
        "Below is a plot that shows the effects of different $C$ values. A hard margin can be approximated by using a very large $C$.\n"
      ],
      "metadata": {
        "id": "BD1YyaMj4COH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hard vs soft margin plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Generate linearly separable data ---\n",
        "np.random.seed(0)\n",
        "n_samples = 20\n",
        "cluster_1 = np.random.randn(n_samples, 2) + np.array([2, 2])\n",
        "cluster_2 = np.random.randn(n_samples, 2) + np.array([-2, -2])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1]*n_samples + [-1]*n_samples)\n",
        "\n",
        "# --- Fit hard margin and soft margin SVM ---\n",
        "svm_hard = SVC(kernel='linear', C=1e10)\n",
        "svm_soft = SVC(kernel='linear', C=0.1)\n",
        "svm_hard.fit(X, y)\n",
        "svm_soft.fit(X, y)\n",
        "\n",
        "# --- Create mesh for decision boundary visualization ---\n",
        "h = 0.05\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "Z_hard = svm_hard.decision_function(grid).reshape(xx.shape)\n",
        "Z_soft = svm_soft.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "titles = ['Hard Margin SVM (C=1e10)', 'Soft Margin SVM (C=0.1)']\n",
        "svms = [svm_hard, svm_soft]\n",
        "Zs = [Z_hard, Z_soft]\n",
        "\n",
        "for ax, title, svm, Z in zip(axs, titles, svms, Zs):\n",
        "    ax.contourf(xx, yy, Z > 0, alpha=0.2, cmap=plt.cm.coolwarm)\n",
        "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
        "\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    ax.scatter(svm.support_vectors_[:, 0],\n",
        "               svm.support_vectors_[:, 1],\n",
        "               s=150, facecolors='none', edgecolors='k', linewidths=1.5, label='Support Vectors')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"x₁\")\n",
        "    ax.set_ylabel(\"x₂\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D-lGQfn85N7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, what happens if data is not fully separable due to noisy data and outliers? That is where the hard margin version of SVM does not have any solution. The soft margin SVM however can handle such data. However, it is left up to the user to decide on an appropriate value of $C$ which will result in a decision boundary that can be generalized to the 'real' data distribution."
      ],
      "metadata": {
        "id": "XGdFWO8Fzvuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Non-linearly separable data with soft margin SVM demo\n",
        "\n",
        "# --- Generate non-linearly separable data ---\n",
        "np.random.seed(1)\n",
        "n_samples = 20\n",
        "cluster_1 = 1.5 * np.random.randn(n_samples, 2) + np.array([1, 1])\n",
        "cluster_2 = np.random.randn(n_samples, 2) + np.array([-1, -1])\n",
        "X = np.vstack((cluster_1, cluster_2))\n",
        "y = np.array([1] * n_samples + [-1] * n_samples)\n",
        "\n",
        "# --- Create mesh grid for decision boundaries ---\n",
        "h = 0.05\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "def plot_svm(C):\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X, y)\n",
        "    Z = clf.decision_function(grid).reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.2, cmap=plt.cm.coolwarm)\n",
        "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.scatter(clf.support_vectors_[:, 0],\n",
        "                clf.support_vectors_[:, 1],\n",
        "                s=150, facecolors='none', edgecolors='k', linewidths=1.5, label='Support Vectors')\n",
        "\n",
        "    plt.title(f\"Soft Margin SVM (C={C:.3f})\")\n",
        "    plt.xlabel(\"x₁\")\n",
        "    plt.ylabel(\"x₂\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create interactive slider (log scale for better exploration)\n",
        "interact(plot_svm, C=FloatLogSlider(value=1.0, base=10, min=-2, max=3, step=0.1, description='C'))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v-ZUk_rJ5qqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-linear data\n",
        "\n",
        "At this point, it might seem like we’ve put a lot of effort into a method that can only separate data with a straight line. Fortunately, SVM has a powerful technique to handle more complex scenarios. When the raw data is not linearly separable, SVM can map it into a higher-dimensional space where a hyperplane *can* separate the clusters.\n",
        "\n",
        "This idea is often called the \"kernel trick\" and is one of the key strengths of SVM. The figure below illustrates how mapping to a higher-dimensional space can make separation possible even when it looks impossible in the original feature space. The original data set $(x_1, x_2)$ is mapped into 3D space $(x_1, x_2) \\mapsto (z_1, z_2, z_3) := (x_1, x_2, x_1^2 + x_2^2)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "DepbbtQ9_sDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Figure 3\n",
        "display(Image(filename='/content/higher-space-mapping.png'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ej1ov3xAg9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From *Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control*; S. L. Brunton, J. N. Kutz, p.236"
      ],
      "metadata": {
        "id": "zfSHOJIEBnur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flow velocity profile classification using SVM\n",
        "\n",
        "# --- Generate labeled dataset ---\n",
        "n_samples_per_class = 200\n",
        "n_points = 51\n",
        "sigma = 0.05  # same noise level\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for _ in range(n_samples_per_class):\n",
        "    _, v_laminar = generate_velocity_profile('laminar', sigma=sigma)\n",
        "    _, v_turbulent = generate_velocity_profile('turbulent', sigma=sigma)\n",
        "    X.append(v_laminar)\n",
        "    y.append(0)\n",
        "    X.append(v_turbulent)\n",
        "    y.append(1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Train-test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Fit SVM classifier ---\n",
        "svm_clf = SVC(kernel='linear')  # or try 'rbf' for more flexibility\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate ---\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Accuracy: {acc:.2f}\")\n",
        "\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Visualize a few test samples ---\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "y_vals = np.linspace(-1, 1, n_points)\n",
        "\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    idx = i\n",
        "    ax.plot(y_vals, X_test[idx], label=\"Velocity profile\")\n",
        "    pred = svm_clf.predict(X_test[idx].reshape(1, -1))[0]\n",
        "    true = y_test[idx]\n",
        "    ax.set_title(f\"True: {'Laminar' if true == 0 else 'Turbulent'} | Pred: {'Laminar' if pred == 0 else 'Turbulent'}\")\n",
        "    ax.set_xlabel(\"y\")\n",
        "    ax.set_ylabel(\"v(y)\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f1xTWI9p2ieY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM reaches 100% accuracy on the laminar vs turbulent flow classification."
      ],
      "metadata": {
        "id": "sQWCJq4BK9Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron (MLP)\n",
        "\n",
        "**Multi-Layer Perceptron (MLP)** is by far the most versatile and complex technique covered in this session. What makes MLPs so useful is the fact that they consist of many interconnected nodes (neurons), which can be organized into multiple layers — an input layer, one or more hidden layers, and an output layer. Thanks to this layered structure, MLPs can be used for a wide range of problems **including regression**, **classification**, and even more advanced tasks like **time series prediction** or **image recognition**. Most importantly, MLPs allow for learning complex non-linear functions that traditional linear models cannot capture.\n",
        "\n",
        "However, this flexibility comes with the challenge of selecting appropriate hyperparameters, which significantly affect performance:\n",
        "\n",
        "- **Hidden Layer Size**: The number of neurons in each hidden layer (e.g. one layer `(100,)` vs. two layers `(100, 50)`). More neurons and more layers increase model capacity but also the risk of overfitting.\n",
        "\n",
        "- **Number of Hidden Layers**: Increasing the depth (i.e. number of layers) enables learning more complex patterns, but also makes training harder.\n",
        "\n",
        "- **Activation Function**: Determines the non-linearity at each neuron. Common choices include 'relu', 'tanh', and 'logistic'.\n",
        "\n",
        "- **Learning Rate**: Controls how fast the model updates its weights during training. Too high can make learning unstable; too low can slow convergence.\n",
        "\n",
        "- **Regularization (alpha)**: Prevents overfitting by penalizing large weights. A higher alpha applies stronger regularization.\n",
        "\n",
        "- **Max Iterations**: The maximum number of training epochs. If the model doesn't converge, you might need to increase this.\n",
        "\n",
        "- **Solver**: The optimization algorithm used (e.g., `'adam'`, `'sgd'`, `'lbfgs'`). `'adam'` is often a good default for most tasks.\n",
        "\n",
        "Choosing good hyperparameters often requires experimentation and sometimes cross-validation. In practice, using tools like `GridSearchCV` or `RandomizedSearchCV` can help automate this process. We will cover MLPs in more detail in the upcoming sessions.\n",
        "\n",
        "In the following example, we see how an MLP can classify a highly non-linear dataset — something that simpler models cannot do."
      ],
      "metadata": {
        "id": "c0uRhpI1SCP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MLP example\n",
        "\n",
        "# --- Generate two spiral dataset ---\n",
        "def generate_spiral(n_points, noise=0.2):\n",
        "    theta = np.sqrt(np.random.rand(n_points)) * 4 * np.pi  # Angle\n",
        "    r = theta\n",
        "    x1 = r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y1 = r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    x2 = -r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y2 = -r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    X = np.vstack((np.column_stack((x1, y1)), np.column_stack((x2, y2))))\n",
        "    y = np.array([0]*n_points + [1]*n_points)\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_spiral(500, noise=0.15)\n",
        "\n",
        "# --- Split data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# --- Train MLP ---\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', max_iter=8000, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# --- Accuracy ---\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# --- Plot decision boundary ---\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    h = 0.05\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.4)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolors='k')\n",
        "    plt.title(\"MLP Classification of Two Spirals\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(mlp, X, y)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9Mgy0Cj6VNmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flow velocity profile classification using MLP\n",
        "\n",
        "# --- Generate labeled dataset ---\n",
        "n_samples_per_class = 200\n",
        "n_points = 51\n",
        "sigma = 0.05  # same noise level\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for _ in range(n_samples_per_class):\n",
        "    _, v_laminar = generate_velocity_profile('laminar', sigma=sigma)\n",
        "    _, v_turbulent = generate_velocity_profile('turbulent', sigma=sigma)\n",
        "    X.append(v_laminar)\n",
        "    y.append(0)\n",
        "    X.append(v_turbulent)\n",
        "    y.append(1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Train-test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Fit MLP classifier ---\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=3000, random_state=42)\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate ---\n",
        "y_pred = mlp_clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"MLP Accuracy: {acc:.2f}\")\n",
        "\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# --- Visualize a few test samples ---\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "y_vals = np.linspace(-1, 1, n_points)\n",
        "\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    idx = i\n",
        "    ax.plot(y_vals, X_test[idx], label=\"Velocity profile\")\n",
        "    pred = mlp_clf.predict(X_test[idx].reshape(1, -1))[0]\n",
        "    true = y_test[idx]\n",
        "    ax.set_title(f\"True: {'Laminar' if true == 0 else 'Turbulent'} | Pred: {'Laminar' if pred == 0 else 'Turbulent'}\")\n",
        "    ax.set_xlabel(\"y\")\n",
        "    ax.set_ylabel(\"v(y)\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u3VrPfgO28Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, MLP also achives 100% accuracy on the fluid flow example."
      ],
      "metadata": {
        "id": "DFgh7DwULFKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification methods comparison\n",
        "\n",
        "When looking at the velocity profiles classification shown throughout this notebook, we see that all of the methods achieved 100% accuracy. Often times, it is good to start with the simplest and most efficient method. If the results are not satisfactory for a given application, you should try out another method. Often times, some hyperparamter tinkering will be required before seeing better results. That is often the case with MLPs. Hence, give it some time when testing new methods as they tend to yield better results with some tuning.\n",
        "\n",
        "\n",
        "Below, you can find decision boundaries derived using logistic regression, k-NN, SVM and MLP. Take a moment to look at the plots and draw conclusions on their classification capacity and overall quality of decision boundary."
      ],
      "metadata": {
        "id": "YrP1X51bYhnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classification methods visualization with Accuracy\n",
        "\n",
        "# --- Generate two spiral dataset ---\n",
        "def generate_spiral(n_points, noise=0.2):\n",
        "    theta = np.sqrt(np.random.rand(n_points)) * 4 * np.pi  # Angle\n",
        "    r = theta\n",
        "    x1 = r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y1 = r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    x2 = -r * np.cos(theta) + np.random.randn(n_points) * noise\n",
        "    y2 = -r * np.sin(theta) + np.random.randn(n_points) * noise\n",
        "\n",
        "    X = np.vstack((np.column_stack((x1, y1)), np.column_stack((x2, y2))))\n",
        "    y = np.array([0]*n_points + [1]*n_points)\n",
        "    return X, y\n",
        "\n",
        "# --- Generate dataset ---\n",
        "np.random.seed(42)\n",
        "X, y = generate_spiral(300, noise=0.25)\n",
        "\n",
        "# --- Create mesh grid for plotting ---\n",
        "h = 0.1\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# --- Logistic Regression with Polynomial Features ---\n",
        "logreg = make_pipeline(PolynomialFeatures(degree=5), LogisticRegression(max_iter=15000))\n",
        "logreg.fit(X, y)\n",
        "Z_logreg = logreg.predict(grid).reshape(xx.shape)\n",
        "acc_logreg = accuracy_score(y, logreg.predict(X))\n",
        "\n",
        "# --- k-Nearest Neighbors ---\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X, y)\n",
        "Z_knn = knn.predict(grid).reshape(xx.shape)\n",
        "acc_knn = accuracy_score(y, knn.predict(X))\n",
        "\n",
        "# --- Multi-Layer Perceptron ---\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', max_iter=8000, random_state=42)\n",
        "mlp.fit(X, y)\n",
        "Z_mlp = mlp.predict(grid).reshape(xx.shape)\n",
        "acc_mlp = accuracy_score(y, mlp.predict(X))\n",
        "\n",
        "# --- SVM with RBF kernel ---\n",
        "svm = SVC(kernel='rbf', gamma='scale', C=100000.0)\n",
        "svm.fit(X, y)\n",
        "Z_svm = svm.predict(grid).reshape(xx.shape)\n",
        "acc_svm = accuracy_score(y, svm.predict(X))\n",
        "\n",
        "# --- Plot all models ---\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Logistic Regression\n",
        "axs[0, 0].contourf(xx, yy, Z_logreg, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[0, 0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[0, 0].set_title(f\"Logistic Regression\\nAcc: {acc_logreg:.2f}\")\n",
        "axs[0, 0].set_xlabel(\"x\")\n",
        "axs[0, 0].set_ylabel(\"y\")\n",
        "axs[0, 0].grid(True)\n",
        "axs[0, 0].set_aspect('equal')\n",
        "\n",
        "# k-NN\n",
        "axs[0, 1].contourf(xx, yy, Z_knn, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[0, 1].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[0, 1].set_title(f\"k-NN (k=5)\\nAcc: {acc_knn:.2f}\")\n",
        "axs[0, 1].set_xlabel(\"x\")\n",
        "axs[0, 1].set_ylabel(\"y\")\n",
        "axs[0, 1].grid(True)\n",
        "axs[0, 1].set_aspect('equal')\n",
        "\n",
        "# SVM\n",
        "axs[1, 0].contourf(xx, yy, Z_svm, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[1, 0].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[1, 0].set_title(f\"SVM (RBF kernel)\\nAcc: {acc_svm:.2f}\")\n",
        "axs[1, 0].set_xlabel(\"x\")\n",
        "axs[1, 0].set_ylabel(\"y\")\n",
        "axs[1, 0].grid(True)\n",
        "axs[1, 0].set_aspect('equal')\n",
        "\n",
        "# MLP\n",
        "axs[1, 1].contourf(xx, yy, Z_mlp, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "axs[1, 1].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k', s=30)\n",
        "axs[1, 1].set_title(f\"MLP (100,100) relu\\nAcc: {acc_mlp:.2f}\")\n",
        "axs[1, 1].set_xlabel(\"x\")\n",
        "axs[1, 1].set_ylabel(\"y\")\n",
        "axs[1, 1].grid(True)\n",
        "axs[1, 1].set_aspect('equal')\n",
        "\n",
        "plt.suptitle(\"Classification Comparison on Two-Spiral Dataset\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kX-oXAB1OZ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first glance, we can see that logistic regression has the lowest accuracy score, which is reflected in a boundary that poorly fits the training data. This is due to the simplicity of this technique. It struggles when dealing with more complicated distributions. On the other hand, k-NN, SVM and MLP both achieve 100% accuracy. However, the two differ in the way that new points get classified. Now, if we wanted to sample a new point and check which class it belongs to, k-NN would need to check that point against every point in the data set in order to find the k nearest neighbors. On the other hand, SVM relies on a model derived from the data allowing for low computational overhead when classifying new data. Similarly,  MLP uses its network of weights to define the decision boundary. This approach makes it a lot easier and faster to classify data after the network is trained."
      ],
      "metadata": {
        "id": "w1wFo83wDeF5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCxYwB1OPRbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}