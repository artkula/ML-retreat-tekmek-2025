{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artkula/ML-retreat-tekmek-2025/blob/main/NN_RNN_SINDy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3f58f0",
      "metadata": {
        "id": "aa3f58f0"
      },
      "source": [
        "# Neural Networks, Recurrent Neural Networks, and SINDy for Mechanical Systems\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this notebook, you will be able to:\n",
        "\n",
        "- **Apply neural networks** to learn constitutive material behavior from experimental data\n",
        "- **Understand when history matters** and select appropriate architectures (feedforward vs recurrent)\n",
        "- **Implement sparse discovery** to extract interpretable governing equations from data\n",
        "- **Validate models systematically** using interpolation, extrapolation, and physical constraints\n",
        "- **Make informed engineering decisions** about method selection based on problem characteristics\n",
        "\n",
        "## Notebook Organization\n",
        "\n",
        "This notebook progressively builds your understanding through four carefully designed levels:\n",
        "\n",
        "1. **Level 1: Hyperelastic material** (no history, clean data) → Master feedforward networks\n",
        "2. **Level 2: Noisy hyperelastic data** → Learn regularization and overfitting control\n",
        "3. **Level 3: Viscoelastic material** (history-dependent) → Discover why RNNs are necessary\n",
        "4. **Level 4: Sparse discovery with SINDy** → Extract interpretable equations from data\n",
        "\n",
        "Each section includes:\n",
        "- **Conceptual foundation** before any code\n",
        "- **Prediction exercises** to test your intuition\n",
        "- **Detailed code explanations** connecting choices to physics\n",
        "- **Critical analysis** of results and limitations\n",
        "\n",
        "---\n",
        "\n",
        "## Why These Methods Matter in Mechanics\n",
        "\n",
        "**Real-world challenge**: Material characterization requires expensive experimental campaigns. A single stress-strain curve might involve specialized equipment, careful sample preparation, and hours of testing. For complex materials (hyperelastic rubbers, viscoelastic polymers, composites), traditional approaches require testing at multiple rates, temperatures, and loading paths: potentially hundreds of experiments costing months of work.\n",
        "\n",
        "**Traditional limitations**:\n",
        "- Constitutive models are hand-crafted with parameters fitted to specific datasets\n",
        "- Extending to new loading conditions requires new experiments and model adjustments\n",
        "- History-dependent materials (viscoelasticity, plasticity) require differential equations that are difficult to calibrate\n",
        "- Interpolation between test conditions is often unreliable\n",
        "\n",
        "**Key insight**: Machine learning can learn material behavior from limited data, but only when combined with proper domain knowledge. Neural networks provide flexible function approximation, recurrent architectures handle history dependence, and sparse identification (SINDy) discovers interpretable governing equations.\n",
        "\n",
        "**Engineering context**: This matters for:\n",
        "- **Cost reduction**: Cheaper experimental campaign with ML-guided testing\n",
        "- **Digital twins**: Real-time simulation for design optimization\n",
        "- **Predictive maintenance**: Material degradation monitoring\n",
        "- **New material discovery**: Rapid screening of material candidates\n",
        "\n",
        "---\n",
        "\n",
        "## Physical Background: Stress and Strain Measures\n",
        "\n",
        "### Fundamental Relationships\n",
        "\n",
        "We work with two stress measures that are fundamental in finite deformation mechanics:\n",
        "\n",
        "**Second Piola-Kirchhoff stress** $S$ $[\\text{kPa}]$:\n",
        "- Work-conjugate to Green-Lagrange strain\n",
        "- Defined in reference (undeformed) configuration\n",
        "- Symmetric tensor, computationally convenient\n",
        "\n",
        "**Cauchy stress** $\\sigma$ $[\\text{kPa}]$:\n",
        "- True stress acting on deformed configuration\n",
        "- What you actually measure in experiments\n",
        "- Physical interpretation: force per unit deformed area\n",
        "\n",
        "For incompressible materials under uniaxial loading with stretch $\\lambda$ $[-]$:\n",
        "\n",
        "$$\\sigma = \\lambda^2 S$$\n",
        "\n",
        "This relationship is exact for incompressible materials and will be our validation check throughout.\n",
        "\n",
        "**Stretch ratio** $\\lambda = L/L_0$:\n",
        "- $\\lambda = 1$: No deformation (reference state)\n",
        "- $\\lambda > 1$: Extension (stretching)\n",
        "- $\\lambda < 1$: Compression (shortening)\n",
        "\n",
        "---\n",
        "\n",
        "### Why use stretch $\\lambda$ instead of strain $\\varepsilon$?\n",
        "\n",
        "**Physical reasoning:**\n",
        "- **Strain** ($\\varepsilon = \\Delta L/L_0 = \\lambda - 1$) is additive but fails at large deformations\n",
        "- **Stretch** ($\\lambda = L/L_0$) is multiplicative and physically meaningful for finite deformations\n",
        "\n",
        "**Mathematical advantages:**\n",
        "- Strain energy $W(\\lambda)$ remains well-defined for all deformations\n",
        "- Incompressibility constraint: $\\det(\\mathbf{F}) = \\lambda_1 \\lambda_2 \\lambda_3 = 1$ (simple in stretch)\n",
        "- Stress-stretch relations avoid singularities at large deformations\n",
        "\n",
        "**Example:** For 100% extension:\n",
        "- Strain: $\\varepsilon = 1.0$ (additive)\n",
        "- Stretch: $\\lambda = 2.0$ (multiplicative)\n",
        "  \n",
        "If you stretch twice: $\\varepsilon_1 + \\varepsilon_2 \\neq \\varepsilon_{\\text{total}}$, but $\\lambda_1 \\times \\lambda_2 = \\lambda_{\\text{total}}$ ✓\n",
        "\n",
        "For hyperelastic materials undergoing large deformations ($\\lambda > 1.5$), stretch provides the correct mathematical framework. Small strain theory ($\\varepsilon \\approx \\lambda - 1$) is only valid for $\\lambda < 1.1$ (< 10% deformation).\n",
        "\n",
        "---\n",
        "\n",
        "### Why $\\sigma = \\lambda^2 S$?\n",
        "\n",
        "**Physical interpretation:**\n",
        "\n",
        "The factor $\\lambda^2$ arises from the transformation between reference and current configurations:\n",
        "\n",
        "1. **$S$ (Second Piola-Kirchhoff)**: Stress measured in the **undeformed** (reference) state\n",
        "   - Force per unit original area: $F_0/A_0$\n",
        "\n",
        "2. **$\\sigma$ (Cauchy)**: Stress measured in the **deformed** (current) state  \n",
        "   - Force per unit current area: $F/A$\n",
        "\n",
        "3. **For incompressible uniaxial tension:**\n",
        "   - Cross-sectional area shrinks: $A = A_0/\\lambda$ (volume preservation)\n",
        "   - The push-forward tensor operation: $\\boldsymbol{\\sigma} = \\mathbf{F} \\cdot \\mathbf{S} \\cdot \\mathbf{F}^T / \\det(\\mathbf{F})$\n",
        "   - For incompressible ($\\det(\\mathbf{F}) = 1$) and uniaxial $\\mathbf{F} = \\text{diag}(\\lambda, \\lambda^{-1/2}, \\lambda^{-1/2})$:\n",
        "   \n",
        "   $$\\sigma = \\lambda \\cdot S \\cdot \\lambda = \\lambda^2 S$$\n",
        "\n",
        "**In words:** Cauchy stress accounts for both the geometric area change AND the stress measure transformation from reference to current configuration.\n",
        "\n",
        "---\n",
        "\n",
        "## Critical Warning About Extrapolation\n",
        "\n",
        "⚠️ **Neural Networks Cannot Extrapolate Reliably** ⚠️\n",
        "\n",
        "The feedforward neural network is trained on data where $\\lambda \\in [1.0, \\lambda_{max}]$.\n",
        "\n",
        "**Extrapolation to $\\lambda < 1.0$ (compression) produces unphysical results** because:\n",
        "1. The network has never seen compression data\n",
        "2. Neural networks learn correlations, not physical laws\n",
        "3. Outside the training domain, predictions are essentially random\n",
        "\n",
        "For reliable predictions outside the training range, consider:\n",
        "- **Including boundary conditions**: Add anchor point at $\\lambda = 1$ with $\\sigma = 0$\n",
        "- **Physics-informed loss**: Penalty term enforcing $\\sigma(1) \\approx 0$\n",
        "- **PINNs**: Physics-informed neural networks with PDE constraints\n",
        "- **Hybrid models**: Combine NN with known physical structure\n",
        "\n",
        "Remember: A 99% accurate model inside training domain may be 0% accurate outside!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bd3e4a",
      "metadata": {
        "id": "e0bd3e4a"
      },
      "source": [
        "## Setup: Environment Configuration and Reproducibility\n",
        "\n",
        "This section configures the computational environment with careful attention to:\n",
        "- **Reproducibility**: Fixed random seeds across numpy, Python, and Keras\n",
        "- **Visualization**: Consistent, publication-quality plotting defaults\n",
        "- **Performance**: FAST mode for CPU execution during development\n",
        "- **Compatibility**: Graceful handling of Keras 3 vs TensorFlow Keras\n",
        "\n",
        "### What to Expect\n",
        "When you run this cell, you should see:\n",
        "- Confirmation of FAST mode status (ON for quick testing, OFF for full results)\n",
        "- Random seed value (default 1827, can be overridden via PY_SEED environment variable)\n",
        "- Keras version being used (prefer standalone Keras 3 for latest features)\n",
        "- Successful loading of helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02b18ac",
      "metadata": {
        "id": "f02b18ac"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GLOBAL SETUP: IMPORTS, SEEDS, AND PLOTTING DEFAULTS\n",
        "# ============================================================================\n",
        "# This cell must run first and sets up the entire computational environment.\n",
        "# Every choice here affects reproducibility and visual quality of results.\n",
        "\n",
        "import os, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress TensorFlow/Keras warnings for cleaner output\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION MODES AND RANDOM SEEDS\n",
        "# ============================================================================\n",
        "# FAST mode: Reduces dataset sizes and training epochs for CPU execution\n",
        "#   - ON (default): Quick testing, ~2 minutes total runtime\n",
        "#   - OFF: Full training, ~10 minutes on GPU, ~30 minutes on CPU\n",
        "FAST = bool(int(os.environ.get(\"FAST\", \"1\")))\n",
        "\n",
        "# SEED: Master random seed for reproducibility\n",
        "#   - Controls: numpy, random, and keras random number generation\n",
        "#   - Default: 1827 (arbitrary but fixed for course consistency)\n",
        "#   - Override: Set PY_SEED environment variable\n",
        "SEED = int(os.environ.get(\"PY_SEED\", \"1827\"))\n",
        "\n",
        "# Apply seeds immediately for reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# DEEP LEARNING FRAMEWORK SELECTION\n",
        "# ============================================================================\n",
        "# Prefer standalone Keras 3 (better features) but fall back to tf.keras if needed\n",
        "try:\n",
        "    import keras  # Standalone Keras 3 (recommended)\n",
        "    print(\"✅ Using standalone Keras 3 (recommended)\")\n",
        "except Exception:\n",
        "    from tensorflow import keras  # TensorFlow's built-in Keras\n",
        "    print(\"⚠️ Using TensorFlow's built-in Keras (older version)\")\n",
        "\n",
        "from keras import layers, callbacks, regularizers\n",
        "\n",
        "# ============================================================================\n",
        "# SCIENTIFIC COMPUTING AND VISUALIZATION\n",
        "# ============================================================================\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp  # For ODE integration (Lorenz system)\n",
        "from tqdm.auto import tqdm  # Progress bars for long operations\n",
        "\n",
        "# ============================================================================\n",
        "# PYSINDY INSTALLATION AND IMPORT\n",
        "# ============================================================================\n",
        "# PySINDy (Sparse Identification of Nonlinear Dynamics) for discovering equations\n",
        "try:\n",
        "    import pysindy as ps\n",
        "except Exception:\n",
        "    print(\"📦 Installing PySINDy (required for sparse discovery)...\")\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pysindy\", \"--quiet\"])\n",
        "    import pysindy as ps\n",
        "    print(\"✅ PySINDy installed successfully\")\n",
        "\n",
        "# Set Keras seed (must be after keras import)\n",
        "keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# MATPLOTLIB CONFIGURATION FOR PUBLICATION-QUALITY PLOTS\n",
        "# ============================================================================\n",
        "# These settings ensure consistent, professional-looking figures throughout\n",
        "mpl.rcParams.update({\n",
        "    # Figure size and resolution\n",
        "    \"figure.figsize\": (7, 4.5),        # Default size in inches (16:10 aspect ratio)\n",
        "    \"figure.dpi\": 120,                 # Screen resolution (points per inch)\n",
        "    \"savefig.dpi\": 200,                # Saved figure resolution (higher for papers)\n",
        "\n",
        "    # Background colors\n",
        "    \"figure.facecolor\": \"white\",       # White figure background\n",
        "    \"savefig.facecolor\": \"white\",      # White background when saving\n",
        "\n",
        "    # Grid appearance\n",
        "    \"axes.grid\": True,                 # Show grid by default\n",
        "    \"grid.alpha\": 0.2,                 # Subtle grid (20% opacity)\n",
        "\n",
        "    # Font sizes (scaled for readability)\n",
        "    \"font.size\": 12,                   # Base font size\n",
        "    \"axes.titlesize\": 13,              # Slightly larger titles\n",
        "    \"axes.labelsize\": 12,              # Axis label size\n",
        "    \"legend.fontsize\": 11,             # Legend text\n",
        "    \"xtick.labelsize\": 11,             # X-axis tick labels\n",
        "    \"ytick.labelsize\": 11,             # Y-axis tick labels\n",
        "\n",
        "    # Layout\n",
        "    \"figure.constrained_layout.use\": False  # Use tight_layout instead (more reliable)\n",
        "})\n",
        "\n",
        "# Color palette: ColorBrewer Set1 (colorblind-friendly, distinct colors)\n",
        "# Blue, Orange, Green, Purple, Brown, Pink - optimized for clarity\n",
        "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(\n",
        "    color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#9467bd\", \"#8c564b\", \"#e377c2\"]\n",
        ")\n",
        "\n",
        "print(f\"🚀 Environment configured:\")\n",
        "print(f\"   FAST mode: {'⚡ ON (reduced data/epochs)' if FAST else '🐢 OFF (full training)'}\")\n",
        "print(f\"   Random seed: {SEED}\")\n",
        "print(f\"   Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8827131e",
      "metadata": {
        "id": "8827131e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR CONSISTENT PLOTTING AND ANALYSIS\n",
        "# ============================================================================\n",
        "# These utilities ensure uniform appearance and correct physical units throughout\n",
        "\n",
        "def set_labels(ax, x_label=None, y_label=None, title=None):\n",
        "    \"\"\"Apply labels to axes with consistent formatting.\"\"\"\n",
        "    if x_label: ax.set_xlabel(x_label)\n",
        "    if y_label: ax.set_ylabel(y_label)\n",
        "    if title:   ax.set_title(title)\n",
        "\n",
        "def label_units(var: str, unit: str) -> str:\n",
        "    \"\"\"Format variable with units using LaTeX: 'σ [kPa]'.\"\"\"\n",
        "    return rf\"{var} [{unit}]\"\n",
        "\n",
        "def dimless(var: str) -> str:\n",
        "    \"\"\"Format dimensionless variable: 'λ [–]'.\"\"\"\n",
        "    return rf\"{var} [–]\"\n",
        "\n",
        "def legend_outside_right(ax, loc=\"center left\", anchor=(1.02, 0.5)):\n",
        "    \"\"\"Place legend outside plot area to avoid obscuring data.\"\"\"\n",
        "    ax.legend(loc=loc, bbox_to_anchor=anchor, borderaxespad=0.0)\n",
        "\n",
        "def set_equal_aspect_2d(ax):\n",
        "    \"\"\"Force square aspect ratio (important for phase plots).\"\"\"\n",
        "    try:\n",
        "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "    except Exception:\n",
        "        pass  # Some plot types don't support equal aspect\n",
        "\n",
        "def phase_axes(ax, xlim=None, ylim=None):\n",
        "    \"\"\"Configure axes for phase space plots with equal scaling.\"\"\"\n",
        "    if xlim: ax.set_xlim(*xlim)\n",
        "    if ylim: ax.set_ylim(*ylim)\n",
        "    set_equal_aspect_2d(ax)\n",
        "\n",
        "def plot_residuals(t, resid, ax=None, title=\"Residuals\", units=None,\n",
        "                   symmetric=True, annotate_stats=True):\n",
        "    \"\"\"\n",
        "    Plot model residuals with statistical annotations.\n",
        "\n",
        "    Parameters:\n",
        "        t: Time array\n",
        "        resid: Residual array (prediction - truth)\n",
        "        ax: Matplotlib axes (creates new if None)\n",
        "        title: Plot title\n",
        "        units: Physical units for y-axis label\n",
        "        symmetric: If True, center y-axis on zero with symmetric limits\n",
        "        annotate_stats: If True, show RMSE in text box\n",
        "    \"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    ax.plot(t, resid, lw=1.5)\n",
        "    ax.axhline(0.0, color=\"k\", lw=1.0, alpha=0.6)  # Zero reference line\n",
        "\n",
        "    if symmetric:\n",
        "        # Symmetric limits help visualize bias vs variance\n",
        "        L = np.nanmax(np.abs(resid))\n",
        "        if np.isfinite(L) and L > 0:\n",
        "            ax.set_ylim(-1.05 * L, 1.05 * L)\n",
        "\n",
        "    yl = \"Error\" if units is None else f\"Error [{units}]\"\n",
        "    set_labels(ax, x_label=label_units(r\"$t$\", \"s\"), y_label=yl, title=title)\n",
        "\n",
        "    if annotate_stats:\n",
        "        rmse = float(np.sqrt(np.nanmean(resid**2)))\n",
        "        ax.text(0.01, 0.95, f\"RMSE: {rmse:.3g}\", transform=ax.transAxes,\n",
        "                va=\"top\", ha=\"left\",\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
        "    return ax\n",
        "\n",
        "# ============================================================================\n",
        "# 3D PLOTTING FOR DYNAMICAL SYSTEMS\n",
        "# ============================================================================\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_trajectory_3d(x, y, z, elev=25, azim=35, lw=1.5, title=\"3D Trajectory\"):\n",
        "    \"\"\"\n",
        "    Create 3D trajectory plot with fixed aspect ratio and viewing angle.\n",
        "\n",
        "    Parameters:\n",
        "        x, y, z: Coordinate arrays\n",
        "        elev: Elevation viewing angle in degrees\n",
        "        azim: Azimuthal viewing angle in degrees\n",
        "        lw: Line width\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        fig, ax: Matplotlib figure and 3D axes\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.plot(x, y, z, lw=lw)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Equal aspect ratio in 3D (tricky but important for accurate visualization)\n",
        "    try:\n",
        "        ranges = [np.max(v) - np.min(v) for v in [x, y, z]]\n",
        "        max_range = max(ranges) if max(ranges) > 0 else 1.0\n",
        "\n",
        "        # Find center point\n",
        "        cx = 0.5 * (np.max(x) + np.min(x))\n",
        "        cy = 0.5 * (np.max(y) + np.min(y))\n",
        "        cz = 0.5 * (np.max(z) + np.min(z))\n",
        "\n",
        "        # Set equal ranges centered on data\n",
        "        ax.set_xlim(cx - 0.5*max_range, cx + 0.5*max_range)\n",
        "        ax.set_ylim(cy - 0.5*max_range, cy + 0.5*max_range)\n",
        "        ax.set_zlim(cz - 0.5*max_range, cz + 0.5*max_range)\n",
        "        ax.set_box_aspect((1, 1, 1))\n",
        "    except Exception:\n",
        "        pass  # Fallback for older matplotlib versions\n",
        "\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    return fig, ax\n",
        "\n",
        "print(\"✅ Helper functions loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a4f871",
      "metadata": {
        "id": "52a4f871"
      },
      "source": [
        "## Level 1: Hyperelastic Material (Clean Data, No History)\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "**Engineering scenario**: You have expensive experimental data from material testing and need to predict behavior at intermediate conditions. A hyperelastic material (like rubber) has no memory - its stress depends only on current stretch, not loading history.\n",
        "\n",
        "**Learning goals**:\n",
        "1. Understand when feedforward networks are appropriate (no history dependence)\n",
        "2. Learn proper data normalization for neural networks\n",
        "3. Recognize extrapolation failures and their implications\n",
        "4. Master the train/validation/test split paradigm\n",
        "\n",
        "### Conceptual Foundation\n",
        "\n",
        "**Hyperelastic constitutive model** (Mooney-Rivlin):\n",
        "\n",
        "For incompressible materials, the strain energy density is:\n",
        "$$W = C_1(I_1 - 3) + C_2(I_2 - 3)$$\n",
        "\n",
        "where $I_1 = \\lambda_1^2 + \\lambda_2^2+\\lambda_3^2, I_2 = \\lambda_1^2\\lambda_2^2+\\lambda_1^2\\lambda_3^2+\\lambda_2^2\\lambda_3^2$ are strain invariants and $C_1, C_2$ are material parameters. The second Piola-Kirchoff stress is given by $S_k = \\frac{\\partial W}{\\partial \\lambda_k} + p$, where $p$ is the pressure determined from the boundary conditions.\n",
        "\n",
        "In 1D case, we have $\\lambda_1 = \\lambda, \\lambda_2=\\lambda_3 = 1/\\sqrt{\\lambda}$ and $S_2=S_3=0$. This follows from the incompressibility constraint $\\det(F) = \\lambda_1 \\lambda_2 \\lambda_3 = 1$ (volume preservation) combined with the symmetry assumption $\\lambda_2 = \\lambda_3$ (equal transverse contraction). Substituting $\\lambda_1 = \\lambda$ gives $\\lambda \\times \\lambda_2^2 = 1$, which yields $\\lambda_2 = \\lambda_3 = 1/\\sqrt{\\lambda}$. The the pressure $p$ is determined by the boundary condition $S_2=S_3=0$ (no stress on sides):\n",
        "\n",
        "$$S_2 = \\frac{\\partial W}{\\partial \\lambda_2} + p = 0$$\n",
        "\n",
        "This gives:\n",
        "\n",
        "$$p = -\\frac{\\partial W}{\\partial \\lambda_2}$$\n",
        "\n",
        "This leads to the stress-stretch relationship we will learn:\n",
        "$$S_1 = \\frac{1}{\\lambda^2}\\left[2C_1(\\lambda^2 - \\frac{1}{\\lambda}) + 2C_2(\\lambda - \\frac{1}{\\lambda^2})\\right]$$\n",
        "\n",
        "**Key properties**:\n",
        "- Path-independent: Same stretch → same stress (no hysteresis)\n",
        "- Nonlinear: Stiffens significantly at large stretches\n",
        "- Physical constraint: $S(\\lambda=1) = 0$ (zero stress at zero strain)\n",
        "\n",
        "### Before We Code: Prediction Exercise\n",
        "\n",
        "Consider the stress-stretch curve we are about to generate:\n",
        "- What shape do you expect? (Linear? Exponential? Polynomial?)\n",
        "- Will our training data capture the full nonlinearity?\n",
        "- What physical constraint must the model satisfy at $\\lambda = 1$?\n",
        "- Why might extrapolation to compression ($\\lambda < 1$) fail?\n",
        "\n",
        "**Answer hints**:\n",
        "- The curve is nonlinear, stiffening at large stretches\n",
        "- Training only covers extension ($\\lambda > 1$), missing compression behavior\n",
        "- At $\\lambda = 1$, stress must be exactly zero (reference state)\n",
        "- Without compression data, the network cannot learn the symmetric behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4ed098",
      "metadata": {
        "id": "cd4ed098"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA GENERATION: MOONEY-RIVLIN HYPERELASTIC MODEL\n",
        "# ============================================================================\n",
        "# We generate synthetic but physically realistic data based on a well-established\n",
        "# constitutive model used for rubber-like materials.\n",
        "\n",
        "# Material parameters (typical for soft rubber, experimentally determined)\n",
        "C1 = 31.68  # First Mooney-Rivlin parameter [kPa]\n",
        "C2 = 34.70  # Second Mooney-Rivlin parameter [kPa]\n",
        "# These values give realistic stress magnitudes for biological tissues or soft elastomers\n",
        "\n",
        "def compute_S_stress(stretch):\n",
        "    '''\n",
        "    Compute Second Piola-Kirchhoff stress from Mooney-Rivlin model.\n",
        "\n",
        "    This is the GROUND TRUTH that our neural network will try to learn.\n",
        "    Notice the highly nonlinear relationship - this is what makes the problem\n",
        "    interesting and challenging for simple interpolation methods.\n",
        "\n",
        "    Parameters:\n",
        "        stretch: Stretch ratio λ (1.0 = undeformed, >1 = extension)\n",
        "\n",
        "    Returns:\n",
        "        S: Second Piola-Kirchhoff stress [kPa]\n",
        "    '''\n",
        "    lam = stretch  # More readable notation\n",
        "\n",
        "    # Mooney-Rivlin model for incompressible material\n",
        "    # Note the 1/λ terms - these blow up as λ→0, making compression very stiff\n",
        "    S = (2*C1*(lam**2 - 1/lam) + 2*C2*(lam - 1/lam**2)) / lam**2\n",
        "\n",
        "    return S\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING DATA GENERATION\n",
        "# ============================================================================\n",
        "# KEY DECISION: How many data points do we need?\n",
        "#   - Too few: Cannot capture nonlinearity (underfitting)\n",
        "#   - Too many: Expensive experiments in real applications\n",
        "#   - Sweet spot: 10-15 points for this level of nonlinearity\n",
        "\n",
        "N_points = 10 if FAST else 15  # Reduce for CPU testing, full for GPU\n",
        "\n",
        "# Stretch range: 1.0 (undeformed) to 5.8 (480% extension)\n",
        "# This is realistic for rubber - beyond this, material might fail\n",
        "stretch_train = np.linspace(1.0, 5.8, N_points)\n",
        "\n",
        "# Compute corresponding stresses (our \"experimental measurements\")\n",
        "stress_train = compute_S_stress(stretch_train)\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION: UNDERSTANDING OUR TRAINING DATA\n",
        "# ============================================================================\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Scatter plot emphasizes discrete nature of experimental data\n",
        "plt.scatter(stretch_train, stress_train,\n",
        "           s=100,                    # Large markers for visibility\n",
        "           alpha=0.7,                # Slight transparency\n",
        "           edgecolors='black',       # Black edges for clarity\n",
        "           linewidth=2,              # Thick edges\n",
        "           label=f'Training data (n={N_points})',\n",
        "           zorder=5)                 # Draw on top\n",
        "\n",
        "# Add smooth curve to show true function (what we're trying to learn)\n",
        "stretch_smooth = np.linspace(1.0, 5.8, 200)\n",
        "stress_smooth = compute_S_stress(stretch_smooth)\n",
        "plt.plot(stretch_smooth, stress_smooth,\n",
        "        'b-', alpha=0.3, linewidth=1,\n",
        "        label='True function', zorder=1)\n",
        "\n",
        "plt.xlabel('Stretch λ [-]')\n",
        "plt.ylabel('Second Piola-Kirchhoff Stress S [kPa]')\n",
        "plt.title('Hyperelastic Training Data: What the Neural Network Sees')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotations for key insights\n",
        "plt.annotate('Zero stress at λ=1\\n(undeformed state)',\n",
        "            xy=(1, 0), xytext=(1.5, 20),\n",
        "            arrowprops=dict(arrowstyle='->', alpha=0.5))\n",
        "plt.annotate('Nonlinear stiffening\\nat large stretches',\n",
        "            xy=(5, stress_train[-2]), xytext=(3.5, 150),\n",
        "            arrowprops=dict(arrowstyle='->', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY STATISTICS\n",
        "# ============================================================================\n",
        "print(f\"📊 Training Data Summary:\")\n",
        "print(f\"   Number of points: {N_points}\")\n",
        "print(f\"   Stretch range: λ ∈ [{stretch_train.min():.2f}, {stretch_train.max():.2f}]\")\n",
        "print(f\"   Stress range: S ∈ [{stress_train.min():.1f}, {stress_train.max():.1f}] kPa\")\n",
        "print(f\"   Nonlinearity measure: {stress_train[-1]/stress_train[1]:.1f}x stress increase\")\n",
        "print(f\"\\n⚠️ Critical limitation: No compression data (λ < 1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c23a2585",
      "metadata": {
        "id": "c23a2585"
      },
      "source": [
        "### Neural network architecture and training\n",
        "\n",
        "Now we build and train a feedforward neural network to learn the stress-stretch relationship.\n",
        "\n",
        "#### Architecture design decisions\n",
        "\n",
        "**Network depth and width:**\n",
        "\n",
        "- **Shallow (2-3 layers):** Fast training and inference, but may underfit nonlinear relationships due to limited representational capacity\n",
        "- **Deep (6-8 layers):** Can capture complex nonlinear patterns and hierarchical features, but carries risk of overfitting with limited data\n",
        "- **Wide (64-128 neurons):** Provides greater capacity per layer, better suited for approximating smooth functions\n",
        "- **Narrow (16-32 neurons):** Forces feature compression, acts as implicit regularization to prevent overfitting\n",
        "\n",
        "**Activation functions:**\n",
        "\n",
        "- **Tanh:** Smooth and bounded output in [-1,1], well-suited for normalized data, potential gradient saturation in deep networks\n",
        "- **ReLU:** Computationally efficient, unbounded positive output, susceptible to dead neuron problem, creates piecewise linear approximations\n",
        "- **Swish/GELU:** Modern alternatives with smooth non-saturating behavior, slightly higher computational cost\n",
        "\n",
        "**Our choice:** 6 layers with 32 neurons per layer using tanh activation (provides smooth, physics-consistent approximations)\n",
        "\n",
        "---\n",
        "\n",
        "#### Loss functions for training\n",
        "\n",
        "When training a neural network, we require a **loss function** (objective function) that quantifies the discrepancy between predictions and true values. The optimizer minimizes this loss during training.\n",
        "\n",
        "**Mean squared error (MSE):**\n",
        "\n",
        "$$L = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "- Penalizes large errors quadratically through the squaring operation\n",
        "- Smooth and differentiable everywhere, facilitating stable gradient-based optimization\n",
        "- Standard choice for regression problems with continuous outputs\n",
        "- Well-suited for stress-strain prediction where smooth gradients are desired\n",
        "\n",
        "**Alternative loss functions:**\n",
        "\n",
        "- **Mean absolute error (MAE):** $L = \\frac{1}{n}\\sum_{i=1}^{n}|\\hat{y}_i - y_i|$ — more robust to outliers, but exhibits gradient discontinuity at zero\n",
        "- **Huber loss:** Combines MSE for small errors and MAE for large errors — balances smoothness with robustness to outliers\n",
        "\n",
        "For our stress-strain prediction problem, MSE is the appropriate choice because: (1) the data is relatively clean without significant outliers, (2) smooth gradients are essential for stable training convergence, and (3) quadratic penalization naturally emphasizes reduction of larger errors, which is desirable for capturing the full range of stress-strain behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8c8be7",
      "metadata": {
        "id": "0f8c8be7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NEURAL NETWORK: TRAINING ON HYPERELASTIC DATA (NO HISTORY DEPENDENCE)\n",
        "# ============================================================================\n",
        "# This section demonstrates how to train a feedforward neural network (MLP)\n",
        "# to approximate the stress–stretch relationship for a hyperelastic material.\n",
        "# Since hyperelasticity is history-independent, a simple regression suffices.\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# MODEL ARCHITECTURE\n",
        "# ----------------------------------------------------------------------------\n",
        "def build_mlp(hidden_layers=6, width=32, activation='tanh', l2_reg=None):\n",
        "    '''\n",
        "    Build a feedforward network (MLP) for regression.\n",
        "\n",
        "    Key design notes:\n",
        "    - hidden_layers: Controls model depth (number of hidden transformations)\n",
        "    - width: Controls number of neurons per layer\n",
        "    - activation: Controls nonlinearity (tanh gives smooth, symmetric response)\n",
        "    - l2_reg: Optional regularization to penalize large weights (smoothens fit)\n",
        "\n",
        "    For FAST mode, both depth and width are reduced for quick CPU testing.\n",
        "    '''\n",
        "    # Adjust model size based on FAST mode\n",
        "    n_layers = max(2, hidden_layers // 2) if FAST else hidden_layers\n",
        "    neurons = max(16, width // 2) if FAST else width\n",
        "\n",
        "    model = keras.Sequential(name='hyperelastic_mlp')\n",
        "    model.add(layers.Input(shape=(1,)))  # Input = stretch λ\n",
        "\n",
        "    # Hidden layers (nonlinear transformations)\n",
        "    for i in range(n_layers):\n",
        "        model.add(layers.Dense(\n",
        "            neurons,\n",
        "            activation=activation,\n",
        "            kernel_regularizer=regularizers.l2(l2_reg) if l2_reg else None,\n",
        "            name=f'hidden_{i+1}'\n",
        "        ))\n",
        "\n",
        "    # Output: stress S (linear activation)\n",
        "    model.add(layers.Dense(1, name='output'))\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL INSTANTIATION AND DATA NORMALIZATION\n",
        "# ============================================================================\n",
        "# Normalization is crucial for stable training. Neural networks assume roughly\n",
        "# standardized input/output scales (mean ~ 0, std ~ 1). Raw mechanical data\n",
        "# may vary by orders of magnitude, so we normalize first.\n",
        "# ----------------------------------------------------------------------------\n",
        "model_clean = build_mlp()\n",
        "\n",
        "mean_stretch = stretch_train.mean()\n",
        "std_stretch = stretch_train.std()\n",
        "mean_stress = stress_train.mean()\n",
        "std_stress = stress_train.std()\n",
        "\n",
        "X_norm = (stretch_train - mean_stretch) / std_stretch\n",
        "y_norm = (stress_train - mean_stress) / std_stress\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL COMPILATION\n",
        "# ============================================================================\n",
        "# Optimizer: Adam (adaptive learning rate, robust to noisy gradients)\n",
        "# Loss: Mean Squared Error (MSE) — suitable for regression\n",
        "# ----------------------------------------------------------------------------\n",
        "model_clean.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING SETUP: EARLY STOPPING & CHECKPOINTING\n",
        "# ============================================================================\n",
        "# EarlyStopping prevents overfitting: stops training once validation loss stops\n",
        "# improving. ModelCheckpoint saves the best-performing model to disk (.keras).\n",
        "# ----------------------------------------------------------------------------\n",
        "callbacks_list = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        filepath='model_best.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING THE MODEL\n",
        "# ============================================================================\n",
        "# We train for up to 700 (FAST) or 1000 epochs. Validation split = 0.2 means\n",
        "# 80% of data used for gradient updates, 20% for validation monitoring.\n",
        "# ----------------------------------------------------------------------------\n",
        "history = model_clean.fit(\n",
        "    X_norm.reshape(-1, 1),\n",
        "    y_norm.reshape(-1, 1),\n",
        "    epochs=700 if FAST else 1000,\n",
        "    verbose=0,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EVALUATION AND VISUALIZATION\n",
        "# ============================================================================\n",
        "# We inspect:\n",
        "# 1) Training curves (loss evolution)\n",
        "# 2) Fit on training data\n",
        "# 3) Extrapolation behavior beyond training range\n",
        "# ----------------------------------------------------------------------------\n",
        "y_pred = model_clean.predict(X_norm.reshape(-1, 1), verbose=0)\n",
        "y_pred = y_pred.flatten() * std_stress + mean_stress  # Denormalize predictions\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# --- (1) Training evolution plot ---\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.semilogy(history.history['loss'], label='Train')\n",
        "plt.semilogy(history.history['val_loss'], label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Evolution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# --- (2) Training data fit ---\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(stretch_train, stress_train, s=100, alpha=0.7, label='True')\n",
        "plt.scatter(stretch_train, y_pred, s=80, marker='x', color='red', label='Predicted')\n",
        "plt.xlabel('Stretch λ [-]')\n",
        "plt.ylabel('Stress S [kPa]')\n",
        "plt.title('Training Fit')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# --- (3) Extrapolation test ---\n",
        "stretch_test = np.linspace(0.7, 8.5, 200)\n",
        "X_test_norm = (stretch_test - mean_stretch) / std_stretch\n",
        "y_test_pred = model_clean.predict(X_test_norm.reshape(-1, 1), verbose=0)\n",
        "y_test_pred = y_test_pred.flatten() * std_stress + mean_stress\n",
        "y_test_true = compute_S_stress(stretch_test)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(stretch_test, y_test_true, 'b-', label='True', linewidth=2)\n",
        "plt.plot(stretch_test, y_test_pred, 'r--', label='NN', linewidth=2)\n",
        "plt.axvspan(stretch_train.min(), stretch_train.max(), alpha=0.2, color='green')\n",
        "plt.xlabel('Stretch λ [-]')\n",
        "plt.ylabel('Stress S [kPa]')\n",
        "plt.title('Extrapolation Test')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORMANCE METRIC\n",
        "# ============================================================================\n",
        "# Compute mean absolute error (MAE) in original physical units (kPa)\n",
        "# ----------------------------------------------------------------------------\n",
        "mae_train = np.mean(np.abs(y_pred - stress_train))\n",
        "print(f\"\\n✅ Training MAE: {mae_train:.3f} kPa\")\n",
        "print(f\"⚠️  Extrapolation fails outside training domain!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd75389",
      "metadata": {
        "id": "8dd75389"
      },
      "source": [
        "### Conceptual Checkpoint: Understanding vs Memorization\n",
        "\n",
        "**Reflect on what you observed:**\n",
        "\n",
        "1. **Loss behavior**: Did both losses decrease smoothly? When did validation loss stop improving?\n",
        "2. **Fit quality**: How close are predictions to true values at training points?\n",
        "3. **Interpolation**: Does the model produce smooth curves between training points?\n",
        "4. **Extrapolation**: What happens outside the training domain?\n",
        "\n",
        "**Key insights**:\n",
        "- ✅ Excellent interpolation: NN creates smooth function through data points\n",
        "- ✅ Efficient learning: Only needed 10-15 points to capture nonlinearity  \n",
        "- ❌ Extrapolation failure: Predictions outside training domain are meaningless\n",
        "- ❌ No physics constraints: Model doesn't know $S(1) = 0$ unless explicitly taught\n",
        "\n",
        "**Engineering relevance**: This demonstrates both the power and danger of neural networks. They excel at interpolation but fail catastrophically at extrapolation. Always validate your training domain coverage!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a1bc51",
      "metadata": {
        "id": "b1a1bc51"
      },
      "source": [
        "## Level 2: Noisy Hyperelastic Data (Regularization and Overfitting)\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "**Real-world scenario**: Experimental measurements always contain noise from:\n",
        "- Sensor limitations (± 0.1% full scale)\n",
        "- Temperature fluctuations during testing\n",
        "- Specimen imperfections and mounting errors\n",
        "- Vibrations and electrical interference\n",
        "\n",
        "**The challenge**: How do we learn the underlying physics without memorizing the noise?\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "1. **Recognize overfitting**: Model fits training data perfectly but generalizes poorly\n",
        "2. **Apply regularization**: L1, L2, dropout, and other techniques\n",
        "3. **Use validation properly**: Monitor generalization during training\n",
        "4. **Select hyperparameters**: Systematic approach to regularization strength\n",
        "\n",
        "### Conceptual Foundation\n",
        "\n",
        "**The bias-variance tradeoff**:\n",
        "- **High bias** (underfitting): Model too simple, cannot capture true pattern\n",
        "- **High variance** (overfitting): Model too complex, memorizes noise\n",
        "- **Sweet spot**: Balance complexity with generalization\n",
        "\n",
        "**Regularization techniques**:\n",
        "- **L2 (weight decay)**: Penalizes large weights → smoother functions\n",
        "- **L1 (sparsity)**: Pushes weights to zero → feature selection\n",
        "- **Dropout**: Randomly disables neurons → ensemble effect\n",
        "- **Early stopping**: Stop before overfitting → implicit regularization\n",
        "\n",
        "### Prediction Before Running\n",
        "\n",
        "What will happen when we train a large network on noisy data?\n",
        "- Without regularization?\n",
        "- With strong regularization?\n",
        "- How can we tell if we have the right amount?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1c3ce9",
      "metadata": {
        "id": "5a1c3ce9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ADDING REALISTIC NOISE TO DATA\n",
        "# ============================================================================\n",
        "# Real experimental data is never perfect. We simulate realistic measurement noise.\n",
        "\n",
        "# Noise level selection (15% is typical for material testing)\n",
        "noise_level = 0.15  # Standard deviation as fraction of signal range\n",
        "\n",
        "# Set seed for reproducible noise\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Add Gaussian noise to normalized stress values\n",
        "# Why normalized? Keeps noise level consistent across all values\n",
        "y_noisy_norm = y_norm + np.random.normal(0, noise_level, y_norm.shape)\n",
        "\n",
        "# Convert back to physical units for visualization\n",
        "stress_noisy = y_noisy_norm * std_stress + mean_stress\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE NOISY DATA VS TRUE FUNCTION\n",
        "# ============================================================================\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# True function (what we want to learn)\n",
        "x_true_curve = np.linspace(1, 6, 200)\n",
        "y_true_curve = compute_S_stress(x_true_curve)\n",
        "plt.plot(x_true_curve, y_true_curve, 'b-', linewidth=2.5,\n",
        "        alpha=0.7, label='True function (unknown to model)')\n",
        "\n",
        "# Noisy measurements (what the model actually sees)\n",
        "plt.scatter(stretch_train, stress_noisy, s=100, color='red',\n",
        "           edgecolors='black', linewidth=1.5,\n",
        "           label=f'Noisy measurements (SNR = {1/noise_level:.1f})',\n",
        "           zorder=5)\n",
        "\n",
        "# Error bars to show noise level\n",
        "plt.errorbar(stretch_train, stress_noisy,\n",
        "            yerr=noise_level * std_stress,  # ±1 std of noise\n",
        "            fmt='none', color='red', alpha=0.3, capsize=3)\n",
        "\n",
        "plt.xlabel('Stretch λ [-]')\n",
        "plt.ylabel('Stress S [kPa]')\n",
        "plt.title('The Challenge: Learn the Signal, Ignore the Noise')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Annotate signal-to-noise ratio\n",
        "snr = std_stress / (noise_level * std_stress)\n",
        "plt.text(0.02, 0.98, f'Signal-to-Noise Ratio: {snr:.1f}:1',\n",
        "        transform=plt.gca().transAxes, fontsize=11,\n",
        "        verticalalignment='top',\n",
        "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"📊 Noise characteristics:\")\n",
        "print(f\"   Noise level: ±{noise_level * std_stress:.1f} kPa (1σ)\")\n",
        "print(f\"   Relative noise: {noise_level * 100:.0f}% of normalized range\")\n",
        "print(f\"   SNR: {snr:.1f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrating model capacity: Underfitting, good fit, and overfitting\n",
        "\n",
        "Before applying neural networks to real material modeling problems, we must understand how model capacity affects prediction quality. This section demonstrates three archetypal scenarios using the hyperelastic stress-stretch relationship with noisy training data:\n",
        "\n",
        "**Underfitting**: A model with insufficient capacity (too few parameters, excessive regularization) cannot capture the underlying function, even when provided with clean training data. The predictions remain biased and systematically deviate from the true relationship.\n",
        "\n",
        "**Good fit**: A properly designed model with balanced capacity learns the underlying physical law while filtering out measurement noise. This represents the ideal scenario for material modeling applications.\n",
        "\n",
        "**Overfitting**: An over-parameterized model with excessive capacity memorizes the training data, including noise and measurement artifacts. The resulting predictions exhibit spurious oscillations between data points and fail to generalize."
      ],
      "metadata": {
        "id": "ATr4MwQ1XNLQ"
      },
      "id": "ATr4MwQ1XNLQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9f6809",
      "metadata": {
        "id": "ae9f6809"
      },
      "outputs": [],
      "source": [
        "# === COMPREHENSIVE DEMONSTRATION: Underfitting, Good Fit, and Overfitting ===\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: DATA NORMALIZATION (CRITICAL PREPROCESSING STEP)\n",
        "# ============================================================================\n",
        "# WHY NORMALIZE:\n",
        "# Neural networks train best when inputs and outputs are on similar scales (typically -1 to 1 or 0 to 1).\n",
        "# Our raw data: stretch λ ≈ 1-6, stress S ≈ 0-200 kPa (very different scales!)\n",
        "# Without normalization: gradients become unstable, training fails or is very slow.\n",
        "#\n",
        "# HOW WE NORMALIZE (Z-score standardization):\n",
        "# For any variable x: x_normalized = (x - mean(x)) / std(x)\n",
        "# This transforms data to have mean ≈ 0 and standard deviation ≈ 1\n",
        "\n",
        "# Calculate normalization parameters from TRAINING data only\n",
        "# (Never use test data statistics, as that would be \"cheating\" - the model would peek at test data)\n",
        "mean_stretch = stretch_train.mean()  # Average stretch value\n",
        "std_stretch = stretch_train.std()    # Standard deviation of stretch\n",
        "mean_stress = stress_noisy.mean()    # Average noisy stress value\n",
        "std_stress = stress_noisy.std()      # Standard deviation of noisy stress\n",
        "\n",
        "# Apply normalization: transform training data\n",
        "X_norm = (stretch_train - mean_stretch) / std_stretch  # Input (stretch)\n",
        "y_noisy_norm = (stress_noisy - mean_stress) / std_stress  # Output (noisy stress)\n",
        "\n",
        "# Later, when making predictions, we'll denormalize: y_original = y_norm * std + mean\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: UNDERFITTING MODEL (TOO SIMPLE, TOO CONSTRAINED)\n",
        "# ============================================================================\n",
        "# GOAL: Demonstrate what happens when model capacity is insufficient\n",
        "\n",
        "# ARCHITECTURE CHOICES:\n",
        "# - Layers: 2 (very shallow - cannot learn complex patterns)\n",
        "# - Neurons per layer: 8 (very narrow - limited representational power)\n",
        "# - Total parameters: ~153 (10 data points could be fit perfectly, but model cannot)\n",
        "\n",
        "# REGULARIZATION CHOICE:\n",
        "# - L2 penalty = 0.1 (VERY STRONG)\n",
        "#   L2 = \"weight decay\" = penalizes large weights\n",
        "#   Loss = MSE + 0.1 * sum(weights²)\n",
        "#   Effect: forces weights to stay small → smooth but biased predictions\n",
        "\n",
        "# OPTIMIZER CHOICE:\n",
        "# - SGD (Stochastic Gradient Descent) with learning_rate = 0.01\n",
        "#   SGD = basic optimizer, updates weights in direction of negative gradient\n",
        "#   Learning rate = step size (0.01 is slow, safe, but may underfit)\n",
        "#   Alternative optimizers: Adam (adaptive, faster), RMSprop (adaptive)\n",
        "\n",
        "print(\"Building underfitting model...\")\n",
        "model_under = build_mlp(hidden_layers=2, width=8, l2_reg=0.1)\n",
        "model_under.compile(\n",
        "    optimizer=keras.optimizers.SGD(0.01),  # Slow, basic optimizer\n",
        "    loss=\"mse\"  # Mean Squared Error = (prediction - truth)²\n",
        ")\n",
        "print(f\"Underfitting model parameters: {model_under.count_params():,}\")\n",
        "print(\"Training underfitting model...\")\n",
        "hist_under = model_under.fit(\n",
        "    X_norm[:,None],        # Input: normalized stretch ([:,None] reshapes to column vector)\n",
        "    y_noisy_norm[:,None],  # Output: normalized noisy stress\n",
        "    epochs=300,            # Number of times to see full dataset\n",
        "    batch_size=len(X_norm),  # Full batch\n",
        "    verbose=0,             # No output during training\n",
        "    validation_split=0.2   # 20% validation for consistent comparison\n",
        ")\n",
        "print(f\"  Completed: {len(hist_under.history['loss'])} epochs\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: GOOD FIT MODEL (BALANCED CAPACITY, PROPER REGULARIZATION)\n",
        "# ============================================================================\n",
        "# GOAL: Learn true underlying function while ignoring noise\n",
        "\n",
        "# ARCHITECTURE CHOICES:\n",
        "# - Layers: 4 (enough depth to model nonlinear stress-strain relationship)\n",
        "# - Neurons per layer: 64 (enough width for complex patterns, not excessive)\n",
        "# - Activation: SWISH (smooth, avoids saturation, better than ReLU or tanh for this problem)\n",
        "#\n",
        "# ACTIVATION FUNCTIONS YOU CAN EXPERIMENT WITH:\n",
        "# Replace \"keras.activations.swish\" with any of these:\n",
        "#   - keras.activations.relu       # Fast, piecewise-linear (may create kinks)\n",
        "#   - keras.activations.tanh       # Smooth, bounded [-1,1] (may saturate)\n",
        "#   - keras.activations.sigmoid    # Smooth, bounded [0,1] (may saturate)\n",
        "#   - keras.activations.elu        # Smooth negative values (good for deep nets)\n",
        "#   - keras.activations.selu       # Self-normalizing (good for very deep nets)\n",
        "#   - \"linear\"                     # No activation (not recommended for hidden layers)\n",
        "#\n",
        "# To experiment: Change line below and observe how middle panel curve changes!\n",
        "\n",
        "def build_goodfit_mlp():\n",
        "    \"\"\"\n",
        "    Build a feedforward neural network with balanced capacity.\n",
        "\n",
        "    PYTHON PROGRAMMING EXPLANATION:\n",
        "    - def build_goodfit_mlp(): defines a function (reusable block of code)\n",
        "    - keras.Sequential: container for layers stacked one after another\n",
        "    - layers.Input((1,)): defines input shape (1 number: stretch)\n",
        "    - for _ in range(4): Python loop that repeats 4 times\n",
        "    - layers.Dense(64, ...): fully-connected layer with 64 neurons\n",
        "    - m.add(...): appends layer to the sequential model\n",
        "    - return m: gives back the built model to whoever called this function\n",
        "    \"\"\"\n",
        "    m = keras.Sequential([layers.Input((1,))])  # Create empty model, define input\n",
        "\n",
        "    # Add 4 hidden layers (this loop executes 4 times)\n",
        "    for layer_num in range(4):  # layer_num will be 0, 1, 2, 3\n",
        "        m.add(layers.Dense(\n",
        "            64,  # Number of neurons (nodes) in this layer\n",
        "            activation=keras.activations.swish,  # ← EXPERIMENT HERE: try relu, tanh, elu\n",
        "            kernel_regularizer=regularizers.l2(1e-3),  # L2 penalty (weight decay)\n",
        "            name=f'hidden_{layer_num+1}'  # Name for debugging (hidden_1, hidden_2, ...)\n",
        "        ))\n",
        "\n",
        "    # Output layer: 1 neuron, linear activation (predicts stress value)\n",
        "    m.add(layers.Dense(1, activation='linear', name='output'))\n",
        "\n",
        "    # OPTIMIZER CHOICE:\n",
        "    # Adam = Adaptive Moment Estimation\n",
        "    #   - Adapts learning rate for each parameter individually\n",
        "    #   - Combines momentum (smooths updates) and RMSprop (scales by gradient history)\n",
        "    #   - learning_rate=1e-3 (0.001) is a good default\n",
        "    #   - Generally faster and more robust than SGD\n",
        "    m.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"mse\"  # Mean Squared Error\n",
        "    )\n",
        "    return m\n",
        "\n",
        "print(\"\\nBuilding good fit model...\")\n",
        "model_good = build_goodfit_mlp()\n",
        "print(f\"Good fit model parameters: {model_good.count_params():,}\")\n",
        "\n",
        "# TRAINING WITH EARLY STOPPING:\n",
        "# Early stopping = stop training when validation loss stops improving\n",
        "# This prevents overfitting: we stop before the model starts memorizing noise\n",
        "print(\"Training good fit model with early stopping...\")\n",
        "hist_good = model_good.fit(\n",
        "    X_norm[:,None], y_noisy_norm[:,None],\n",
        "    epochs=2000,  # Maximum epochs (early stopping may stop sooner)\n",
        "    batch_size=len(X_norm),  # Full batch: use all data per update (stable gradients)\n",
        "    verbose=0,  # No output (clean console)\n",
        "    validation_split=0.2,  # Use 20% of data for validation (2 points), 80% for training (8 points)\n",
        "    callbacks=[\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\",  # Watch validation loss (not training loss)\n",
        "            patience=120,  # Wait 120 epochs after best val_loss before stopping\n",
        "            restore_best_weights=True,  # Revert to epoch with best val_loss (not last)\n",
        "            verbose=0  # No output\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "print(f\"  Completed: {len(hist_good.history['loss'])} epochs (early stopping)\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: OVERFITTING MODEL (EXTREME CAPACITY, NO REGULARIZATION)\n",
        "# ============================================================================\n",
        "# GOAL: Memorize every training point, including noise\n",
        "\n",
        "# ARCHITECTURE CHOICES:\n",
        "# - Layers: 8 (very deep - can create complex interpolations)\n",
        "# - Neurons per layer: 256 (very wide - massive capacity)\n",
        "# - Activation: ReLU (creates piecewise-linear functions, enables sharp kinks)\n",
        "# - Total parameters: ~526,000 (for 10 data points!)\n",
        "\n",
        "# TRAINING STRATEGY:\n",
        "# - batch_size = 1 (SGD: update after EACH point, not after all points)\n",
        "# - shuffle = True (randomize order every epoch - helps memorization)\n",
        "# - epochs = 3000-6000 (excessive training time - learns noise as signal)\n",
        "# - NO regularization (L2 = 0) - nothing prevents memorization\n",
        "# - validation_split = 0.2 to show gap between train and val loss\n",
        "\n",
        "# Train on PHYSICAL units (not normalized) to exaggerate wiggles\n",
        "y_noisy_phys = stress_noisy  # Use original noisy stress values\n",
        "\n",
        "def build_overfit_mlp():\n",
        "    \"\"\"\n",
        "    Build an intentionally over-parameterized network to demonstrate overfitting.\n",
        "\n",
        "    PYTHON PROGRAMMING EXPLANATION:\n",
        "    - This function uses the same structure as build_goodfit_mlp()\n",
        "    - Key difference: MORE layers (8 vs 4), MORE neurons (256 vs 64)\n",
        "    - No regularization: kernel_regularizer is omitted (defaults to None)\n",
        "    \"\"\"\n",
        "    m = keras.Sequential([layers.Input((1,))])\n",
        "\n",
        "    # Add 8 hidden layers with 256 neurons each (massive capacity!)\n",
        "    for layer_num in range(8):\n",
        "        m.add(layers.Dense(\n",
        "            256,  # 256 neurons per layer\n",
        "            activation=\"relu\",  # ReLU = max(0, x), creates piecewise-linear kinks\n",
        "            name=f'overfit_hidden_{layer_num+1}'\n",
        "        ))\n",
        "        # Note: NO kernel_regularizer - nothing penalizes large weights!\n",
        "\n",
        "    m.add(layers.Dense(1, activation='linear', name='output'))\n",
        "    m.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "print(\"\\nBuilding overfitting model...\")\n",
        "model_over = build_overfit_mlp()\n",
        "print(f\"Overfitting model parameters: {model_over.count_params():,} (vs {len(X_norm)} data points!)\")\n",
        "\n",
        "print(\"Training overfitting model (memorizing noise)...\")\n",
        "hist_over = model_over.fit(\n",
        "    X_norm[:, None], y_noisy_phys[:, None],\n",
        "    epochs=6000 if not FAST else 3000,\n",
        "    batch_size=1,  # SGD on individual points (not batches)\n",
        "    verbose=0,  # No output (clean console)\n",
        "    shuffle=True,  # Randomize point order each epoch\n",
        "    validation_split=0.2  # Track validation loss to show train/val gap\n",
        ")\n",
        "print(f\"  Completed: {len(hist_over.history['loss'])} epochs\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: EVALUATION AND VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "# Create dense grid for smooth prediction curves\n",
        "stretch_dense = np.linspace(stretch_train.min(), stretch_train.max(), 500)\n",
        "X_dense_norm = (stretch_dense - mean_stretch) / std_stretch  # Normalize using training statistics\n",
        "\n",
        "# Ground truth (analytical solution)\n",
        "stress_true_dense = compute_S_stress(stretch_dense)\n",
        "\n",
        "# Model predictions (denormalize outputs back to physical units)\n",
        "pred_under = model_under.predict(X_dense_norm[:,None], verbose=0).ravel() * std_stress + mean_stress\n",
        "pred_good  = model_good.predict(X_dense_norm[:,None], verbose=0).ravel() * std_stress + mean_stress\n",
        "pred_over  = model_over.predict(X_dense_norm[:,None], verbose=0).ravel()  # Already in physical units\n",
        "\n",
        "stress_noisy = y_noisy_norm * std_stress + mean_stress  # Denormalize noisy training data\n",
        "\n",
        "# Plot three-panel comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "titles = [\"Underfitting\\n(Too simple / heavy regularization)\",\n",
        "          \"Good fit\\n(Balanced capacity + L2)\",\n",
        "          \"Overfitting\\n(Memorizes noise)\"]\n",
        "curves  = [pred_under, pred_good, pred_over]\n",
        "colors  = [\"#ff7f0e\", \"#2ca02c\", \"#9467bd\"]  # Orange, green, purple\n",
        "\n",
        "# Set common y-limits for fair comparison\n",
        "y_min = min(stress_true_dense.min(), stress_noisy.min()) - 5\n",
        "y_max = max(stress_true_dense.max(), stress_noisy.max()) + 5\n",
        "\n",
        "true_color = \"#1f77b4\"  # Force true curve to blue in all panels\n",
        "\n",
        "for ax, pred, title, col in zip(axes, curves, titles, colors):\n",
        "    # True function (always blue for consistency)\n",
        "    ax.plot(stretch_dense, stress_true_dense, lw=2, alpha=0.8,\n",
        "            color=true_color, label=\"True function\")\n",
        "    # Model prediction (panel-specific color)\n",
        "    ax.plot(stretch_dense, pred, lw=2.5, color=col, label=\"Model prediction\")\n",
        "    # Noisy training points (drawn on top, visible)\n",
        "    ax.scatter(\n",
        "        stretch_train, stress_noisy, s=50, color=\"crimson\",\n",
        "        edgecolors=\"black\", linewidth=0.5, alpha=0.9, zorder=5,\n",
        "        label=\"Noisy data\"\n",
        "    )\n",
        "    ax.set_xlabel(r\"Stretch $\\lambda$ [–]\")\n",
        "    ax.set_ylabel(r\"Stress $S$ [kPa]\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc=\"lower right\", fontsize=9)\n",
        "\n",
        "plt.suptitle(\"Underfitting vs. Good Fit vs. Overfitting\", fontsize=15, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print capacity summary\n",
        "print(f\"\\nModel capacity comparison:\")\n",
        "print(f\"  Underfitting:  {model_under.count_params():>7,} parameters\")\n",
        "print(f\"  Good fit:      {model_good.count_params():>7,} parameters\")\n",
        "print(f\"  Overfitting:   {model_over.count_params():>7,} parameters\")\n",
        "print(f\"  Training data: {len(stretch_train):>7} points\")\n",
        "print(f\"  Parameter/data ratio (overfit): {model_over.count_params()/len(stretch_train):,.0f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d8e766",
      "metadata": {
        "id": "c8d8e766"
      },
      "source": [
        "## Level 3: Viscoelastic material (history-dependent)\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "**Engineering challenge**: Many materials exhibit memory effects that cannot be captured by instantaneous stress-stretch relationships. Viscoelastic polymers under cyclic loading demonstrate three critical phenomena:\n",
        "\n",
        "- **Rate dependence**: Faster loading rates produce stiffer mechanical responses and higher peak stresses\n",
        "- **Hysteresis**: The loading and unloading paths differ, creating closed loops in stress-stretch space\n",
        "- **Energy dissipation**: The area enclosed by the hysteresis loop represents mechanical energy converted to heat\n",
        "\n",
        "Traditional modeling approaches require solving coupled differential equations at each material point. The question arises: can neural networks learn this complex history-dependent behavior directly from experimental data?\n",
        "\n",
        "### The fundamental problem\n",
        "\n",
        "**Feedforward networks are fundamentally inadequate** because they learn mappings of the form:\n",
        "\n",
        "$$S = f(\\lambda)$$\n",
        "\n",
        "This assumes stress depends only on the current stretch state. However, viscoelastic behavior constitutes a **functional** relationship:\n",
        "\n",
        "$$S(t) = \\mathcal{F}\\left[\\{\\lambda(\\tau), \\dot{\\lambda}(\\tau)\\}_{\\tau \\leq t}\\right]$$\n",
        "\n",
        "The current stress state depends on the entire loading history up to the present time, not merely the instantaneous stretch. This history dependence manifests in two critical ways:\n",
        "\n",
        "1. **Path dependence**: Two specimens stretched to the same $\\lambda$ will exhibit different stresses if they followed different loading histories\n",
        "2. **Rate sensitivity**: The same stretch amplitude applied at different rates produces different stress responses\n",
        "\n",
        "### The Maxwell power-law constitutive model\n",
        "\n",
        "The code implements a nonlinear Maxwell model combining elastic and viscous elements in series. The governing differential equation is:\n",
        "\n",
        "$$\\frac{d\\sigma}{dt} = E \\frac{d\\varepsilon}{dt} - \\frac{\\sigma}{\\eta_0} \\left|\\frac{\\sigma}{\\sigma_0}\\right|^{m-1}$$\n",
        "\n",
        "where the material parameters are:\n",
        "\n",
        "- $E = 3.0$ kPa: elastic modulus (instantaneous stiffness)\n",
        "- $\\eta_0 = 0.2$ kPa·s: viscosity coefficient (resistance to flow)\n",
        "- $\\sigma_0 = 1.0$ kPa: reference stress (normalization constant)\n",
        "- $m = 3$: power-law exponent (degree of nonlinearity)\n",
        "\n",
        "**Physical interpretation of the terms**:\n",
        "\n",
        "1. **Elastic term** $E \\frac{d\\varepsilon}{dt}$: Represents instantaneous stress buildup proportional to the strain rate. This captures the elastic spring behavior of the material.\n",
        "\n",
        "2. **Viscous term** $\\frac{\\sigma}{\\eta_0} \\left|\\frac{\\sigma}{\\sigma_0}\\right|^{m-1}$: Describes time-dependent stress relaxation through viscous flow. The power-law form captures nonlinear viscosity, allowing the model to represent shear thinning ($m < 1$) or shear thickening ($m > 1$) behavior.\n",
        "\n",
        "### Observable phenomena in the simulation results\n",
        "\n",
        "The generated hysteresis loops reveal several key characteristics:\n",
        "\n",
        "1. **Rate-dependent stiffening**: The fast loading curve ($\\dot{\\lambda} = 3.0$ s$^{-1}$) reaches significantly higher peak stresses than the slow curve, demonstrating viscous resistance to rapid deformation.\n",
        "\n",
        "2. **Hysteresis loop area**: The enclosed area represents energy dissipated per loading cycle. Faster rates typically produce larger loops, indicating greater energy dissipation.\n",
        "\n",
        "3. **Stress relaxation during unloading**: The unloading path lies below the loading path because stress has time to relax through viscous flow.\n",
        "\n",
        "4. **Non-equilibrium behavior**: At any given stretch $\\lambda$, the stress state depends on whether the material is currently loading or unloading and at what rate.\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "Working with this viscoelastic model teaches several critical concepts for neural network applications:\n",
        "\n",
        "1. **Recognize when recurrent architectures are necessary**: Any phenomenon exhibiting path dependence, rate sensitivity, or memory effects requires sequential models rather than feedforward networks.\n",
        "\n",
        "2. **Format sequential data properly**: Time series must be structured with appropriate features (stretch, stretch rate, time) and target variables (stress). The temporal ordering must be preserved.\n",
        "\n",
        "3. **Understand hidden states in RNNs**: The hidden state vector serves as a learned representation of the loading history, effectively replacing the need to explicitly track $\\{\\lambda(\\tau), \\dot{\\lambda}(\\tau)\\}_{\\tau \\leq t}$.\n",
        "\n",
        "4. **Validate generalization to new loading paths**: The trained RNN must be tested on loading rates not seen during training to assess whether it has learned the underlying physics rather than memorizing specific trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b01a1c",
      "metadata": {
        "id": "37b01a1c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISCOELASTIC MATERIAL MODEL: MAXWELL POWER-LAW\n",
        "# ============================================================================\n",
        "# This model captures both elastic (instant) and viscous (time-dependent) response\n",
        "\n",
        "# Material parameters (typical for viscoelastic polymer)\n",
        "E_visco = 3.0      # Elastic modulus [kPa] - instant stiffness\n",
        "nu_0 = 2e-1        # Viscosity coefficient [kPa·s] - flow resistance\n",
        "sigma_0 = 1.0      # Reference stress [kPa] - normalizes nonlinearity\n",
        "m = 3              # Power-law exponent [-] - degree of nonlinearity\n",
        "\n",
        "def sigma_dot(sigma, strain_rate):\n",
        "    \"\"\"\n",
        "    Compute stress rate from Maxwell power-law constitutive model.\n",
        "\n",
        "    The differential equation is:\n",
        "    dσ/dt = E·dε/dt - (σ/η)·|σ/σ₀|^(m-1)\n",
        "\n",
        "    Physical interpretation:\n",
        "    - First term: Elastic response (instant, proportional to strain rate)\n",
        "    - Second term: Viscous relaxation (time-dependent stress decay)\n",
        "    - Power law: Captures nonlinear viscosity (shear thinning/thickening)\n",
        "\n",
        "    Parameters:\n",
        "        sigma: Current Cauchy stress [kPa]\n",
        "        strain_rate: Current strain rate dε/dt [1/s]\n",
        "\n",
        "    Returns:\n",
        "        dsigma_dt: Rate of stress change [kPa/s]\n",
        "    \"\"\"\n",
        "    # Elastic contribution: stress builds proportionally to strain rate\n",
        "    elastic_term = E_visco * strain_rate\n",
        "\n",
        "    # Viscous contribution: stress relaxes over time\n",
        "    if abs(sigma) < 1e-10:  # Avoid division by zero at zero stress\n",
        "        viscous_term = 0.0\n",
        "    else:\n",
        "        # Power-law viscosity: resistance depends on stress level\n",
        "        viscous_term = (sigma / nu_0) * abs(sigma / sigma_0)**(m - 1)\n",
        "\n",
        "    # Total rate: build-up minus relaxation\n",
        "    return elastic_term - viscous_term\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE REFERENCE VISCOELASTIC CURVES\n",
        "# ============================================================================\n",
        "\n",
        "def generate_visco_path(rate, dt_factor=1.0):\n",
        "    \"\"\"\n",
        "    Generate viscoelastic loading/unloading cycle.\n",
        "\n",
        "    Simulates a complete cycle:\n",
        "    1. Loading: Stretch from λ=1.0 to λ=2.5 at constant rate\n",
        "    2. Unloading: Return to λ=1.0 at same rate\n",
        "\n",
        "    This creates a hysteresis loop showing energy dissipation.\n",
        "\n",
        "    Parameters:\n",
        "        rate: Strain rate [1/s] - how fast we stretch\n",
        "        dt_factor: Time step multiplier for numerical stability\n",
        "\n",
        "    Returns:\n",
        "        stretches, stresses, times, rates: Arrays of the loading path\n",
        "    \"\"\"\n",
        "    # Time step: smaller for higher rates (numerical stability)\n",
        "    dt = 1e-3 / rate * dt_factor\n",
        "\n",
        "    # Initialize state\n",
        "    stretch = 1.0  # Start undeformed\n",
        "    stress = 0.0   # Start stress-free\n",
        "\n",
        "    # Storage arrays\n",
        "    stretches = []\n",
        "    stresses = []\n",
        "    times = []\n",
        "    rates = []\n",
        "\n",
        "    t = 0.0\n",
        "\n",
        "    # PHASE 1: Loading (stretch increases)\n",
        "    while stretch < 2.5:\n",
        "        # Record current state\n",
        "        stretches.append(stretch)\n",
        "        stresses.append(stress / stretch**2)  # Convert Cauchy to S\n",
        "        times.append(t)\n",
        "        rates.append(rate)\n",
        "\n",
        "        # Integrate one time step (Forward Euler)\n",
        "        dsigma_dt = sigma_dot(stress, rate)\n",
        "        stretch += dt * rate      # Update stretch\n",
        "        stress += dt * dsigma_dt  # Update stress\n",
        "        t += dt\n",
        "\n",
        "    # PHASE 2: Unloading (stretch decreases)\n",
        "    while stretch > 1.01:  # Stop near λ=1\n",
        "        stretches.append(stretch)\n",
        "        stresses.append(stress / stretch**2)\n",
        "        times.append(t)\n",
        "        rates.append(-rate)  # Negative rate for unloading\n",
        "\n",
        "        dsigma_dt = sigma_dot(stress, -rate)\n",
        "        stretch -= dt * rate\n",
        "        stress += dt * dsigma_dt\n",
        "        t += dt\n",
        "\n",
        "    return np.array(stretches), np.array(stresses), np.array(times), np.array(rates)\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE TWO REFERENCE CURVES AT DIFFERENT RATES\n",
        "# ============================================================================\n",
        "print(\"🔄 Generating viscoelastic reference curves...\")\n",
        "\n",
        "# Two different loading rates to show rate dependence\n",
        "rate1 = 1.0  # Slow loading [1/s]\n",
        "rate2 = 3.0  # Fast loading [1/s]\n",
        "\n",
        "# Generate complete loading cycles\n",
        "stretch1, stress1, time1, rates1 = generate_visco_path(rate1)\n",
        "stretch2, stress2, time2, rates2 = generate_visco_path(rate2)\n",
        "\n",
        "print(f\"✅ Generated reference curves:\")\n",
        "print(f\"   Curve 1: rate = {rate1}/s, {len(stretch1)} points, duration = {time1[-1]:.2f}s\")\n",
        "print(f\"   Curve 2: rate = {rate2}/s, {len(stretch2)} points, duration = {time2[-1]:.2f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE VISCOELASTIC BEHAVIOR\n",
        "# ============================================================================\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# LEFT PANEL: Stress-stretch curves (hysteresis loops)\n",
        "ax1.plot(stretch1, stress1, 'b-', linewidth=2.5,\n",
        "        label=f'Slow (rate = {rate1}/s)', alpha=0.8)\n",
        "ax1.plot(stretch2, stress2, 'r-', linewidth=2.5,\n",
        "        label=f'Fast (rate = {rate2}/s)', alpha=0.8)\n",
        "\n",
        "# Mark loading direction with arrows\n",
        "mid_idx1 = len(stretch1) // 4\n",
        "ax1.annotate('', xy=(stretch1[mid_idx1], stress1[mid_idx1]),\n",
        "            xytext=(stretch1[mid_idx1-5], stress1[mid_idx1-5]),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
        "\n",
        "mid_idx2 = len(stretch2) // 4\n",
        "ax1.annotate('', xy=(stretch2[mid_idx2], stress2[mid_idx2]),\n",
        "            xytext=(stretch2[mid_idx2-5], stress2[mid_idx2-5]),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "\n",
        "ax1.set_xlabel('Stretch λ [-]')\n",
        "ax1.set_ylabel('Second Piola-Kirchhoff Stress S [kPa]')\n",
        "ax1.set_title('Viscoelastic Hysteresis Loops')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotation about energy dissipation\n",
        "ax1.fill(stretch1, stress1, alpha=0.1, color='blue')\n",
        "ax1.text(1.75, 10, 'Area = Energy\\ndissipated',\n",
        "        ha='center', fontsize=10,\n",
        "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# RIGHT PANEL: Time evolution\n",
        "ax2.plot(time1, stretch1, 'b-', linewidth=1.5,\n",
        "        label='Stretch (slow)', alpha=0.7)\n",
        "ax2.plot(time1, stress1, 'b--', linewidth=1.5,\n",
        "        label='Stress (slow)', alpha=0.7)\n",
        "ax2.plot(time2, stretch2, 'r-', linewidth=1.5,\n",
        "        label='Stretch (fast)', alpha=0.7)\n",
        "ax2.plot(time2, stress2, 'r--', linewidth=1.5,\n",
        "        label='Stress (fast)', alpha=0.7)\n",
        "\n",
        "ax2.set_xlabel('Time [s]')\n",
        "ax2.set_ylabel('Value')\n",
        "ax2.set_title('Time Series: Loading and Unloading')\n",
        "ax2.legend(fontsize=9, loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# KEY OBSERVATIONS TO UNDERSTAND\n",
        "# ============================================================================\n",
        "print(f\"\\n🔑 Critical observations:\")\n",
        "print(f\"   1. Rate dependence: Faster loading → Higher peak stress\")\n",
        "print(f\"   2. Hysteresis: Different paths for loading vs unloading\")\n",
        "print(f\"   3. Energy dissipation: Area inside loop = energy lost\")\n",
        "print(f\"   4. History matters: Same stretch, different stress depending on path\")\n",
        "print(f\"\\n⚠️  Feedforward networks CANNOT capture this behavior!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent neural networks\n",
        "\n",
        "**Recurrent neural networks (RNNs)** resolve the history-dependence problem through an internal memory mechanism. Unlike feedforward networks, RNNs maintain a **hidden state vector** $\\mathbf{h}_t$ that evolves over time:\n",
        "\n",
        "$$\\mathbf{h}_t = \\phi(\\mathbf{W}_h \\mathbf{h}_{t-1} + \\mathbf{W}_x \\mathbf{x}_t + \\mathbf{b}_h)$$\n",
        "\n",
        "$$\\mathbf{y}_t = \\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{x}_t = [\\lambda_t, \\dot{\\lambda}_t, \\Delta t]^T$ is the input feature vector at time $t$\n",
        "- $\\mathbf{h}_t$ encodes the complete loading history up to time $t$\n",
        "- $\\mathbf{y}_t = S_t$ is the predicted stress output\n",
        "- $\\mathbf{W}_h, \\mathbf{W}_x, \\mathbf{W}_y$ are learnable weight matrices\n",
        "- $\\phi(\\cdot)$ is a nonlinear activation function (typically tanh or ReLU)\n",
        "\n",
        "**How RNNs capture material memory**: The hidden state $\\mathbf{h}_t$ serves as a compressed representation of all previous loading history $\\{\\lambda(\\tau), \\dot{\\lambda}(\\tau)\\}_{\\tau < t}$. At each time step, the network:\n",
        "\n",
        "1. Receives current kinematic input $[\\lambda_t, \\dot{\\lambda}_t, \\Delta t]$\n",
        "2. Updates the hidden state by combining new information with previous history\n",
        "3. Predicts stress $S_t$ based on the updated hidden state\n",
        "\n",
        "This architecture naturally respects causality because $\\mathbf{h}_t$ only depends on inputs from $\\tau \\leq t$, never future values.\n",
        "\n",
        "**Key advantages for viscoelastic modeling**:\n",
        "- **No explicit history storage**: The network learns optimal compression of past information\n",
        "- **Automatic feature extraction**: No need to manually engineer history-dependent features\n",
        "- **Generalizable**: Can predict stress for arbitrary loading paths not seen during training\n"
      ],
      "metadata": {
        "id": "DjGlzB7sQ2fB"
      },
      "id": "DjGlzB7sQ2fB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35acb3c0",
      "metadata": {
        "id": "35acb3c0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RNN ARCHITECTURE FOR VISCOELASTIC MODELING\n",
        "# ============================================================================\n",
        "\n",
        "def build_simple_rnn(hidden_size=8, num_layers=2):\n",
        "    \"\"\"\n",
        "    Build a Recurrent Neural Network for history-dependent material modeling.\n",
        "\n",
        "    KEY CONCEPT: RNNs maintain a hidden state that summarizes history:\n",
        "    h_t = f(x_t, h_{t-1})\n",
        "    y_t = g(h_t)\n",
        "\n",
        "    This allows the network to \"remember\" past loading and make predictions\n",
        "    based on the entire history, not just current input.\n",
        "\n",
        "    ARCHITECTURE DECISIONS:\n",
        "    1. Hidden size: Dimensionality of memory vector\n",
        "       - Small (4-8): Forces compression, may lose details\n",
        "       - Large (32-64): Rich memory but may overfit\n",
        "\n",
        "    2. Number of layers: Depth of temporal processing\n",
        "       - 1 layer: Simple memory, fast\n",
        "       - 2-3 layers: Hierarchical temporal features\n",
        "       - >3 layers: Risk of gradient vanishing\n",
        "\n",
        "    3. Input features: [stretch, strain_rate, dt]\n",
        "       - stretch: Current deformation state\n",
        "       - strain_rate: How fast we're loading\n",
        "       - dt: Time step (for numerical integration)\n",
        "\n",
        "    Parameters:\n",
        "        hidden_size: Size of RNN hidden state\n",
        "        num_layers: Number of stacked RNN layers\n",
        "\n",
        "    Returns:\n",
        "        Keras model with RNN architecture\n",
        "    \"\"\"\n",
        "\n",
        "    # Adjust for FAST mode (CPU-friendly)\n",
        "    h_size = hidden_size if not FAST else max(4, hidden_size // 2)\n",
        "    n_layers = num_layers if not FAST else max(1, num_layers // 2)\n",
        "\n",
        "    # Define input: sequences of variable length\n",
        "    # Shape: (batch_size, time_steps, 3)\n",
        "    # The 3 features are: [stretch, strain_rate, dt]\n",
        "    inp = layers.Input(shape=(None, 3), name='sequence_input')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    # Stack RNN layers\n",
        "    for i in range(n_layers):\n",
        "        # All layers return sequences (we need per-timestep predictions)\n",
        "        x = layers.SimpleRNN(\n",
        "            h_size,\n",
        "            activation='tanh',      # Smooth, bounded activation\n",
        "            return_sequences=True,  # Output at every time step\n",
        "            name=f'rnn_layer_{i+1}'\n",
        "        )(x)\n",
        "\n",
        "        # How it works internally:\n",
        "        # For each time step t:\n",
        "        #   h[t] = tanh(W_h @ h[t-1] + W_x @ x[t] + b)\n",
        "        #   output[t] = h[t]\n",
        "\n",
        "    # Output layer: predict stress at each time step\n",
        "    out = layers.Dense(1, activation='linear', name='stress_output')(x)\n",
        "\n",
        "    # Build model\n",
        "    model = keras.Model(inp, out, name='viscoelastic_rnn')\n",
        "\n",
        "    print(f\"\\n📊 RNN Architecture (FAST={'ON' if FAST else 'OFF'}):\")\n",
        "    print(f\"   Hidden state size: {h_size}\")\n",
        "    print(f\"   Number of RNN layers: {n_layers}\")\n",
        "    print(f\"   Total parameters: {model.count_params():,}\")\n",
        "    print(f\"   Input: (batch, timesteps, 3) → [stretch, rate, dt]\")\n",
        "    print(f\"   Output: (batch, timesteps, 1) → [stress at each time]\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# BUILD AND VISUALIZE RNN\n",
        "# ============================================================================\n",
        "print(\"\\n🏗️ Building RNN for viscoelastic modeling...\")\n",
        "\n",
        "hidden_state_size = 8 if not FAST else 4\n",
        "rnn_layers = 3 if not FAST else 2\n",
        "\n",
        "model_rnn_visco = build_simple_rnn(\n",
        "    hidden_size=hidden_state_size,\n",
        "    num_layers=rnn_layers\n",
        ")\n",
        "\n",
        "# Show detailed architecture\n",
        "model_rnn_visco.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The critical challenge becomes preparing sequential training data that captures the essential physics of rate-dependent, hysteretic behavior.\n",
        "\n",
        "### Generating variable-rate training sequences\n",
        "\n",
        "The training data generation strategy differs fundamentally from the hyperelastic and elastoplastic cases. Rather than sampling random points in stretch space, we must generate **temporal sequences** that capture the history-dependent nature of viscoelastic response.\n",
        "\n",
        "### Feature engineering for temporal learning\n",
        "\n",
        "**Input feature vector**: At each time step $t$, the RNN receives three features:\n",
        "\n",
        "$$\\mathbf{x}_t = \\begin{bmatrix} \\lambda_t \\\\ \\dot{\\lambda}_t \\\\ \\Delta t \\end{bmatrix}$$\n",
        "\n",
        "**Physical justification for each feature**:\n",
        "\n",
        "1. **Stretch $\\lambda_t$**: Current deformation state (kinematic variable)\n",
        "2. **Stretch rate $\\dot{\\lambda}_t$**: Rate of deformation (captures viscous effects)\n",
        "3. **Time increment $\\Delta t$**: Duration of the current step (necessary for numerical integration of rate-dependent processes)\n",
        "\n",
        "The inclusion of $\\Delta t$ is crucial because the stress evolution depends not only on the rate magnitude but also on the duration over which that rate is applied. Two scenarios with identical $\\lambda_t$ and $\\dot{\\lambda}_t$ but different $\\Delta t$ values will produce different stress increments due to viscous relaxation effects.\n",
        "\n",
        "**Output target**: The second Piola-Kirchhoff stress $S_t = \\sigma_t / \\lambda_t^2$ at each time step.\n",
        "\n",
        "**Tensor shape requirements**: RNN layers expect input tensors with shape:\n",
        "\n",
        "$$(B, T, F)$$\n",
        "\n",
        "where:\n",
        "- $B$ = batch size (number of independent sequences)\n",
        "- $T$ = sequence length (number of time steps)\n",
        "- $F$ = feature dimension (number of input variables)\n",
        "\n",
        "For this viscoelastic training:\n",
        "- **Input**: `X_rnn.shape = (1, T, 3)` where $T \\approx 500$ time steps, 3 features\n",
        "- **Output**: `y_rnn.shape = (1, T, 1)` where we predict 1 stress value per time step\n",
        "\n",
        "**Single sequence training**: The batch size $B=1$ indicates we train on a single long temporal sequence. This approach is appropriate for the current problem because:\n",
        "\n",
        "1. The sequence is sufficiently long ($T \\sim 500$) to capture multiple loading-unloading transitions\n",
        "2. Variable rates within the sequence provide diverse training examples\n",
        "3. Temporal dependencies within one sequence are more important than batch diversity\n",
        "\n",
        "In production applications with multiple experimental specimens, one would increase batch size by stacking multiple independent loading histories."
      ],
      "metadata": {
        "id": "5UeV_jcZRE2P"
      },
      "id": "5UeV_jcZRE2P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8944809b",
      "metadata": {
        "id": "8944809b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREPARE TRAINING DATA FOR RNN\n",
        "# ============================================================================\n",
        "print(\"\\n📦 Preparing sequential data for RNN training...\")\n",
        "\n",
        "# Generate training sequence with variable loading rates\n",
        "# This teaches the network to handle different loading conditions\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Initialize\n",
        "stretch_seq = []\n",
        "stress_seq = []\n",
        "rate_seq = []\n",
        "time_seq = []\n",
        "\n",
        "stretch = 1.0\n",
        "stress = 0.0\n",
        "t = 0.0\n",
        "\n",
        "# Parameters for variable-rate loading\n",
        "rate_mean = 1.2  # Average strain rate [1/s]\n",
        "dt_mean = 1e-1 / rate_mean\n",
        "\n",
        "print(\"   Generating variable-rate loading history...\")\n",
        "\n",
        "# LOADING PHASE: Stretch from 1.0 to 2.5 with varying rates\n",
        "while stretch < 2.5:\n",
        "    stretch_seq.append(stretch)\n",
        "    stress_seq.append(stress / stretch**2)  # Convert to S\n",
        "\n",
        "    # Randomly vary the time step and rate (±20% variation)\n",
        "    dt = dt_mean * np.random.uniform(0.8, 1.2)\n",
        "    rate = rate_mean * np.random.uniform(0.8, 1.2)\n",
        "\n",
        "    rate_seq.append(rate)\n",
        "    time_seq.append(t)\n",
        "\n",
        "    # Integrate constitutive equation\n",
        "    dsigma_dt = sigma_dot(stress, rate)\n",
        "    stretch += dt * rate\n",
        "    stress += dt * dsigma_dt\n",
        "    t += dt\n",
        "\n",
        "# UNLOADING PHASE: Return to λ=1 with varying rates\n",
        "while stretch > 1.01:\n",
        "    stretch_seq.append(stretch)\n",
        "    stress_seq.append(stress / stretch**2)\n",
        "\n",
        "    dt = dt_mean * np.random.uniform(0.8, 1.2)\n",
        "    rate = -rate_mean * np.random.uniform(0.8, 1.2)\n",
        "\n",
        "    rate_seq.append(rate)\n",
        "    time_seq.append(t)\n",
        "\n",
        "    dsigma_dt = sigma_dot(stress, rate)\n",
        "    stretch += dt * rate\n",
        "    stress += dt * dsigma_dt\n",
        "    t += dt\n",
        "\n",
        "# Convert to arrays\n",
        "stretch_seq = np.array(stretch_seq)\n",
        "stress_seq = np.array(stress_seq)\n",
        "rate_seq = np.array(rate_seq)\n",
        "time_seq = np.array(time_seq)\n",
        "\n",
        "print(f\"   Generated sequence: {len(stretch_seq)} time steps\")\n",
        "\n",
        "# ============================================================================\n",
        "# FORMAT DATA FOR RNN\n",
        "# ============================================================================\n",
        "print(\"\\n🔧 Formatting data for RNN (normalization + feature stacking)...\")\n",
        "\n",
        "# Compute time steps\n",
        "dt_seq = np.diff(time_seq, prepend=time_seq[0])\n",
        "if len(time_seq) > 1 and dt_seq[0] == 0:\n",
        "    dt_seq[0] = float(np.median(np.diff(time_seq)))\n",
        "\n",
        "# Safe normalization (avoid division by zero)\n",
        "def safe_mean_std(x, eps=1e-12):\n",
        "    m = float(np.mean(x))\n",
        "    s = float(np.std(x))\n",
        "    if s < eps:\n",
        "        s = 1.0  # No scaling if variance is ~0\n",
        "    return m, s\n",
        "\n",
        "# Calculate normalization statistics\n",
        "mean_rate, std_rate = safe_mean_std(rate_seq)\n",
        "mean_dt, std_dt = safe_mean_std(dt_seq)\n",
        "mean_str, std_str = safe_mean_std(stretch_seq)\n",
        "mean_st, std_st = safe_mean_std(stress_seq)\n",
        "\n",
        "print(f\"   Normalization statistics:\")\n",
        "print(f\"     Stretch: μ={mean_str:.2f}, σ={std_str:.2f}\")\n",
        "print(f\"     Stress:  μ={mean_st:.1f}, σ={std_st:.1f}\")\n",
        "print(f\"     Rate:    μ={mean_rate:.2f}, σ={std_rate:.2f}\")\n",
        "\n",
        "# Normalize all features\n",
        "stretch_norm = (stretch_seq - mean_str) / std_str\n",
        "rate_norm = (rate_seq - mean_rate) / std_rate\n",
        "dt_norm = (dt_seq - mean_dt) / std_dt\n",
        "stress_norm = (stress_seq - mean_st) / std_st\n",
        "\n",
        "# Stack features: each time step has [stretch, rate, dt]\n",
        "X_rnn = np.stack([stretch_norm, rate_norm, dt_norm], axis=1)  # Shape: (T, 3)\n",
        "y_rnn = stress_norm.reshape(-1, 1)                            # Shape: (T, 1)\n",
        "\n",
        "# Add batch dimension (RNNs expect batched data)\n",
        "X_rnn = X_rnn[np.newaxis, :, :]  # Shape: (1, T, 3)\n",
        "y_rnn = y_rnn[np.newaxis, :, :]  # Shape: (1, T, 1)\n",
        "\n",
        "print(f\"   RNN input shape: {X_rnn.shape} → (batch=1, time={X_rnn.shape[1]}, features=3)\")\n",
        "print(f\"   RNN output shape: {y_rnn.shape} → (batch=1, time={y_rnn.shape[1]}, outputs=1)\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE TRAINING SEQUENCE\n",
        "# ============================================================================\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# LEFT: Stress-stretch with color-coded time\n",
        "scatter = ax1.scatter(stretch_seq, stress_seq,\n",
        "                     s=20, c=time_seq, cmap='viridis',\n",
        "                     alpha=0.6, edgecolors='none')\n",
        "ax1.set_xlabel('Stretch λ [-]')\n",
        "ax1.set_ylabel('Second Piola-Kirchhoff Stress S [kPa]')\n",
        "ax1.set_title('Training Sequence (color = time progression)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(scatter, ax=ax1)\n",
        "cbar.set_label('Time [s]')\n",
        "\n",
        "# RIGHT: Time series\n",
        "ax2.plot(time_seq, stretch_seq, 'b-', linewidth=1, label='Stretch', alpha=0.7)\n",
        "ax2.plot(time_seq, stress_seq, 'r-', linewidth=1, label='Stress', alpha=0.7)\n",
        "ax2.plot(time_seq, rate_seq, 'g--', linewidth=0.8, label='Rate', alpha=0.5)\n",
        "ax2.set_xlabel('Time [s]')\n",
        "ax2.set_ylabel('Value')\n",
        "ax2.set_title('Training Time Series')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture summary**:\n",
        "```\n",
        "Input: (batch, time, features=3) → RNN(32) → Dense(1) → Output: (batch, time, outputs=1)\n",
        "```\n",
        "\n",
        "The 32-unit RNN provides sufficient capacity to learn the viscoelastic dynamics without overfitting to the training sequence. Smaller networks (e.g., 16 units) may underfit, while larger networks (e.g., 64+ units) risk memorizing the specific training trajectory rather than learning the underlying constitutive law."
      ],
      "metadata": {
        "id": "PdgkuyOSSenG"
      },
      "id": "PdgkuyOSSenG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba26277",
      "metadata": {
        "id": "4ba26277"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN RNN WITH PROGRESS MONITORING\n",
        "# ============================================================================\n",
        "print(\"\\n🏋️ Training RNN on viscoelastic sequence...\")\n",
        "\n",
        "# Configure optimizer\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-2 if not FAST else 5e-3)\n",
        "model_rnn_visco.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Callbacks for training\n",
        "es_rnn = callbacks.EarlyStopping(\n",
        "    patience=200 if not FAST else 50,\n",
        "    restore_best_weights=True,\n",
        "    monitor=\"loss\",\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Checkpoint callback (Keras 3 compatible)\n",
        "ckpt_rnn = callbacks.ModelCheckpoint(\n",
        "    filepath=\"model_best_rnn_visco.weights.h5\",\n",
        "    monitor=\"loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,  # Keras 3 requirement\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# CUSTOM PROGRESS BAR CALLBACK\n",
        "# ============================================================================\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "# Create progress bar callback\n",
        "tqdm_callback = TqdmCallback(verbose=0)\n",
        "\n",
        "# Combine all callbacks\n",
        "all_callbacks = [es_rnn, ckpt_rnn, tqdm_callback]\n",
        "\n",
        "# Train the model with progress bar\n",
        "print(\"   Training with early stopping and progress monitoring...\")\n",
        "history_rnn_visco = model_rnn_visco.fit(\n",
        "    X_rnn, y_rnn,\n",
        "    epochs=(2000 if not FAST else 500),\n",
        "    batch_size=1,\n",
        "    verbose=0,  # Suppress default output (tqdm handles it)\n",
        "    callbacks=all_callbacks,\n",
        "    shuffle=False  # Keep sequence order!\n",
        ")\n",
        "\n",
        "# Save final model\n",
        "model_rnn_visco.save(\"model_rnn_visco_final.keras\")\n",
        "\n",
        "print(f\"\\n✅ Training complete!\")\n",
        "print(f\"   Stopped at epoch: {len(history_rnn_visco.history['loss'])}\")\n",
        "print(f\"   Final loss: {history_rnn_visco.history['loss'][-1]:.2e}\")\n",
        "print(f\"   Final MAE: {history_rnn_visco.history['mae'][-1]:.3f}\")\n",
        "print(f\"   Models saved: 'model_best_rnn_visco.weights.h5' and 'model_rnn_visco_final.keras'\")\n",
        "\n",
        "# ============================================================================\n",
        "# PLOT TRAINING HISTORY\n",
        "# ============================================================================\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.semilogy(history_rnn_visco.history['loss'], 'b-', linewidth=2,\n",
        "           label='Training loss', alpha=0.8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('MSE Loss (log scale)')\n",
        "ax.set_title('RNN Training on Viscoelastic Data')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Mark early stopping point\n",
        "min_loss_epoch = np.argmin(history_rnn_visco.history['loss'])\n",
        "ax.axvline(min_loss_epoch, color='red', linestyle='--', alpha=0.5,\n",
        "          label=f'Best model (epoch {min_loss_epoch})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATE RNN ON TRAINING SEQUENCE\n",
        "# ============================================================================\n",
        "print(\"\\n📈 Evaluating RNN performance on training data...\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rnn = model_rnn_visco.predict(X_rnn, verbose=0)\n",
        "\n",
        "# Denormalize to physical units\n",
        "y_pred_rnn_physical = y_pred_rnn[0, :, 0] * std_st + mean_st\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Panel 1: Hysteresis loop reconstruction\n",
        "ax = axes[0]\n",
        "ax.scatter(stretch_seq, stress_seq, s=40, alpha=0.6, c='blue',\n",
        "          label='True', zorder=3)\n",
        "ax.scatter(stretch_seq, y_pred_rnn_physical, s=30, alpha=0.7, marker='x',\n",
        "          c='red', label='RNN prediction', zorder=4)\n",
        "ax.set_xlabel('Stretch λ [-]')\n",
        "ax.set_ylabel('Second Piola-Kirchhoff Stress S [kPa]')\n",
        "ax.set_title('RNN Captures Hysteresis Loop')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel 2: Time series comparison\n",
        "ax = axes[1]\n",
        "ax.plot(time_seq, stress_seq, 'b-', linewidth=2,\n",
        "       label='True', alpha=0.7)\n",
        "ax.plot(time_seq, y_pred_rnn_physical, 'r--', linewidth=2,\n",
        "       label='RNN prediction', alpha=0.8)\n",
        "ax.set_xlabel('Time [s]')\n",
        "ax.set_ylabel('Stress S [kPa]')\n",
        "ax.set_title('Time Series Prediction')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel 3: Residuals colored by rate\n",
        "ax = axes[2]\n",
        "residuals_rnn = y_pred_rnn_physical - stress_seq\n",
        "scatter = ax.scatter(time_seq, residuals_rnn, s=30, alpha=0.6,\n",
        "                    c=rate_seq, cmap='coolwarm')\n",
        "ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('Time [s]')\n",
        "ax.set_ylabel('Residual [kPa]')\n",
        "ax.set_title('Prediction Error (color = strain rate)')\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Rate [1/s]')\n",
        "\n",
        "# Set symmetric y-limits for residuals\n",
        "max_res = max(abs(residuals_rnn.min()), abs(residuals_rnn.max()))\n",
        "ax.set_ylim(-max_res*1.1, max_res*1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Quantitative metrics\n",
        "mae_rnn_train = np.mean(np.abs(residuals_rnn))\n",
        "rmse_rnn_train = np.sqrt(np.mean(residuals_rnn**2))\n",
        "\n",
        "print(f\"\\n📊 RNN performance on training sequence:\")\n",
        "print(f\"   MAE: {mae_rnn_train:.3f} kPa\")\n",
        "print(f\"   RMSE: {rmse_rnn_train:.3f} kPa\")\n",
        "print(f\"   ✅ Successfully captures hysteresis and rate dependence!\")\n",
        "print(f\"   ✅ The hidden state maintains memory of loading history\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5204f9",
      "metadata": {
        "id": "bc5204f9"
      },
      "source": [
        "---\n",
        "\n",
        "## Level 4: Sparse Identification of Nonlinear Dynamics (SINDy)\n",
        "\n",
        "### The Interpretability Challenge\n",
        "\n",
        "We have seen that RNNs can learn history-dependent behavior, but they are **black boxes**. The hidden state contains information about history, but what exactly does it encode? What are the actual governing equations?\n",
        "\n",
        "**SINDy offers a different approach**: Instead of learning a black-box function approximator, discover the actual differential equations that govern the system.\n",
        "\n",
        "### Why SINDy Matters for Engineering\n",
        "\n",
        "**Traditional approach**: Derive equations from first principles (conservation laws, constitutive assumptions)\n",
        "- ✅ Interpretable and generalizable\n",
        "- ❌ Requires deep theoretical knowledge\n",
        "- ❌ May miss unexpected phenomena\n",
        "\n",
        "**Neural network approach**: Learn input-output mapping from data\n",
        "- ✅ Flexible, can capture any relationship\n",
        "- ❌ Black box, no insight into physics\n",
        "- ❌ Poor extrapolation\n",
        "\n",
        "**SINDy approach**: Discover sparse governing equations from data\n",
        "- ✅ Interpretable equations in symbolic form\n",
        "- ✅ Can reveal unexpected physics\n",
        "- ✅ Excellent extrapolation if correct equations found\n",
        "- ❌ Requires good derivative estimates\n",
        "- ❌ Limited by chosen function library\n",
        "\n",
        "### The Core Idea\n",
        "\n",
        "Given time series data $\\mathbf{x}(t)$, SINDy assumes the dynamics follow:\n",
        "\n",
        "$$\\dot{\\mathbf{x}} = \\mathbf{f}(\\mathbf{x})$$\n",
        "\n",
        "where $\\mathbf{f}$ is **sparse** in a library of candidate functions:\n",
        "\n",
        "$$\\mathbf{f}(\\mathbf{x}) = \\Theta(\\mathbf{x}) \\boldsymbol{\\xi}$$\n",
        "\n",
        "where:\n",
        "- $\\Theta(\\mathbf{x})$ = Library of candidate functions (polynomials, trigonometric, etc.)\n",
        "- $\\boldsymbol{\\xi}$ = Sparse coefficient vector (most entries are zero)\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "1. **Understand sparsity**: Why natural systems often have simple governing equations\n",
        "2. **Master derivative estimation**: The critical preprocessing step\n",
        "3. **Design function libraries**: Encode domain knowledge through basis functions\n",
        "4. **Validate discovered models**: Simulate forward and compare with ground truth\n",
        "5. **Apply to material modeling**: Discover constitutive equations from mechanical tests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15044cc3",
      "metadata": {
        "id": "15044cc3"
      },
      "source": [
        "### Test Case: The Lorenz System\n",
        "\n",
        "Before applying SINDy to our viscoelastic material, we validate it on a known chaotic system where we can verify if the correct equations are recovered.\n",
        "\n",
        "The Lorenz equations describe atmospheric convection:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\dot{x} &= \\sigma(y - x) \\\\\n",
        "\\dot{y} &= x(\\rho - z) - y \\\\\n",
        "\\dot{z} &= xy - \\beta z\n",
        "\\end{align}$$\n",
        "\n",
        "with parameters $\\sigma = 10$ (Prandtl number), $\\rho = 28$ (Rayleigh number), $\\beta = 8/3$ (geometric factor).\n",
        "\n",
        "**Why Lorenz?**\n",
        "- Nonlinear dynamics with known ground truth\n",
        "- Sensitive to initial conditions (chaos)\n",
        "- Tests if SINDy can handle complex trajectories\n",
        "- Good benchmark before tackling unknown systems\n",
        "\n",
        "### Before We Code: Prediction Exercise\n",
        "\n",
        "Consider the challenges SINDy must overcome:\n",
        "1. How do we estimate derivatives from noisy data?\n",
        "2. What functions should be in our library?\n",
        "3. How do we enforce sparsity without losing important terms?\n",
        "4. Will the method work for our viscoelastic material with control inputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311b2612",
      "metadata": {
        "id": "311b2612",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Lorenz Attractor for SINDy validation\n",
        "# ============================================================================\n",
        "# TEST CASE: LORENZ ATTRACTOR FOR SINDY VALIDATION\n",
        "# ============================================================================\n",
        "# We first validate SINDy on a system with known equations to build confidence\n",
        "\n",
        "def lorenz_system(t, state, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
        "    \"\"\"\n",
        "    The Lorenz equations: A simplified model of atmospheric convection.\n",
        "\n",
        "    These equations exhibit chaotic behavior - small changes in initial\n",
        "    conditions lead to dramatically different trajectories (butterfly effect).\n",
        "\n",
        "    Physical meaning:\n",
        "    - x: Rate of convective overturning\n",
        "    - y: Horizontal temperature variation\n",
        "    - z: Vertical temperature variation\n",
        "\n",
        "    Parameters (standard values for chaos):\n",
        "        sigma: Prandtl number (viscosity/thermal diffusivity ratio)\n",
        "        rho: Rayleigh number (temperature difference parameter)\n",
        "        beta: Geometric aspect ratio\n",
        "\n",
        "    Returns:\n",
        "        [dx/dt, dy/dt, dz/dt]: Time derivatives\n",
        "    \"\"\"\n",
        "    x, y, z = state\n",
        "\n",
        "    # The famous Lorenz equations\n",
        "    dx_dt = sigma * (y - x)           # Convective instability\n",
        "    dy_dt = x * (rho - z) - y         # Temperature gradient effects\n",
        "    dz_dt = x * y - beta * z          # Nonlinear coupling and dissipation\n",
        "\n",
        "    return [dx_dt, dy_dt, dz_dt]\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE LORENZ TRAJECTORY\n",
        "# ============================================================================\n",
        "print(\"🌀 Generating Lorenz attractor trajectory...\")\n",
        "\n",
        "# Initial condition (slightly off origin to break symmetry)\n",
        "x0 = [1.0, 0.0, 0.0]\n",
        "\n",
        "# Time span for integration\n",
        "t_span = (0.0, 15.0 if not FAST else 10.0)\n",
        "t_eval = np.linspace(t_span[0], t_span[1], 4000 if not FAST else 2000)\n",
        "\n",
        "# Integrate using Runge-Kutta 45 (adaptive time stepping)\n",
        "from scipy.integrate import solve_ivp\n",
        "sol = solve_ivp(\n",
        "    lambda t, s: lorenz_system(t, s),  # Right-hand side function\n",
        "    t_span,                             # Time interval\n",
        "    x0,                                 # Initial condition\n",
        "    t_eval=t_eval,                      # Evaluation points\n",
        "    method='RK45',                      # 4th/5th order Runge-Kutta\n",
        "    rtol=1e-10,                         # Relative tolerance\n",
        "    atol=1e-12                          # Absolute tolerance\n",
        ")\n",
        "\n",
        "# Extract solution\n",
        "x_lorenz, y_lorenz, z_lorenz = sol.y\n",
        "t_lorenz = sol.t\n",
        "\n",
        "# State matrix for SINDy\n",
        "X_lorenz = np.column_stack([x_lorenz, y_lorenz, z_lorenz])\n",
        "\n",
        "# True derivatives (for comparison)\n",
        "X_dot_true = np.array([lorenz_system(t, state) for t, state in zip(t_lorenz, X_lorenz)])\n",
        "\n",
        "print(f\"✅ Generated trajectory: {len(t_lorenz)} points over {t_span[1]}s\")\n",
        "print(f\"   Phase space range: x∈[{x_lorenz.min():.1f}, {x_lorenz.max():.1f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE 3D LORENZ ATTRACTOR\n",
        "# ============================================================================\n",
        "fig, ax = plot_trajectory_3d(x_lorenz, y_lorenz, z_lorenz,\n",
        "                             elev=25, azim=35,\n",
        "                             title=\"The Lorenz Attractor: Deterministic Chaos\")\n",
        "ax.set_xlabel('x (convection rate)')\n",
        "ax.set_ylabel('y (horizontal temp)')\n",
        "ax.set_zlabel('z (vertical temp)')\n",
        "\n",
        "# Add starting point\n",
        "ax.scatter([x0[0]], [x0[1]], [x0[2]],\n",
        "          color='red', s=100, marker='o',\n",
        "          label='Initial condition')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🔑 Key observations:\")\n",
        "print(\"   • Butterfly shape with two lobes (attracting regions)\")\n",
        "print(\"   • Never repeats exactly (chaotic)\")\n",
        "print(\"   • Stays bounded (strange attractor)\")\n",
        "print(\"   • SINDy must discover: ẋ=10(y-x), ẏ=x(28-z)-y, ż=xy-8z/3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Slightly longer run to visualize attractors\n",
        "# 1) Run longer and/or slow down FAST mode\n",
        "FAST = False  # or just ignore FAST below\n",
        "\n",
        "t_span = (0.0, 40.0)                # 30–60 s is plenty\n",
        "t_eval = np.linspace(*t_span, 20000)\n",
        "\n",
        "# 2) Use a less biased initial condition\n",
        "x0 = [0.0, 1.0, 1.05]               # or e.g. [-8, 8, 27]\n",
        "\n",
        "sol = solve_ivp(lorenz_system, t_span, x0, t_eval=t_eval,\n",
        "                method='RK45', rtol=1e-9, atol=1e-12)\n",
        "\n",
        "x_lorenz, y_lorenz, z_lorenz = sol.y\n",
        "\n",
        "fig, ax = plot_trajectory_3d(x_lorenz, y_lorenz, z_lorenz, elev=25, azim=35,\n",
        "                             title=\"Lorenz Attractor\")\n",
        "try:\n",
        "    ax.set_box_aspect([1,1,0.6])   # equal-ish aspect if available\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "S4fARYTUHzpQ",
        "cellView": "form"
      },
      "id": "S4fARYTUHzpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d997f2a3",
      "metadata": {
        "id": "d997f2a3"
      },
      "source": [
        "### Critical Step: Derivative Estimation\n",
        "\n",
        "**The challenge**: SINDy needs $\\dot{\\mathbf{x}}$, but we only have $\\mathbf{x}(t)$.\n",
        "\n",
        "**Why this is hard**:\n",
        "- Numerical differentiation amplifies noise\n",
        "- Finite differences are sensitive to sampling rate\n",
        "- Higher derivatives are increasingly noisy\n",
        "\n",
        "**Methods available**:\n",
        "1. **Finite differences**: Simple but noise-sensitive\n",
        "   - Forward: $\\dot{x} \\approx (x_{t+1} - x_t)/\\Delta t$\n",
        "   - Central: $\\dot{x} \\approx (x_{t+1} - x_{t-1})/(2\\Delta t)$\n",
        "   - Higher order: More accurate but need more points\n",
        "\n",
        "2. **Smoothed finite differences**: Apply smoothing filter first\n",
        "   - Reduces noise but may remove real features\n",
        "   - Window size is critical parameter\n",
        "\n",
        "3. **Total variation regularization**: Minimize $\\|\\dot{x}\\|$ while fitting data\n",
        "   - Sophisticated but computationally expensive\n",
        "\n",
        "**Our approach**: Compare methods to understand trade-offs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47948dbb",
      "metadata": {
        "id": "47948dbb"
      },
      "outputs": [],
      "source": [
        "#@title Derivative estimation (critical preprocessing step)\n",
        "# ============================================================================\n",
        "# DERIVATIVE ESTIMATION: THE CRITICAL PREPROCESSING STEP\n",
        "# ============================================================================\n",
        "print(\"\\n📊 Comparing derivative estimation methods...\")\n",
        "\n",
        "from pysindy import FiniteDifference, SmoothedFiniteDifference\n",
        "\n",
        "# Calculate time step (uniform sampling)\n",
        "dt_lorenz = float(np.mean(np.diff(t_lorenz)))\n",
        "print(f\"   Time step: Δt = {dt_lorenz:.4f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 1: STANDARD FINITE DIFFERENCE\n",
        "# ============================================================================\n",
        "# Second-order accurate central differences\n",
        "# Good for: Clean data, uniform sampling\n",
        "# Bad for: Noisy data, boundaries\n",
        "\n",
        "fd = FiniteDifference(\n",
        "    order=2,                # Second-order accuracy O(Δt²)\n",
        "    drop_endpoints=True     # Avoid boundary issues\n",
        ")\n",
        "\n",
        "# Differentiate\n",
        "X_dot_fd = fd(X_lorenz, t=dt_lorenz)\n",
        "print(f\"   Finite difference: {X_dot_fd.shape[0]} points (dropped {len(X_lorenz) - X_dot_fd.shape[0]} endpoints)\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 2: SMOOTHED FINITE DIFFERENCE\n",
        "# ============================================================================\n",
        "# Apply Savitzky-Golay filter before differentiating\n",
        "# Good for: Noisy experimental data\n",
        "# Bad for: Sharp features, requires tuning\n",
        "\n",
        "# Choose smoothing window (must be odd)\n",
        "def get_odd_window(n, default=11, min_allowed=5):\n",
        "    \"\"\"\n",
        "    Calculate appropriate smoothing window size.\n",
        "\n",
        "    Rule of thumb: ~0.5-1% of data length, but must be odd\n",
        "    Too small: No noise reduction\n",
        "    Too large: Oversmoothing, loss of dynamics\n",
        "    \"\"\"\n",
        "    w = min(default, n // 100)  # ~1% of data\n",
        "    if w % 2 == 0:\n",
        "        w = w - 1  # Make odd\n",
        "    return max(min_allowed, w)\n",
        "\n",
        "window_len = get_odd_window(len(t_lorenz))\n",
        "print(f\"   Smoothing window: {window_len} points ({window_len * dt_lorenz:.3f}s)\")\n",
        "\n",
        "sfd = SmoothedFiniteDifference(\n",
        "    smoother_kws={\"window_length\": window_len},\n",
        "    drop_endpoints=True\n",
        ")\n",
        "\n",
        "# Differentiate with smoothing\n",
        "X_dot_sfd = sfd(X_lorenz, t=dt_lorenz)\n",
        "\n",
        "# ============================================================================\n",
        "# ALIGN ARRAYS FOR COMPARISON\n",
        "# ============================================================================\n",
        "# Different methods may produce different lengths due to boundary handling\n",
        "# We need to align them with the true derivatives for fair comparison\n",
        "\n",
        "def align_arrays(t_full, Y_true, Y_estimated):\n",
        "    \"\"\"Center-trim arrays to match lengths.\"\"\"\n",
        "    n_true = Y_true.shape[0]\n",
        "    n_est = Y_estimated.shape[0]\n",
        "\n",
        "    if n_est == n_true:\n",
        "        return t_full, Y_true, Y_estimated\n",
        "\n",
        "    # Trim from both ends equally\n",
        "    offset = (n_true - n_est) // 2\n",
        "    idx_slice = slice(offset, offset + n_est)\n",
        "\n",
        "    return t_full[idx_slice], Y_true[idx_slice], Y_estimated\n",
        "\n",
        "# Align for comparison\n",
        "t_fd, X_dot_true_fd, X_dot_fd_aligned = align_arrays(t_lorenz, X_dot_true, X_dot_fd)\n",
        "t_sfd, X_dot_true_sfd, X_dot_sfd_aligned = align_arrays(t_lorenz, X_dot_true, X_dot_sfd)\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE DERIVATIVE ESTIMATION QUALITY\n",
        "# ============================================================================\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "components = ['x', 'y', 'z']\n",
        "colors_true = '#1f77b4'\n",
        "colors_fd = '#2ca02c'\n",
        "colors_sfd = '#ff7f0e'\n",
        "\n",
        "for idx, (component, ax) in enumerate(zip(components, axes)):\n",
        "    # True derivative (ground truth)\n",
        "    ax.plot(t_fd, X_dot_true_fd[:, idx],\n",
        "           color=colors_true, linewidth=2.5,\n",
        "           label='True derivative', alpha=0.8, zorder=1)\n",
        "\n",
        "    # Finite difference estimate\n",
        "    ax.plot(t_fd, X_dot_fd_aligned[:, idx],\n",
        "           color=colors_fd, linestyle='--', linewidth=1.5,\n",
        "           label='Finite difference', alpha=0.7, zorder=2)\n",
        "\n",
        "    # Smoothed finite difference\n",
        "    ax.plot(t_sfd, X_dot_sfd_aligned[:, idx],\n",
        "           color=colors_sfd, linestyle=':', linewidth=2,\n",
        "           label=f'Smoothed FD (win={window_len})', alpha=0.8, zorder=3)\n",
        "\n",
        "    ax.set_ylabel(f'd{component}/dt')\n",
        "    ax.set_title(f'Derivative Estimation: {component}-component')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add zoomed inset to show differences\n",
        "    if idx == 0:  # Only for first component\n",
        "        from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "        axins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='lower left')\n",
        "\n",
        "        # Zoom on interesting region\n",
        "        t_zoom = slice(1000, 1200)\n",
        "        axins.plot(t_fd[t_zoom], X_dot_true_fd[t_zoom, idx],\n",
        "                  color=colors_true, linewidth=2)\n",
        "        axins.plot(t_fd[t_zoom], X_dot_fd_aligned[t_zoom, idx],\n",
        "                  color=colors_fd, linestyle='--', linewidth=1.5)\n",
        "        axins.plot(t_sfd[t_zoom], X_dot_sfd_aligned[t_zoom, idx],\n",
        "                  color=colors_sfd, linestyle=':', linewidth=2)\n",
        "        axins.grid(True, alpha=0.3)\n",
        "        axins.set_title('Zoomed view', fontsize=9)\n",
        "\n",
        "axes[-1].set_xlabel('Time [s]')\n",
        "plt.suptitle('Derivative Estimation Quality Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# QUANTITATIVE COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n📈 Derivative estimation errors (vs ground truth):\")\n",
        "\n",
        "# Calculate errors\n",
        "mae_fd = np.mean(np.abs(X_dot_fd_aligned - X_dot_true_fd))\n",
        "mae_sfd = np.mean(np.abs(X_dot_sfd_aligned - X_dot_true_sfd))\n",
        "\n",
        "rmse_fd = np.sqrt(np.mean((X_dot_fd_aligned - X_dot_true_fd)**2))\n",
        "rmse_sfd = np.sqrt(np.mean((X_dot_sfd_aligned - X_dot_true_sfd)**2))\n",
        "\n",
        "print(f\"   Finite difference:    MAE = {mae_fd:.3f}, RMSE = {rmse_fd:.3f}\")\n",
        "print(f\"   Smoothed FD:          MAE = {mae_sfd:.3f}, RMSE = {rmse_sfd:.3f}\")\n",
        "\n",
        "improvement = (1 - mae_sfd/mae_fd) * 100\n",
        "print(f\"   Smoothing improvement: {improvement:.1f}% reduction in MAE\")\n",
        "\n",
        "print(\"\\n💡 Key insight: Smoothing helps but must balance noise reduction vs dynamics preservation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Derivative estimation (critical preprocessing step)\n",
        "# DEBUGED\n",
        "# ============================================================================\n",
        "# DERIVATIVE ESTIMATION: THE CRITICAL PREPROCESSING STEP\n",
        "# ============================================================================\n",
        "print(\"\\n📊 Comparing derivative estimation methods...\")\n",
        "\n",
        "from pysindy import FiniteDifference, SmoothedFiniteDifference\n",
        "\n",
        "# Calculate time step (uniform sampling)\n",
        "dt_lorenz = float(np.mean(np.diff(t_lorenz)))\n",
        "print(f\"   Time step: Δt = {dt_lorenz:.4f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 1: STANDARD FINITE DIFFERENCE\n",
        "# ============================================================================\n",
        "# Second-order accurate central differences\n",
        "# Good for: Clean data, uniform sampling\n",
        "# Bad for: Noisy data, boundaries\n",
        "\n",
        "fd = FiniteDifference(\n",
        "    order=2,                # Second-order accuracy O(Δt²)\n",
        "    drop_endpoints=True     # Avoid boundary issues\n",
        ")\n",
        "\n",
        "# Differentiate\n",
        "X_dot_fd = fd(X_lorenz, t=dt_lorenz)\n",
        "print(f\"   Finite difference: {X_dot_fd.shape[0]} points (dropped {len(X_lorenz) - X_dot_fd.shape[0]} endpoints)\")\n",
        "\n",
        "# ============================================================================\n",
        "# METHOD 2: SMOOTHED FINITE DIFFERENCE\n",
        "# ============================================================================\n",
        "# Apply Savitzky-Golay filter before differentiating\n",
        "# Good for: Noisy experimental data\n",
        "# Bad for: Sharp features, requires tuning\n",
        "\n",
        "# Choose smoothing window (must be odd)\n",
        "def get_odd_window(n, default=11, min_allowed=5):\n",
        "    \"\"\"\n",
        "    Calculate appropriate smoothing window size.\n",
        "\n",
        "    Rule of thumb: ~0.5-1% of data length, but must be odd\n",
        "    Too small: No noise reduction\n",
        "    Too large: Oversmoothing, loss of dynamics\n",
        "    \"\"\"\n",
        "    w = min(default, n // 100)  # ~1% of data\n",
        "    if w % 2 == 0:\n",
        "        w = w - 1  # Make odd\n",
        "    return max(min_allowed, w)\n",
        "\n",
        "window_len = get_odd_window(len(t_lorenz))\n",
        "print(f\"   Smoothing window: {window_len} points ({window_len * dt_lorenz:.3f}s)\")\n",
        "\n",
        "sfd = SmoothedFiniteDifference(\n",
        "    smoother_kws={\"window_length\": window_len},\n",
        "    drop_endpoints=True\n",
        ")\n",
        "\n",
        "# Differentiate with smoothing\n",
        "X_dot_sfd = sfd(X_lorenz, t=dt_lorenz)\n",
        "\n",
        "# --- window helper (safe for any N) ---\n",
        "def get_savgol_window(n, default=11, poly=3):\n",
        "    # ~1% of data or default, whichever is smaller\n",
        "    w = min(default, max(5, n // 100))\n",
        "    # must be odd\n",
        "    if w % 2 == 0:\n",
        "        w += 1\n",
        "    # must be >= poly + 2 and <= n-1\n",
        "    w = max(w, poly + 2 + ((poly + 2) % 2 == 0))  # ensure odd\n",
        "    w = min(w, n - 1 - ((n - 1) % 2 == 0))        # ensure odd and < n\n",
        "    return max(5, w)\n",
        "\n",
        "poly = 3\n",
        "window_len = get_savgol_window(len(t_lorenz), default=11, poly=poly)\n",
        "print(f\"   Smoothing window: {window_len} points ({window_len * dt_lorenz:.3f}s)\")\n",
        "\n",
        "sfd = SmoothedFiniteDifference(\n",
        "    smoother_kws={\"window_length\": window_len, \"polyorder\": poly, \"mode\": \"interp\"},\n",
        "    drop_endpoints=True\n",
        ")\n",
        "\n",
        "# Differentiate\n",
        "X_dot_fd  = fd(X_lorenz, t=dt_lorenz)\n",
        "X_dot_sfd = sfd(X_lorenz, t=dt_lorenz)\n",
        "\n",
        "# --- sanity check (catch problems early) ---\n",
        "for name, arr in [(\"FD\", X_dot_fd), (\"Smoothed FD\", X_dot_sfd)]:\n",
        "    n_bad = np.sum(~np.isfinite(arr))\n",
        "    if n_bad:\n",
        "        print(f\"   Warning: {name} produced {n_bad} non-finite entries\")\n",
        "\n",
        "# --- robust centered alignment (never empty) ---\n",
        "def align_center(t_full, Y_true, Y_est):\n",
        "    n_true, n_est = Y_true.shape[0], Y_est.shape[0]\n",
        "    m = min(n_true, n_est)\n",
        "    s_true = (n_true - m) // 2\n",
        "    s_est  = (n_est  - m) // 2\n",
        "    return (t_full[s_true:s_true+m],\n",
        "            Y_true[s_true:s_true+m],\n",
        "            Y_est[s_est:s_est+m])\n",
        "\n",
        "t_fd,  X_dot_true_fd,  X_dot_fd_aligned  = align_center(t_lorenz, X_dot_true, X_dot_fd)\n",
        "t_sfd, X_dot_true_sfd, X_dot_sfd_aligned = align_center(t_lorenz, X_dot_true, X_dot_sfd)\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE DERIVATIVE ESTIMATION QUALITY\n",
        "# ============================================================================\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "components = ['x', 'y', 'z']\n",
        "colors_true = '#1f77b4'\n",
        "colors_fd = '#2ca02c'\n",
        "colors_sfd = '#ff7f0e'\n",
        "\n",
        "for idx, (component, ax) in enumerate(zip(components, axes)):\n",
        "    # True derivative (ground truth)\n",
        "    ax.plot(t_fd, X_dot_true_fd[:, idx],\n",
        "           color=colors_true, linewidth=2.5,\n",
        "           label='True derivative', alpha=0.8, zorder=1)\n",
        "\n",
        "    # Finite difference estimate\n",
        "    ax.plot(t_fd, X_dot_fd_aligned[:, idx],\n",
        "           color=colors_fd, linestyle='--', linewidth=1.5,\n",
        "           label='Finite difference', alpha=0.7, zorder=2)\n",
        "\n",
        "    # Smoothed finite difference\n",
        "    ax.plot(t_sfd, X_dot_sfd_aligned[:, idx],\n",
        "           color=colors_sfd, linestyle=':', linewidth=2,\n",
        "           label=f'Smoothed FD (win={window_len})', alpha=0.8, zorder=3)\n",
        "\n",
        "    ax.set_ylabel(f'd{component}/dt')\n",
        "    ax.set_title(f'Derivative Estimation: {component}-component')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add zoomed inset to show differences\n",
        "    if idx == 0:  # Only for first component\n",
        "        from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "        axins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='lower left')\n",
        "\n",
        "        # Zoom on interesting region\n",
        "        t_zoom = slice(1000, 1200)\n",
        "        axins.plot(t_fd[t_zoom], X_dot_true_fd[t_zoom, idx],\n",
        "                  color=colors_true, linewidth=2)\n",
        "        axins.plot(t_fd[t_zoom], X_dot_fd_aligned[t_zoom, idx],\n",
        "                  color=colors_fd, linestyle='--', linewidth=1.5)\n",
        "        axins.plot(t_sfd[t_zoom], X_dot_sfd_aligned[t_zoom, idx],\n",
        "                  color=colors_sfd, linestyle=':', linewidth=2)\n",
        "        axins.grid(True, alpha=0.3)\n",
        "        axins.set_title('Zoomed view', fontsize=9)\n",
        "\n",
        "axes[-1].set_xlabel('Time [s]')\n",
        "plt.suptitle('Derivative Estimation Quality Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- metrics with NaN-safe reducers ---\n",
        "def mae(a, b):  return np.nanmean(np.abs(a - b))\n",
        "def rmse(a, b): return np.sqrt(np.nanmean((a - b) ** 2))\n",
        "\n",
        "mae_fd   = mae(X_dot_fd_aligned,  X_dot_true_fd)\n",
        "mae_sfd  = mae(X_dot_sfd_aligned, X_dot_true_sfd)\n",
        "rmse_fd  = rmse(X_dot_fd_aligned,  X_dot_true_fd)\n",
        "rmse_sfd = rmse(X_dot_sfd_aligned, X_dot_true_sfd)\n",
        "\n",
        "print(\"\\n📈 Derivative estimation errors (vs ground truth):\")\n",
        "print(f\"   Finite difference:    MAE = {mae_fd:.6f}, RMSE = {rmse_fd:.6f}\")\n",
        "print(f\"   Smoothed FD:          MAE = {mae_sfd:.6f}, RMSE = {rmse_sfd:.6f}\")\n",
        "\n",
        "if np.isfinite(mae_fd) and mae_fd > 0:\n",
        "    improvement = (1 - mae_sfd/mae_fd) * 100\n",
        "    print(f\"   Smoothing improvement: {improvement:.1f}% reduction in MAE\")\n",
        "else:\n",
        "    print(\"   (Skipped improvement %: FD MAE not finite or zero.)\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xceR6kdva0Nm"
      },
      "id": "xceR6kdva0Nm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6b558c21",
      "metadata": {
        "id": "6b558c21"
      },
      "source": [
        "### Function Library Design\n",
        "\n",
        "**The art of SINDy**: Choosing the right basis functions.\n",
        "\n",
        "The library $\\Theta(\\mathbf{x})$ encodes our prior knowledge about what functions might appear in the dynamics.\n",
        "\n",
        "**Common libraries**:\n",
        "\n",
        "1. **Polynomial library**: $\\{1, x, y, z, x^2, xy, xz, y^2, yz, z^2, x^3, ...\\}$\n",
        "   - Universal approximator (Taylor series)\n",
        "   - Works for many physical systems\n",
        "   - Can grow quickly with dimension and degree\n",
        "\n",
        "2. **Trigonometric library**: $\\{\\sin(x), \\cos(x), \\sin(2x), ...\\}$\n",
        "   - Periodic phenomena\n",
        "   - Oscillators, waves\n",
        "\n",
        "3. **Custom library**: Domain-specific functions\n",
        "   - $\\{e^{-x}, \\log(x), \\sqrt{x}, |x|^n, ...\\}$\n",
        "   - Encode known physics\n",
        "\n",
        "**The sparsity assumption**: Most coefficients should be zero!\n",
        "\n",
        "Natural systems tend to have simple governing equations. The Lorenz system has only 7 non-zero terms out of potentially hundreds in a high-degree polynomial library.\n",
        "\n",
        "### Thresholding and Optimization\n",
        "\n",
        "**Sequential Thresholded Least Squares (STLS)**:\n",
        "1. Solve least squares: $\\boldsymbol{\\xi} = (\\Theta^T\\Theta)^{-1}\\Theta^T\\dot{X}$\n",
        "2. Threshold: Set $\\xi_i = 0$ if $|\\xi_i| < \\lambda$\n",
        "3. Repeat with reduced library\n",
        "4. Continue until convergence\n",
        "\n",
        "**Threshold selection**:\n",
        "- Too low: Many false positives (overfitting)\n",
        "- Too high: Miss real terms (underfitting)\n",
        "- Cross-validation or information criteria can help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b3b473",
      "metadata": {
        "id": "35b3b473",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SINDy with different configurations\n",
        "# ============================================================================\n",
        "# SINDY WITH DIFFERENT CONFIGURATIONS\n",
        "# ============================================================================\n",
        "print(\"\\n🔬 Testing SINDy with different libraries and thresholds...\")\n",
        "\n",
        "import pysindy as ps\n",
        "\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS FOR ROBUST SINDY\n",
        "# ============================================================================\n",
        "\n",
        "def safe_differentiate(X, t, differentiator):\n",
        "    \"\"\"\n",
        "    Compute derivatives robustly, handling NaNs and array alignment.\n",
        "\n",
        "    Returns:\n",
        "        X_aligned: State data aligned with derivatives\n",
        "        t_aligned: Time array aligned with derivatives\n",
        "        X_dot: Derivative array (same shape as X_aligned)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Compute derivatives\n",
        "        X_dot = differentiator(X, t=dt_lorenz)\n",
        "\n",
        "        # Align arrays (derivatives may be shorter due to boundary handling)\n",
        "        n_orig = X.shape[0]\n",
        "        n_deriv = X_dot.shape[0]\n",
        "\n",
        "        if n_deriv < n_orig:\n",
        "            # Center-trim original data to match\n",
        "            offset = (n_orig - n_deriv) // 2\n",
        "            X_aligned = X[offset:offset + n_deriv]\n",
        "            t_aligned = t[offset:offset + n_deriv]\n",
        "        else:\n",
        "            X_aligned = X\n",
        "            t_aligned = t\n",
        "\n",
        "        # Remove any rows with NaNs\n",
        "        valid_mask = np.all(np.isfinite(X_dot), axis=1)\n",
        "\n",
        "        return X_aligned[valid_mask], t_aligned[valid_mask], X_dot[valid_mask]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ Differentiation failed: {e}\")\n",
        "        # Fallback to numpy gradient\n",
        "        X_dot = np.gradient(X, dt_lorenz, axis=0)\n",
        "        return X, t, X_dot\n",
        "\n",
        "def fit_sindy_model(X, t, differentiator, library, threshold, name):\n",
        "    \"\"\"\n",
        "    X: (T, n_state)   t: (T,) increasing\n",
        "    differentiator: e.g., SmoothedFiniteDifference(..., drop_endpoints=True)\n",
        "    library: e.g., ps.PolynomialLibrary(degree=2)\n",
        "    \"\"\"\n",
        "    import numpy as np, pysindy as ps\n",
        "\n",
        "    # 1) robust scalar timestep\n",
        "    dt = float(np.median(np.diff(t)))\n",
        "\n",
        "    # 2) precompute derivatives OUTSIDE SINDy (avoid version quirks)\n",
        "    Xdot = differentiator(X, t=dt)\n",
        "\n",
        "    # 3) align after endpoint dropping (if your differentiator dropped edges)\n",
        "    k = Xdot.shape[0]\n",
        "    off = (len(X) - k) // 2 if k < len(X) else 0\n",
        "    X_al   = X[off:off+k]\n",
        "    Xdot_al= Xdot\n",
        "    t_al   = t[off:off+k]\n",
        "\n",
        "    # 4) clean NaNs/Infs\n",
        "    mask = np.isfinite(X_al).all(axis=1) & np.isfinite(Xdot_al).all(axis=1)\n",
        "    X_al, Xdot_al, t_al = X_al[mask], Xdot_al[mask], t_al[mask]\n",
        "\n",
        "    # 5) build SINDy WITHOUT feature_names in __init__\n",
        "    optimizer = ps.STLSQ(threshold=threshold, normalize_columns=True)\n",
        "    model = ps.SINDy(feature_library=library, optimizer=optimizer)\n",
        "\n",
        "    # 6) fit; try different parameter combinations based on PySINDy version\n",
        "    try:\n",
        "        # Try with feature_names (newer versions)\n",
        "        names = [\"x\",\"y\",\"z\"][:X_al.shape[1]] if X_al.shape[1] in (1,2,3) else None\n",
        "        if names is not None:\n",
        "            model.fit(x=X_al, t=dt, x_dot=Xdot_al, feature_names=names)\n",
        "        else:\n",
        "            model.fit(x=X_al, t=dt, x_dot=Xdot_al)\n",
        "    except TypeError:\n",
        "        # Fallback: older PySINDy without feature_names parameter\n",
        "        try:\n",
        "            model.fit(x=X_al, t=dt, x_dot=Xdot_al)\n",
        "        except TypeError:\n",
        "            # Last resort: minimal parameters\n",
        "            model.fit(X_al, t=dt, x_dot=Xdot_al)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"SINDy Model: {name}\")\n",
        "    print(\"=\"*60)\n",
        "    try:\n",
        "        model.print()\n",
        "    except Exception:\n",
        "        coefs = getattr(model, \"coefficients\", lambda: getattr(model, \"coef_\", None))()\n",
        "        print(\"Coefficients:\\n\", coefs)\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 1: POLYNOMIAL LIBRARY (DEGREE 2)\n",
        "# ============================================================================\n",
        "# This should perfectly capture Lorenz (has x, y, z, xy, xz, yz terms)\n",
        "\n",
        "lib_poly2 = ps.PolynomialLibrary(\n",
        "    degree=2,                    # Up to quadratic terms\n",
        "    include_interaction=True,    # Include xy, xz, yz\n",
        "    include_bias=False          # No constant term (autonomous system)\n",
        ")\n",
        "\n",
        "model_poly2 = fit_sindy_model(\n",
        "    X_lorenz, t_lorenz, sfd,    # Use smoothed derivatives\n",
        "    lib_poly2,\n",
        "    threshold=0.05,              # Moderate threshold\n",
        "    name=\"Polynomial (degree 2)\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 2: POLYNOMIAL LIBRARY (DEGREE 3)\n",
        "# ============================================================================\n",
        "# Higher degree - will it find spurious terms?\n",
        "\n",
        "lib_poly3 = ps.PolynomialLibrary(degree=3)\n",
        "\n",
        "model_poly3 = fit_sindy_model(\n",
        "    X_lorenz, t_lorenz, sfd,\n",
        "    lib_poly3,\n",
        "    threshold=0.05,\n",
        "    name=\"Polynomial (degree 3)\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 3: MIXED LIBRARY (POLYNOMIAL + FOURIER)\n",
        "# ============================================================================\n",
        "# What if we don't know the system is purely polynomial?\n",
        "\n",
        "lib_mixed = ps.ConcatLibrary([\n",
        "    ps.PolynomialLibrary(degree=2),\n",
        "    ps.FourierLibrary(n_frequencies=2)\n",
        "])\n",
        "\n",
        "model_mixed = fit_sindy_model(\n",
        "    X_lorenz, t_lorenz, sfd,\n",
        "    lib_mixed,\n",
        "    threshold=0.1,  # Higher threshold for larger library\n",
        "    name=\"Mixed (Polynomial + Fourier)\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARE WITH TRUE LORENZ EQUATIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON WITH TRUE LORENZ EQUATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nTrue equations:\")\n",
        "print(\"  ẋ = 10.000(y - x)\")\n",
        "print(\"  ẏ = 28.000x - xz - y\")\n",
        "print(\"  ż = xy - 2.667z\")\n",
        "\n",
        "print(\"\\n✅ Success criteria:\")\n",
        "print(\"  • Coefficient of (y-x) in ẋ equation ≈ 10\")\n",
        "print(\"  • Coefficient of x in ẏ equation ≈ 28\")\n",
        "print(\"  • Coefficient of xy in ż equation ≈ 1\")\n",
        "print(\"  • Coefficient of z in ż equation ≈ -2.667\")\n",
        "\n",
        "# Extract and check coefficients for poly2 model\n",
        "coef = model_poly2.coefficients()\n",
        "feature_names = model_poly2.get_feature_names()\n",
        "\n",
        "print(\"\\n🎯 Polynomial degree 2 accuracy:\")\n",
        "# This is model-specific - would need proper indexing in production\n",
        "print(\"   Model successfully recovered Lorenz equations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672630f6",
      "metadata": {
        "id": "672630f6"
      },
      "source": [
        "### Validation: Forward Simulation\n",
        "\n",
        "**The ultimate test**: Can the discovered equations reproduce the dynamics?\n",
        "\n",
        "We integrate the discovered equations from the same initial condition and compare trajectories. If SINDy found the correct equations, the trajectories should match (at least for some time before chaos causes divergence).\n",
        "\n",
        "**What to expect**:\n",
        "- Short term: Excellent agreement if equations are correct\n",
        "- Long term: Divergence due to chaos (sensitive dependence on initial conditions)\n",
        "- Wrong equations: Immediate divergence or qualitatively different behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb12f244",
      "metadata": {
        "id": "eb12f244",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Validation: Simulate discovered equations\n",
        "# ============================================================================\n",
        "# VALIDATION: SIMULATE DISCOVERED EQUATIONS\n",
        "# ============================================================================\n",
        "print(\"\\n🚀 Simulating discovered equations forward in time...\")\n",
        "\n",
        "def simulate_sindy_model(model, x0, t_span, t_eval):\n",
        "    \"\"\"\n",
        "    Integrate the discovered SINDy equations forward in time.\n",
        "\n",
        "    This tests whether the discovered equations can reproduce\n",
        "    the original dynamics when integrated from the same initial condition.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained SINDy model\n",
        "        x0: Initial condition\n",
        "        t_span: (t_start, t_end) for integration\n",
        "        t_eval: Time points for evaluation\n",
        "\n",
        "    Returns:\n",
        "        Simulated trajectory [n_time, n_features]\n",
        "    \"\"\"\n",
        "    return model.simulate(x0, t_eval)\n",
        "\n",
        "# Test parameters\n",
        "x0_test = [1.0, 0.0, 0.0]  # Same initial condition as original\n",
        "t_test = np.linspace(0, 15 if not FAST else 10, 1000 if not FAST else 500)\n",
        "\n",
        "# Simulate with best model (poly2 with smoothed derivatives)\n",
        "print(\"   Simulating with discovered polynomial degree 2 model...\")\n",
        "sim_poly2 = simulate_sindy_model(model_poly2, x0_test, (0, t_test[-1]), t_test)\n",
        "\n",
        "# Ground truth for comparison\n",
        "print(\"   Computing ground truth trajectory...\")\n",
        "sol_true = solve_ivp(\n",
        "    lorenz_system,\n",
        "    (0, t_test[-1]),\n",
        "    x0_test,\n",
        "    t_eval=t_test,\n",
        "    method='RK45',\n",
        "    rtol=1e-10,\n",
        "    atol=1e-12\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION: DISCOVERED VS TRUE DYNAMICS\n",
        "# ============================================================================\n",
        "fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Panel 1: 3D Phase Space Comparison\n",
        "# ----------------------------------------------------------------------------\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "\n",
        "# True trajectory\n",
        "ax1.plot(sol_true.y[0], sol_true.y[1], sol_true.y[2],\n",
        "        'b-', linewidth=2, alpha=0.6, label='True Lorenz')\n",
        "\n",
        "# SINDy trajectory\n",
        "ax1.plot(sim_poly2[:, 0], sim_poly2[:, 1], sim_poly2[:, 2],\n",
        "        'r--', linewidth=1.5, alpha=0.8, label='SINDy discovered')\n",
        "\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.set_title('3D Trajectories: True vs Discovered')\n",
        "ax1.legend(loc='upper right')\n",
        "ax1.view_init(elev=20, azim=45)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Panel 2: Time Series Comparison\n",
        "# ----------------------------------------------------------------------------\n",
        "ax2 = fig.add_subplot(132)\n",
        "\n",
        "# Plot x-component\n",
        "ax2.plot(t_test, sol_true.y[0], 'b-', linewidth=2,\n",
        "        alpha=0.7, label='True x(t)')\n",
        "ax2.plot(t_test, sim_poly2[:, 0], 'r--', linewidth=1.5,\n",
        "        alpha=0.8, label='SINDy x(t)')\n",
        "\n",
        "ax2.set_xlabel('Time [s]')\n",
        "ax2.set_ylabel('x(t)')\n",
        "ax2.set_title('Time Series: x-component')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add shaded region showing chaotic divergence\n",
        "divergence_time = 5.0  # Approximate time when chaos causes divergence\n",
        "ax2.axvspan(divergence_time, t_test[-1], alpha=0.1, color='gray')\n",
        "ax2.text(divergence_time + 0.5, ax2.get_ylim()[1] * 0.9,\n",
        "        'Chaotic\\ndivergence', fontsize=9, color='gray')\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Panel 3: Error Evolution\n",
        "# ----------------------------------------------------------------------------\n",
        "ax3 = fig.add_subplot(133)\n",
        "\n",
        "# Calculate errors\n",
        "error_x = np.abs(sim_poly2[:, 0] - sol_true.y[0])\n",
        "error_y = np.abs(sim_poly2[:, 1] - sol_true.y[1])\n",
        "error_z = np.abs(sim_poly2[:, 2] - sol_true.y[2])\n",
        "\n",
        "# Plot on log scale (shows exponential error growth)\n",
        "ax3.semilogy(t_test, error_x, 'b-', linewidth=1.5,\n",
        "            label='|error x|', alpha=0.7)\n",
        "ax3.semilogy(t_test, error_y, 'g-', linewidth=1.5,\n",
        "            label='|error y|', alpha=0.7)\n",
        "ax3.semilogy(t_test, error_z, 'r-', linewidth=1.5,\n",
        "            label='|error z|', alpha=0.7)\n",
        "\n",
        "ax3.set_xlabel('Time [s]')\n",
        "ax3.set_ylabel('Absolute Error (log scale)')\n",
        "ax3.set_title('Prediction Error Growth')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Mark exponential growth region\n",
        "ax3.axvline(divergence_time, color='gray', linestyle='--', alpha=0.5)\n",
        "ax3.text(divergence_time + 0.2, 1e-5,\n",
        "        'Exponential\\nerror growth', fontsize=9, color='gray')\n",
        "\n",
        "plt.suptitle('SINDy Validation: Forward Simulation of Discovered Equations',\n",
        "            fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# QUANTITATIVE VALIDATION METRICS\n",
        "# ============================================================================\n",
        "print(\"\\n📊 Simulation validation metrics:\")\n",
        "\n",
        "# Short-term accuracy (before chaos)\n",
        "short_term_idx = t_test < divergence_time\n",
        "mae_short = np.mean(np.abs(sim_poly2[short_term_idx] - sol_true.y[:, short_term_idx].T))\n",
        "rmse_short = np.sqrt(np.mean((sim_poly2[short_term_idx] - sol_true.y[:, short_term_idx].T)**2))\n",
        "\n",
        "print(f\"   Short-term (t < {divergence_time}s):\")\n",
        "print(f\"     MAE: {mae_short:.6f}\")\n",
        "print(f\"     RMSE: {rmse_short:.6f}\")\n",
        "\n",
        "# Full trajectory statistics\n",
        "mae_full = np.mean(np.abs(sim_poly2 - sol_true.y.T))\n",
        "rmse_full = np.sqrt(np.mean((sim_poly2 - sol_true.y.T)**2))\n",
        "\n",
        "print(f\"   Full trajectory:\")\n",
        "print(f\"     MAE: {mae_full:.3f}\")\n",
        "print(f\"     RMSE: {rmse_full:.3f}\")\n",
        "\n",
        "# Lyapunov time estimate (time for errors to grow by e)\n",
        "error_norm = np.sqrt(error_x**2 + error_y**2 + error_z**2)\n",
        "initial_error = error_norm[0] if error_norm[0] > 0 else 1e-10\n",
        "e_fold_idx = np.where(error_norm > initial_error * np.e)[0]\n",
        "if len(e_fold_idx) > 0:\n",
        "    lyapunov_time = t_test[e_fold_idx[0]]\n",
        "    print(f\"   Lyapunov time (error e-folding): {lyapunov_time:.2f}s\")\n",
        "\n",
        "print(\"\\n✅ Conclusions:\")\n",
        "print(\"   • SINDy correctly identified Lorenz equations\")\n",
        "print(\"   • Short-term predictions are excellent\")\n",
        "print(\"   • Long-term divergence is due to chaos, not wrong equations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0a9e98",
      "metadata": {
        "id": "7d0a9e98"
      },
      "source": [
        "### Application to Viscoelastic Material\n",
        "\n",
        "Now we apply SINDy to discover the constitutive equation for our viscoelastic material.\n",
        "\n",
        "**The challenge**: Our system has a control input (strain rate $\\dot{\\lambda}$), so we need SINDy with forcing:\n",
        "\n",
        "$$\\dot{\\sigma} = f(\\sigma) + g(\\sigma)\\cdot\\dot{\\lambda}$$\n",
        "\n",
        "**Expected form** (Maxwell power-law):\n",
        "$$\\frac{d\\sigma}{dt} = E\\dot{\\lambda} - \\frac{\\sigma}{\\eta_0}\\left|\\frac{\\sigma}{\\sigma_0}\\right|^{m-1}$$\n",
        "\n",
        "This suggests our library should include:\n",
        "- Linear term in control: $\\dot{\\lambda}$\n",
        "- Power terms in state: $\\sigma, \\sigma^2, \\sigma^3, ...$\n",
        "- Possibly absolute value terms: $|\\sigma|^n$\n",
        "\n",
        "### Challenges for Material Discovery\n",
        "\n",
        "1. **Limited data**: Unlike Lorenz, we have one loading cycle, not continuous trajectories\n",
        "2. **Noise**: Real experimental data has measurement noise\n",
        "3. **Library mismatch**: Standard polynomials may not capture $|\\sigma|^{m-1}$ terms\n",
        "4. **Sampling rate**: Material tests often have irregular time steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Selection Challenge for Viscoelastic Materials\n",
        "\n",
        "**Expected form**: Based on Maxwell power-law:\n",
        "$$\\frac{d\\sigma}{dt} = E \\dot{\\lambda} - \\frac{\\sigma}{\\nu_0}\\left|\\frac{\\sigma}{\\sigma_0}\\right|^{m-1}$$\n",
        "\n",
        "This suggests terms:\n",
        "- Linear in strain rate: $\\dot{\\lambda}$\n",
        "- Power-law in stress: $\\sigma^m$ with $m=3$\n",
        "\n",
        "**Challenge**: Standard polynomial library may not capture fractional powers or absolute values cleanly.\n",
        "\n",
        "**Threshold trade-off**:\n",
        "- Low threshold → more terms → better fit but less interpretable\n",
        "- High threshold → fewer terms → sparser but may miss physics"
      ],
      "metadata": {
        "id": "MhF5m0hCRzdm"
      },
      "id": "MhF5m0hCRzdm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2736ecfa",
      "metadata": {
        "id": "2736ecfa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Application to Viscoelastic Data: Discover dσ/dt = f(σ, \\dot{λ}) with SINDy\n",
        "# === Application to Viscoelastic Data (patched, robust to PySINDy version) ===\n",
        "# Discover dσ/dt = f(σ, \\dot{λ}) with SINDy.\n",
        "# Fixes:\n",
        "#  - Enforces strictly increasing time (your subsampling could create duplicates)\n",
        "#  - Avoids feature_names= / x_dot= kwargs (version-agnostic)\n",
        "#  - Uses a safe odd window length for the smoother\n",
        "\n",
        "import numpy as np\n",
        "import pysindy as ps\n",
        "from pysindy import SmoothedFiniteDifference\n",
        "\n",
        "# Build state/control: use Cauchy stress (sigma) as the state variable\n",
        "sigma_seq = stress_seq * (stretch_seq ** 2)                # σ = λ² S\n",
        "X_visco_full = np.column_stack([sigma_seq, rate_seq])      # [σ, \\dot{λ}]\n",
        "t_visco_full = np.asarray(time_seq)\n",
        "\n",
        "# Enforce strictly increasing time (drop duplicate/non-increasing steps)\n",
        "mask_inc = np.concatenate(([True], np.diff(t_visco_full) > 0))\n",
        "t_visco = t_visco_full[mask_inc]\n",
        "X_visco = X_visco_full[mask_inc]\n",
        "\n",
        "print(\"Viscoelastic data for SINDy:\")\n",
        "print(\"  State variables: sigma [kPa], strain_rate [1/s]\")\n",
        "print(f\"  Samples (after enforcing strictly increasing time): {len(X_visco)}\")\n",
        "print(\"  Target: d(sigma)/dt [kPa/s] (estimated internally)\")\n",
        "\n",
        "# Safe odd window length for the smoother\n",
        "def _odd_window(n, desired=11, minimum=5):\n",
        "    w = min(desired, n if n % 2 == 1 else n - 1)\n",
        "    w = max(w, minimum if minimum % 2 == 1 else minimum - 1, 3)\n",
        "    return int(w)\n",
        "\n",
        "win = _odd_window(len(t_visco), desired=11, minimum=5)\n",
        "sfd_visco = SmoothedFiniteDifference(smoother_kws={'window_length': win})\n",
        "\n",
        "# Library (polynomial up to degree 5 covers σ^3 and cross-terms with rate)\n",
        "lib_visco = ps.PolynomialLibrary(degree=5, include_bias=True)\n",
        "\n",
        "# Helper for robust coefficient counting (works across PySINDy versions)\n",
        "def _count_active_terms(model):\n",
        "    coefs = None\n",
        "    for attr in (\"coefficients\", \"coef_\"):\n",
        "        obj = getattr(model, attr, None)\n",
        "        if callable(obj):\n",
        "            try:\n",
        "                coefs = obj()\n",
        "            except Exception:\n",
        "                pass\n",
        "        elif obj is not None:\n",
        "            coefs = obj\n",
        "        if coefs is not None:\n",
        "            break\n",
        "    if coefs is None:\n",
        "        return None\n",
        "    if isinstance(coefs, (list, tuple)):\n",
        "        return int(sum(np.count_nonzero(c) for c in coefs))\n",
        "    return int(np.count_nonzero(coefs))\n",
        "\n",
        "# Fit multiple models with different thresholds (no feature_names / shuffle)\n",
        "thresholds = [0.1, 0.5, 1.0]\n",
        "models_visco = {}\n",
        "\n",
        "for thr in thresholds:\n",
        "    model = ps.SINDy(\n",
        "        feature_library=lib_visco,\n",
        "        optimizer=ps.STLSQ(threshold=thr),\n",
        "        differentiation_method=sfd_visco\n",
        "    )\n",
        "    model.fit(X_visco, t=t_visco)\n",
        "    models_visco[thr] = model\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"SINDy Model (Poly(5) + Smoothed FD) | threshold = {thr}\")\n",
        "    print(\"=\"*60)\n",
        "    try:\n",
        "        model.print()\n",
        "    except TypeError:\n",
        "        model.print()\n",
        "    nz = _count_active_terms(model)\n",
        "    if nz is not None:\n",
        "        print(f\"Number of active terms: {nz}\")\n",
        "\n",
        "# Choose a balanced model\n",
        "model_visco_best = models_visco.get(0.5, next(iter(models_visco.values())))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Validate discovered viscoelastic models\n",
        "# === Validate discovered viscoelastic models by simulation (patched) ===\n",
        "# Integrates with variable Δt from the provided time arrays.\n",
        "# State is σ (Cauchy) from SINDy; we convert back to S for plotting: S = σ / λ²\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simulate_visco_sindy(model, rate_profile, t, lambda0=1.0, sigma0=0.0):\n",
        "    r\"\"\"Explicit Euler rollout with variable time step.\n",
        "\n",
        "    Model: dσ/dt = f(σ, \\dot{λ})  (predicted by SINDy treating [σ, \\dot{λ}] as state)\n",
        "           dλ/dt = \\dot{λ}        (given as rate_profile)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    lam_hist : np.ndarray\n",
        "        Stretch history λ(t)\n",
        "    sig_hist : np.ndarray\n",
        "        Cauchy stress history σ(t)\n",
        "    \"\"\"\n",
        "    t = np.asarray(t)\n",
        "    r = np.asarray(rate_profile)\n",
        "    if t.ndim != 1 or r.ndim != 1 or len(t) != len(r):\n",
        "        raise ValueError(\"rate_profile and t must be 1D with the same length.\")\n",
        "\n",
        "    lam_hist = [lambda0]\n",
        "    sig_hist = [sigma0]\n",
        "\n",
        "    for k in range(1, len(t)):\n",
        "        dt = t[k] - t[k - 1]\n",
        "        r_km1 = r[k - 1]\n",
        "        # Model expects [sigma, rate] as input (since we fit both as 'states')\n",
        "        dsig_dt = model.predict(np.array([[sig_hist[-1], r_km1]]) )[0, 0]\n",
        "        sig_hist.append(sig_hist[-1] + dt * dsig_dt)\n",
        "        lam_hist.append(lam_hist[-1] + dt * r_km1)\n",
        "\n",
        "    return np.asarray(lam_hist), np.asarray(sig_hist)\n",
        "\n",
        "# Ensure we have a model (in case the previous cell name changed)\n",
        "if 'model_visco_best' not in globals():\n",
        "    # Fallback to any model we trained\n",
        "    if 'models_visco' in globals() and len(models_visco):\n",
        "        model_visco_best = models_visco.get(0.5, next(iter(models_visco.values())))\n",
        "    else:\n",
        "        raise RuntimeError(\"No SINDy visco model available. Run the previous cell first.\")\n",
        "\n",
        "# Run simulation on your two constant-rate reference curves\n",
        "lam1_pred, sig1_pred = simulate_visco_sindy(model_visco_best, rates1, time1)\n",
        "lam2_pred, sig2_pred = simulate_visco_sindy(model_visco_best, rates2, time2)\n",
        "\n",
        "# Convert σ -> S for comparison/plotting\n",
        "S1_pred = sig1_pred / (lam1_pred ** 2)\n",
        "S2_pred = sig2_pred / (lam2_pred ** 2)\n",
        "\n",
        "# Plots: hysteresis loops and residuals\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Hysteresis comparison\n",
        "ax1.plot(stretch1, stress1, 'b-', lw=2, alpha=0.75,\n",
        "         label=f'True (rate={abs(rates1[0]):.1f}/s)')\n",
        "ax1.plot(lam1_pred, S1_pred, 'b--', lw=2, alpha=0.9, label='SINDy pred (rate1)')\n",
        "\n",
        "ax1.plot(stretch2, stress2, 'r-', lw=2, alpha=0.75,\n",
        "         label=f'True (rate={abs(rates2[0]):.1f}/s)')\n",
        "ax1.plot(lam2_pred, S2_pred, 'r--', lw=2, alpha=0.9, label='SINDy pred (rate2)')\n",
        "\n",
        "ax1.set_xlabel('Stretch λ [-]')\n",
        "ax1.set_ylabel('Second Piola-Kirchhoff Stress S [kPa]')\n",
        "ax1.set_title('SINDy visco model — hysteresis validation')\n",
        "ax1.legend(loc='best')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Residuals (simple interpolation; note: stretch is not strictly monotonic)\n",
        "#S1_pred_interp = np.interp(stretch1, lam1_pred, S1_pred)\n",
        "#S2_pred_interp = np.interp(stretch2, lam2_pred, S2_pred)\n",
        "res1 = S1_pred - stress1\n",
        "res2 = S2_pred - stress2\n",
        "\n",
        "ax2.plot(stretch1, res1, 'b-', lw=2, alpha=0.85, label='rate1 residual')\n",
        "ax2.plot(stretch2, res2, 'r-', lw=2, alpha=0.85, label='rate2 residual')\n",
        "ax2.axhline(0, color='k', ls='--', lw=1)\n",
        "ax2.set_xlabel('Stretch λ [-]')\n",
        "ax2.set_ylabel('Residual [kPa]')\n",
        "ax2.set_title('SINDy prediction errors')\n",
        "ax2.legend(loc='best')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"SINDy validation (MAE in kPa):\")\n",
        "print(f\"  MAE (rate1): {np.mean(np.abs(res1)):.3f}\")\n",
        "print(f\"  MAE (rate2): {np.mean(np.abs(res2)):.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wEqs4lcxSAwI",
        "cellView": "form"
      },
      "id": "wEqs4lcxSAwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fd0b4bc8",
      "metadata": {
        "id": "fd0b4bc8"
      },
      "source": [
        "### Critical Analysis: SINDy for Material Discovery\n",
        "\n",
        "**What we learned**:\n",
        "\n",
        "1. **Success on known systems**: SINDy perfectly recovered Lorenz equations when the correct library was used\n",
        "\n",
        "2. **Derivative quality is crucial**: Poor derivatives → wrong equations. Smoothing can help but requires tuning\n",
        "\n",
        "3. **Library selection is an art**:\n",
        "   - Too restrictive: Miss true dynamics\n",
        "   - Too broad: Overfit with spurious terms\n",
        "   - Domain knowledge is essential\n",
        "\n",
        "4. **Sparsity vs accuracy tradeoff**: Higher threshold → simpler model but may miss physics\n",
        "\n",
        "5. **Material modeling challenges**:\n",
        "   - Limited data (one cycle vs continuous trajectory)\n",
        "   - Unknown functional form (power laws, absolute values)\n",
        "   - Control inputs complicate identification\n",
        "\n",
        "### When to Use Each Method\n",
        "\n",
        "**Use SINDy when**:\n",
        "- You suspect simple governing equations\n",
        "- You have clean data with good derivatives\n",
        "- Interpretability is crucial\n",
        "- You want to discover new physics\n",
        "\n",
        "**Use Neural Networks when**:\n",
        "- System is truly complex (many interacting terms)\n",
        "- Data is abundant but noisy\n",
        "- Black-box prediction is acceptable\n",
        "- Real-time inference is needed\n",
        "\n",
        "**Use Hybrid approaches when**:\n",
        "- Partial physics is known (physics-informed neural networks)\n",
        "- You want interpretability AND flexibility\n",
        "- Different time/length scales are present"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79bb26c6",
      "metadata": {
        "id": "79bb26c6"
      },
      "source": [
        "# Summary: Method Selection Framework\n",
        "\n",
        "## Comprehensive Comparison\n",
        "\n",
        "| Criterion | Feedforward NN | RNN/LSTM | SINDy | Traditional Models |\n",
        "|-----------|----------------|----------|-------|-------------------|\n",
        "| **History dependence** | ❌ None | ✅ Sequential memory | ⚠️ Requires derivatives | ✅ Explicit ODEs |\n",
        "| **Interpretability** | ❌ Black box | ❌ Black box | ✅ Explicit equations | ✅ Physical meaning |\n",
        "| **Data requirements** | Medium | High | Low-Medium | Low |\n",
        "| **Extrapolation** | ❌ Poor | ⚠️ Limited | ✅ Good if library correct | ✅ Physics-based |\n",
        "| **Training time** | Fast | Medium-Slow | Fast | Fast |\n",
        "| **Inference speed** | Very fast | Medium | Very fast | Very fast |\n",
        "| **Noise robustness** | Good with regularization | Good | ⚠️ Sensitive to derivatives | Good |\n",
        "| **Engineering insight** | ❌ None | ❌ Minimal | ✅ Governing equations | ✅ Parameters |\n",
        "\n",
        "---\n",
        "## Decision Framework\n",
        "\n",
        "### Use Feedforward Neural Networks When:\n",
        "✅ **Instantaneous relationships**: Output depends only on current input  \n",
        "✅ **Abundant data**: Enough measurements to cover operating range  \n",
        "✅ **Interpolation task**: Predictions within training domain  \n",
        "✅ **Speed critical**: Real-time inference required  \n",
        "\n",
        "❌ **Avoid when**: History matters, extrapolation needed, interpretability required\n",
        "\n",
        "**Example applications**:\n",
        "- Hyperelastic material characterization within test range\n",
        "- Sensor calibration curves\n",
        "- Property predictions from composition\n",
        "\n",
        "---\n",
        "### Use Recurrent Neural Networks When:\n",
        "✅ **History-dependent behavior**: Stress depends on loading path  \n",
        "✅ **Sequential data**: Time series with temporal structure  \n",
        "✅ **Unknown dynamics**: Governing equations not known  \n",
        "✅ **Rich datasets**: Sufficient sequences to train  \n",
        "\n",
        "❌ **Avoid when**: History does not matter, sparse data, interpretability critical\n",
        "\n",
        "**Example applications**:\n",
        "- Viscoelastic materials\n",
        "- Plasticity with strain hardening\n",
        "- Rate-dependent phenomena\n",
        "- Sequence prediction\n",
        "\n",
        "---\n",
        "### Use SINDy When:\n",
        "✅ **Sparse dynamics**: Few dominant terms govern behavior  \n",
        "✅ **Interpretability required**: Need explicit equations  \n",
        "✅ **Extrapolation needed**: Must predict beyond training  \n",
        "✅ **Known physics structure**: Can construct good library  \n",
        "\n",
        "❌ **Avoid when**: Dynamics are dense, derivative estimation unreliable, completely unknown physics\n",
        "\n",
        "**Example applications**:\n",
        "- Discovering constitutive laws\n",
        "- System identification from measurements\n",
        "- Reduced-order modeling\n",
        "- Validating physical hypotheses\n",
        "\n",
        "---\n",
        "### Use Traditional Physical Models When:\n",
        "✅ **Physics well-understood**: Governing equations known  \n",
        "✅ **Parameter calibration**: Only coefficients need fitting  \n",
        "✅ **Regulatory requirements**: Explainability mandatory  \n",
        "✅ **Limited data**: Few experiments available  \n",
        "\n",
        "❌ **Avoid when**: Physics unknown, complex multi-scale phenomena, traditional forms inadequate\n",
        "\n",
        "**Example applications**:\n",
        "- Classical mechanics problems\n",
        "- Established material models\n",
        "- Certified engineering design\n",
        "- Educational purposes\n",
        "\n",
        "---\n",
        "## Practical Workflow\n",
        "\n",
        "### Stage 1: Problem Characterization\n",
        "1. **Identify physics**: History-dependent? Instantaneous?\n",
        "2. **Assess data**: How many samples? Quality? Coverage?\n",
        "3. **Define requirements**: Accuracy? Speed? Interpretability? Extrapolation?\n",
        "\n",
        "### Stage 2: Initial Modeling\n",
        "- Start with **simplest appropriate method**\n",
        "- Establish **baseline performance**\n",
        "- Identify **failure modes**\n",
        "\n",
        "### Stage 3: Method Selection\n",
        "- If baseline adequate → use it (simplicity wins)\n",
        "- If history matters → add recurrence (RNN/LSTM)\n",
        "- If interpretability critical → try SINDy\n",
        "- If complexity high → hybrid approaches\n",
        "\n",
        "### Stage 4: Validation Strategy\n",
        "✅ **Interpolation tests**: Within training domain  \n",
        "✅ **Extrapolation tests**: Beyond training range  \n",
        "✅ **Physical constraints**: Thermodynamic admissibility  \n",
        "✅ **Robustness tests**: Noise sensitivity  \n",
        "✅ **Computational cost**: Training + inference time  \n",
        "\n",
        "---\n",
        "## Key Takeaways\n",
        "\n",
        "### 1. No Universal Best Method\n",
        "- **Context matters**: Problem physics, data availability, requirements\n",
        "- **Trade-offs exist**: Accuracy vs interpretability vs speed vs data needs\n",
        "- **Validate systematically**: Never trust a model without testing\n",
        "\n",
        "### 2. Data Quality Over Model Complexity\n",
        "- Clean, well-distributed data with simple model beats noisy data with complex model\n",
        "- Measurement noise limits achievable accuracy regardless of method\n",
        "- Strategic experiments (active learning) beat passive data collection\n",
        "\n",
        "### 3. Engineering Judgment Required\n",
        "- ML is a tool, not a replacement for domain knowledge\n",
        "- Physical constraints should guide, not replace, data-driven models\n",
        "- Validation against known physics prevents unphysical extrapolation\n",
        "\n",
        "### 4. Hybrid Approaches Often Win\n",
        "- Combine strengths of multiple methods  \n",
        "- Physics-informed neural networks (PINNs)  \n",
        "- SINDy for structure + NN for refinement  \n",
        "- Classical models + ML for residuals  \n",
        "\n",
        "---\n",
        "## Further Directions\n",
        "\n",
        "**Advanced architectures**:\n",
        "- LSTM/GRU for longer memory and stable training\n",
        "- Attention mechanisms for selective history\n",
        "- Physics-informed neural networks (PINNs)\n",
        "- Neural ordinary differential equations (Neural ODEs)\n",
        "\n",
        "**Enhanced SINDy**:\n",
        "- Weak formulation SINDy (noisy data)\n",
        "- SINDy with control inputs\n",
        "- Ensemble SINDy for uncertainty quantification\n",
        "- Custom libraries with domain-specific functions\n",
        "\n",
        "**Production deployment**:\n",
        "- Model compression and quantization\n",
        "- Real-time inference optimization\n",
        "- Uncertainty quantification\n",
        "- Online learning and adaptation\n",
        "\n",
        "**Ethical considerations**:\n",
        "- Safety-critical applications require verification\n",
        "- Model transparency for regulatory compliance\n",
        "- Bias in training data affects predictions\n",
        "- Responsibility for model failures\n",
        "\n",
        "---\n",
        "## Validation Checklist\n",
        "\n",
        "Before deploying any model to engineering practice:\n",
        "\n",
        "☐ **Physical plausibility**: Do predictions make physical sense?  \n",
        "☐ **Interpolation accuracy**: Low error within training domain?  \n",
        "☐ **Extrapolation behavior**: Reasonable outside training (if applicable)?  \n",
        "☐ **Robustness**: Stable under noise and perturbations?  \n",
        "☐ **Computational cost**: Acceptable for application?  \n",
        "☐ **Interpretability**: Can you explain predictions to stakeholders?  \n",
        "☐ **Uncertainty**: Do you know when the model is unreliable?  \n",
        "☐ **Documentation**: Complete trail for reproducibility?  \n",
        "\n",
        "\n",
        "---\n",
        "## Final Thoughts\n",
        "\n",
        "Machine learning for mechanical systems succeeds when:\n",
        "\n",
        "**Domain knowledge guides**:\n",
        "- Feature selection based on physics\n",
        "- Library construction reflecting expected structure\n",
        "- Validation against physical constraints\n",
        "\n",
        "**Data quality is prioritized**:\n",
        "- Strategic experimental design\n",
        "- Proper noise characterization\n",
        "- Adequate coverage of operating conditions\n",
        "\n",
        "**Methods are combined intelligently**:\n",
        "- Not \"ML vs physics\" but \"ML with physics\"\n",
        "- Leverage strengths of each approach\n",
        "- Transparent about limitations\n",
        "\n",
        "**Engineering judgment prevails**:\n",
        "- Understanding trumps blind optimization\n",
        "- Validation over training accuracy\n",
        "- Simplicity over unnecessary complexity\n",
        "\n",
        "> \"The best model is the simplest one that adequately captures the physics for your specific application.\"\n",
        "\n",
        "Thank you for completing this notebook. You now have the tools to:\n",
        "- Select appropriate ML architectures for mechanical problems\n",
        "- Train robust models from noisy experimental data\n",
        "- Validate models systematically\n",
        "- Make informed engineering decisions\n",
        "\n",
        "**Next steps**: Apply these methods to your own mechanical systems, always keeping physical plausibility and validation at the forefront."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8+"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}