{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artkula/ML-retreat-tekmek-2025/blob/main/unsupervised-learning-PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e21d8e1",
      "metadata": {
        "id": "1e21d8e1"
      },
      "source": [
        "# Introduction to PCA and unsupervised learning\n",
        "\n",
        "In this lesson we will be learning about *unsupervised learning*. This means that the computer has to work something out about data without knowing the correct answer to any data points a priori. Generally we say that there are three broad areas, clustering, anomaly detection, and dimensionality reduction.\n",
        "\n",
        "The methods we will examine in each category are\n",
        "\n",
        "- Dimensionality reduction\n",
        "    - Principal Component Analysis (PCA)\n",
        "    - Sparse PCA\n",
        "- Clustering / Unsupervised classification\n",
        "    - K-means\n",
        "    - Hierarchical clustering with dendrograms\n",
        "- Anomaly detection\n",
        "    - k-Nearest-Neighbours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c64bffd",
      "metadata": {
        "id": "2c64bffd"
      },
      "source": [
        "# Dimensionality reduction\n",
        "\n",
        "High fidelity measurements in engineering often produce vectors with hundreds or thousands of degrees of freedom, yet the dominant behavior is driven by a small number of patterns. **Dimensionality reduction** finds those low rank coordinates. By projecting data onto a smaller subspace we obtain a tractable description, filter noise, and reveal structure that is hard to see in the raw variables. Importantly, this is done **without labels**: the directions are inferred from statistical regularities in the snapshots.\n",
        "\n",
        "Classical principal component analysis (PCA) looks for orthogonal directions of maximum variance and yields dense modes that combine information from many sensors. Sparse PCA adds a sparsity penalty so that each mode uses only a few locations, improving interpretability at a small cost in captured variance. Both are linear, transparent, and fast.\n",
        "\n",
        "---\n",
        "\n",
        "# The flow dataset we use\n",
        "\n",
        "We work with a time series of a **2D, incompressible, unsteady double gyre** velocity field on a uniform Cartesian grid, augmented with a multi scale stochastic perturbation. Each snapshot contains the two components\n",
        "$$\n",
        "u(x,y,t), \\quad v(x,y,t),\n",
        "$$\n",
        "and our PCA is performed on the **stacked vector field** $[u \\mid v]$.\n",
        "\n",
        "**Domain and base flow**\n",
        "- Spatial domain: $x \\in [0,2]$, $y \\in [0,1]$.\n",
        "- The base flow is given by the streamfunction\n",
        "$$\n",
        "\\psi(x,y,t) \\;=\\; A \\,\\sin\\big(\\pi\\, f(x,t)\\big)\\, \\sin(\\pi y),\n",
        "$$\n",
        "with\n",
        "$$\n",
        "f(x,t) \\;=\\; a(t)\\,x^2 \\;+\\; b(t)\\,x, \\qquad\n",
        "a(t) \\;=\\; \\varepsilon \\sin(\\omega t), \\quad b(t) \\;=\\; 1 - 2\\varepsilon \\sin(\\omega t).\n",
        "$$\n",
        "Velocities follow from $\\mathbf{u}=(u,v)=(\\partial_y \\psi,\\,-\\partial_x \\psi)$.\n",
        "\n",
        "**Engineering relevance**\n",
        "- The double gyre is a clean proxy for **recirculating cells and transport barriers**, which appear in cavity flows, stirred tanks, and environmental/oceanic flows (e.g., mesoscale eddies). It is widely used to study **mixing, coherent structures, and Lagrangian transport**.\n",
        "- Its controlled unsteadiness makes it an excellent testbed for dimensionality reduction: a few modes capture the dominant sway of the gyres while higher modes capture smaller scale content.\n",
        "\n",
        "**Why it is asymmetric**\n",
        "- When $\\varepsilon > 0$, the coefficients $a(t)$ and $b(t)$ vary with $\\sin(\\omega t)$. This shifts the separatrix and the vortex centers horizontally, so the two gyres are **not mirror symmetric** at most times. Symmetry is approximately restored when $\\sin(\\omega t)=0$ (i.e., $a(t)=0$ and $b(t)=1$).\n",
        "- We also add a stochastic perturbation that further breaks symmetry in each snapshot.\n",
        "\n",
        "**Perturbation: multi scale noise and a simple knob**\n",
        "- We superimpose a fractional Brownian motion (fBm) field on $u$ and $v$ to mimic unresolved small scale effects.\n",
        "- A single parameter\n",
        "$$\n",
        "\\texttt{perturbation_strength} \\in \\mathbb{R}^+\n",
        "$$\n",
        "scales the perturbation amplitude. Increasing it injects more small scale variation and pushes energy into higher PCA modes; decreasing it yields a cleaner, nearly rank 1 dataset.\n",
        "\n",
        "**What we feed to PCA**\n",
        "- For each time $t_k$, we flatten $u(x,y,t_k)$ and $v(x,y,t_k)$, then form a row\n",
        "$$\n",
        "x_k \\;=\\; \\big[u(x_1,y_1,t_k),\\ldots,u(x_N,y_N,t_k)\\,\\mid\\,v(x_1,y_1,t_k),\\ldots,v(x_N,y_N,t_k)\\big].\n",
        "$$\n",
        "- The snapshot matrix $X \\in \\mathbb{R}^{T \\times 2N}$ is centered in time, then factorized by SVD to obtain spatial modes and temporal coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generator functions and other utilities (run but no need to see)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML # Import for Colab display\n",
        "import time\n",
        "\n",
        "# --- (Self-contained noise generation function) ---\n",
        "def generate_perlin_fbm(shape, res, octaves=4, persistence=0.5, lacunarity=2.0, seed=0):\n",
        "    \"\"\"\n",
        "    Generates a 3D Fractional Brownian Motion field using vectorized Perlin noise.\n",
        "    No external libraries needed beyond NumPy.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    noise = np.zeros(shape)\n",
        "    frequency = 1.0\n",
        "    amplitude = 1.0\n",
        "\n",
        "    for _ in range(octaves):\n",
        "        # Create grid points and random gradients\n",
        "        # FIX IS HERE: Ensure the resolution for the noise grid is at least 1,\n",
        "        # which makes the gradient grid size at least 2.\n",
        "        t_res, y_res, x_res = (max(1, int(r * frequency)) for r in res)\n",
        "\n",
        "        grid_shape = (t_res + 1, y_res + 1, x_res + 1)\n",
        "\n",
        "        gradients = rng.uniform(-1, 1, size=(*grid_shape, 3))\n",
        "        gradients /= np.linalg.norm(gradients, axis=-1, keepdims=True)\n",
        "\n",
        "        # Create coordinates for each point in the output shape\n",
        "        t_coords, y_coords, x_coords = np.meshgrid(\n",
        "            np.linspace(0, t_res, shape[0], endpoint=False),\n",
        "            np.linspace(0, y_res, shape[1], endpoint=False),\n",
        "            np.linspace(0, x_res, shape[2], endpoint=False),\n",
        "            indexing='ij'\n",
        "        )\n",
        "\n",
        "        # Calculate grid indices and local coordinates\n",
        "        t0, y0, x0 = t_coords.astype(int), y_coords.astype(int), x_coords.astype(int)\n",
        "        t, y, x = t_coords - t0, y_coords - y0, x_coords - x0\n",
        "\n",
        "        # Smoothstep function for interpolation weights\n",
        "        def fade(t): return t * t * t * (t * (t * 6 - 15) + 10)\n",
        "        wt, wy, wx = fade(t), fade(y), fade(x)\n",
        "\n",
        "        # Calculate dot products with gradients at the 8 corners of the cube\n",
        "        noise_val = np.zeros(shape)\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                for k in range(2):\n",
        "                    grad = gradients[t0 + i, y0 + j, x0 + k]\n",
        "                    dot_prod = grad[..., 0] * (t - i) + grad[..., 1] * (y - j) + grad[..., 2] * (x - k)\n",
        "\n",
        "                    # Interpolate\n",
        "                    interp_weight = (wt if i == 1 else 1 - wt) * \\\n",
        "                                    (wy if j == 1 else 1 - wy) * \\\n",
        "                                    (wx if k == 1 else 1 - wx)\n",
        "                    noise_val += dot_prod * interp_weight\n",
        "\n",
        "        noise += noise_val * amplitude\n",
        "        amplitude *= persistence\n",
        "        frequency *= lacunarity\n",
        "\n",
        "    return noise\n",
        "\n",
        "def generate_perturbed_gyre_data(\n",
        "    grid_res=20,       # Resolution of the spatial grid\n",
        "    n_snapshots=200,   # Number of time steps\n",
        "    A=0.1,             # Gyre velocity magnitude\n",
        "    epsilon=0.25,      # Gyre oscillation amplitude\n",
        "    omega=2 * np.pi / 10, # Gyre oscillation frequency\n",
        "\n",
        "    # --- Perturbation Field Parameters ---\n",
        "    perturbation_strength=0.4,\n",
        "    noise_scale=12.0,\n",
        "    octaves=4,\n",
        "    persistence=0.5,\n",
        "    lacunarity=2.0,\n",
        "    time_scale=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a Double Gyre dataset with a synthesized stochastic perturbation field.\n",
        "    \"\"\"\n",
        "    # 1. Define Spatial and Temporal Grid\n",
        "    x_points = np.linspace(0, 2, grid_res * 2); y_points = np.linspace(0, 1, grid_res)\n",
        "    xx, yy = np.meshgrid(x_points, y_points)\n",
        "    t_space = np.linspace(0, 20, n_snapshots)\n",
        "\n",
        "    # 2. Generate the entire 3D (time, y, x) noise fields in one go\n",
        "    shape_3d = (n_snapshots, grid_res, grid_res * 2)\n",
        "    res_3d = (t_space.size*time_scale/10, y_points.size*noise_scale/100, x_points.size*noise_scale/100)\n",
        "\n",
        "    print(\"Generating 3D noise field for u_perturb...\")\n",
        "    u_perturb_3d = generate_perlin_fbm(shape_3d, res_3d, octaves, persistence, lacunarity, seed=42)\n",
        "    print(\"Generating 3D noise field for v_perturb...\")\n",
        "    v_perturb_3d = generate_perlin_fbm(shape_3d, res_3d, octaves, persistence, lacunarity, seed=99)\n",
        "\n",
        "    snapshot_data = []\n",
        "    for i, t in enumerate(t_space):\n",
        "        # 3. Calculate base gyre flow for the current time step\n",
        "        a_t = epsilon * np.sin(omega * t); b_t = 1 - 2 * epsilon * np.sin(omega * t)\n",
        "        f_x_t = a_t * xx**2 + b_t * xx; df_dx = 2 * a_t * xx + b_t\n",
        "        u_gyre = -np.pi * A * np.sin(np.pi * f_x_t) * np.cos(np.pi * yy)\n",
        "        v_gyre = np.pi * A * np.cos(np.pi * f_x_t) * np.sin(np.pi * yy) * df_dx\n",
        "\n",
        "        # 4. Combine the base flow and the perturbation\n",
        "        u_total = u_gyre + perturbation_strength * A * u_perturb_3d[i]\n",
        "        v_total = v_gyre + perturbation_strength * A * v_perturb_3d[i]\n",
        "        speed = np.sqrt(u_total**2 + v_total**2)\n",
        "        snapshot_data.append(speed.flatten())\n",
        "\n",
        "    return np.array(snapshot_data), xx, yy\n",
        "\n",
        "def animate_flow(data_matrix, xx, yy, title_prefix, vmin, vmax):\n",
        "    \"\"\"Creates a matplotlib animation object for the given flow data.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    grid_shape = xx.shape\n",
        "    initial_frame = data_matrix[0, :].reshape(grid_shape)\n",
        "    quad_mesh = ax.pcolormesh(xx, yy, initial_frame, cmap='inferno', vmin=vmin, vmax=vmax, shading='gouraud')\n",
        "    fig.colorbar(quad_mesh, ax=ax, label='Flow Speed')\n",
        "\n",
        "    def update(frame):\n",
        "        ax.cla()\n",
        "        speed_field = data_matrix[frame, :].reshape(grid_shape)\n",
        "        ax.pcolormesh(xx, yy, speed_field, cmap='inferno', vmin=vmin, vmax=vmax, shading='gouraud')\n",
        "        ax.set_title(f\"{title_prefix} (Frame: {frame})\")\n",
        "        ax.set_xlabel('x')\n",
        "        ax.set_ylabel('y')\n",
        "        ax.set_aspect('equal', 'box')\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=data_matrix.shape[0], interval=50, blit=False)\n",
        "    plt.close(fig)\n",
        "    return ani"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1Mnk8AbbWW5s"
      },
      "id": "1Mnk8AbbWW5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the value of the variable `perturbation_strength` to adjust the level of noise."
      ],
      "metadata": {
        "id": "b-WJUx7oX5sz"
      },
      "id": "b-WJUx7oX5sz"
    },
    {
      "cell_type": "code",
      "source": [
        "perturbation_strength = 0.7 # Noise level"
      ],
      "metadata": {
        "id": "XUJHDHLFVQ-4"
      },
      "id": "XUJHDHLFVQ-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run for data generation and visualisation\n",
        "# MARCO\n",
        "# 1. Generate and visualise data  — REVISED (keep generation, replace visualisation)\n",
        "print(\"Generating Perturbed Double Gyre data...\")\n",
        "X_speed, xx, yy = generate_perturbed_gyre_data()   # ← unchanged\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# --- Parameters must match your generator defaults ---\n",
        "A = 0.1\n",
        "epsilon = 0.25\n",
        "omega = 2 * np.pi / 10.0\n",
        "# perturbation_strength = 0.7# was 0.4\n",
        "noise_scale = 12.0\n",
        "octaves = 4\n",
        "persistence = 0.5\n",
        "lacunarity = 2.0\n",
        "time_scale = 0.5\n",
        "\n",
        "# --- Rebuild the perturbed velocity field (u_total, v_total) to MATCH X_speed ---\n",
        "# We replicate exactly what happens inside generate_perturbed_gyre_data(),\n",
        "# but this time we KEEP u and v so we can visualise vectors/streamlines.\n",
        "ny, nx = yy.shape\n",
        "n_snapshots = X_speed.shape[0]\n",
        "x_points = xx[0, :]\n",
        "y_points = yy[:, 0]\n",
        "t_space = np.linspace(0, 20, n_snapshots)\n",
        "\n",
        "shape_3d = (n_snapshots, ny, nx)\n",
        "res_3d = (t_space.size * time_scale / 10,\n",
        "          y_points.size * noise_scale / 100,\n",
        "          x_points.size * noise_scale / 100)\n",
        "\n",
        "try:\n",
        "    print(\"Reconstructing perturbation fields to match generator seeds...\")\n",
        "    u_perturb_3d = generate_perlin_fbm(shape_3d, res_3d, octaves, persistence, lacunarity, seed=42)\n",
        "    v_perturb_3d = generate_perlin_fbm(shape_3d, res_3d, octaves, persistence, lacunarity, seed=99)\n",
        "except NameError:\n",
        "    # Fallback if helper isn't defined (shouldn't happen in your notebook)\n",
        "    print(\"Warning: generate_perlin_fbm not found. Falling back to unperturbed gyre (no noise).\")\n",
        "    u_perturb_3d = np.zeros(shape_3d)\n",
        "    v_perturb_3d = np.zeros(shape_3d)\n",
        "\n",
        "u_total = np.zeros_like(u_perturb_3d)\n",
        "v_total = np.zeros_like(v_perturb_3d)\n",
        "\n",
        "for i, t in enumerate(t_space):\n",
        "    a_t = epsilon * np.sin(omega * t)\n",
        "    b_t = 1.0 - 2.0 * epsilon * np.sin(omega * t)\n",
        "    f_x_t = a_t * xx**2 + b_t * xx\n",
        "    df_dx = 2.0 * a_t * xx + b_t\n",
        "    # Base double-gyre field\n",
        "    u_gyre = -np.pi * A * np.sin(np.pi * f_x_t) * np.cos(np.pi * yy)\n",
        "    v_gyre =  np.pi * A * np.cos(np.pi * f_x_t) * np.sin(np.pi * yy) * df_dx\n",
        "    # Add perturbations (exactly like the generator)\n",
        "    u_total[i] = u_gyre + perturbation_strength * A * u_perturb_3d[i]\n",
        "    v_total[i] = v_gyre + perturbation_strength * A * v_perturb_3d[i]\n",
        "\n",
        "# -------------------------\n",
        "# Static figure (streamlines + sparse quiver)\n",
        "# -------------------------\n",
        "t0_idx = n_snapshots // 10  # pick an early time to show separatrix clearly\n",
        "U0, V0 = u_total[t0_idx], v_total[t0_idx]\n",
        "S0 = np.sqrt(U0**2 + V0**2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 4))\n",
        "im = ax.imshow(S0, origin='lower',\n",
        "               extent=[x_points.min(), x_points.max(), y_points.min(), y_points.max()],\n",
        "               alpha=0.5, aspect='auto')\n",
        "strm = ax.streamplot(x_points, y_points, U0, V0, density=1.2, color=S0)\n",
        "step = max(1, ny // 10)\n",
        "ax.quiver(xx[::step, ::step], yy[::step, ::step], U0[::step, ::step], V0[::step, ::step],\n",
        "          scale=4.0 / A, width=0.002)\n",
        "ax.set_title(\"Double Gyre — streamlines (instantaneous field)\")\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
        "ax.set_xlim(x_points.min(), x_points.max()); ax.set_ylim(y_points.min(), y_points.max())\n",
        "cbar = fig.colorbar(im, ax=ax, shrink=0.85)\n",
        "cbar.set_label(r\"speed $|\\mathbf{u}|=\\sqrt{u^2+v^2}$\") # colorbar\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- VELOCITY COMPONENTS ANIMATION (drop-in) ---\n",
        "# Shows u(x,y,t) and v(x,y,t) side-by-side over time (fast, no streamlines).\n",
        "# Assumes u_total, v_total, xx, yy already exist from your generation cell.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Tunables (increase DECIM or decrease N_FRAMES to speed up further)\n",
        "N_FRAMES = 40         # total frames to render (e.g., 20–60)\n",
        "INTERVAL_MS = 60      # playback interval (ms)\n",
        "DECIM = 1             # spatial decimation (2,3,... to downsample grid for speed)\n",
        "\n",
        "nT = u_total.shape[0]\n",
        "frame_idx = np.linspace(0, nT - 1, N_FRAMES, dtype=int)\n",
        "\n",
        "# Common, symmetric color scale across time & components\n",
        "abs99 = np.nanpercentile(\n",
        "    np.abs(np.concatenate([u_total.ravel(), v_total.ravel()])),\n",
        "    99\n",
        ")\n",
        "vmin, vmax = -abs99, abs99\n",
        "\n",
        "# Prepare figure and first frame\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "im_u = axes[0].imshow(\n",
        "    u_total[frame_idx[0]][::DECIM, ::DECIM],\n",
        "    origin='lower',\n",
        "    extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "    vmin=vmin, vmax=vmax,\n",
        "    aspect='auto'\n",
        ")\n",
        "axes[0].set_title(f\"u(x,y,t idx={frame_idx[0]})\")\n",
        "axes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\")\n",
        "cbar_u = fig.colorbar(im_u, ax=axes[0], shrink=0.8)\n",
        "cbar_u.set_label(\"u\")\n",
        "\n",
        "im_v = axes[1].imshow(\n",
        "    v_total[frame_idx[0]][::DECIM, ::DECIM],\n",
        "    origin='lower',\n",
        "    extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "    vmin=vmin, vmax=vmax,\n",
        "    aspect='auto'\n",
        ")\n",
        "axes[1].set_title(f\"v(x,y,t idx={frame_idx[0]})\")\n",
        "axes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"y\")\n",
        "cbar_v = fig.colorbar(im_v, ax=axes[1], shrink=0.8)\n",
        "cbar_v.set_label(\"v\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "def update(fi):\n",
        "    # Update image data only (fast)\n",
        "    im_u.set_data(u_total[fi][::DECIM, ::DECIM])\n",
        "    im_v.set_data(v_total[fi][::DECIM, ::DECIM])\n",
        "    axes[0].set_title(f\"u(x,y,t idx={fi})\")\n",
        "    axes[1].set_title(f\"v(x,y,t idx={fi})\")\n",
        "    # Return artists if you want blitting; with blit=False it's optional\n",
        "    return [im_u, im_v]\n",
        "\n",
        "ani_uv = FuncAnimation(fig, update, frames=frame_idx, interval=INTERVAL_MS, blit=False)\n",
        "plt.close(fig)\n",
        "\n",
        "display(HTML(ani_uv.to_html5_video()))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3T9xlsUVpO8x"
      },
      "id": "3T9xlsUVpO8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA): a quick, practical primer\n",
        "\n",
        "**Goal.** Find orthogonal directions (\"modes\") that capture as much variation as possible in the data, ranked from most to least variance. Keeping only the first few gives the **best** low-rank linear approximation (least-squares optimal).\n",
        "\n",
        "**Our data matrix.**  \n",
        "Let $X \\in \\mathbb{R}^{T \\times F}$ with:\n",
        "- **Rows** = time snapshots $T$.  \n",
        "- **Columns** = **sensors** $F$. Here a sensor is a grid location **and** a component, so $F=2N$:  \n",
        "  $[u(x_1,y_1),\\dots,u(x_N,y_N)\\;|\\;v(x_1,y_1),\\dots,v(x_N,y_N)]$.\n",
        "\n",
        "**Compute PCA via SVD.**\n",
        "1. **Center** each column (remove temporal mean): $X_c = X - \\mathbf{1}\\mu$.\n",
        "2. **SVD:** $X_c = U\\,\\Sigma\\,V^\\top$.\n",
        "   - Columns of $V$: **spatial modes** (split each into $u$-part and $v$-part, then reshape to the grid).  \n",
        "   - Rows of $U\\Sigma$: **temporal coefficients** (how each mode evolves in time).  \n",
        "   - **Explained variance** of mode $i$: $\\sigma_i^2 / \\sum_j \\sigma_j^2$.  \n",
        "     Cumulative = $\\sum_{i=1}^r \\sigma_i^2 / \\sum_j \\sigma_j^2$.\n",
        "\n",
        "**Reconstruction rank-$r$.**  \n",
        "Best rank-$r$ approximation:\n",
        "$\n",
        "X^{(r)} \\;=\\; U_{(:,1:r)}\\,\\Sigma_{1:r}\\,V_{(:,1:r)}^\\top \\;+\\; \\text{mean}.\n",
        "$\n",
        "\n",
        "Snapshot $k$ is:\n",
        "$\n",
        "x_k^{(r)} \\;=\\; \\sum_{i=1}^r \\big(U_{k i}\\,\\sigma_i\\big)\\, v_i^\\top \\;+\\; \\mu.\n",
        "$\n",
        "\n",
        "**Relation to POD.**  \n",
        "On a uniform grid, this is exactly **Proper Orthogonal Decomposition (POD)**.  \n",
        "\n",
        "**Notes.**\n",
        "- No feature standardization here (that would distort physical units for POD).  \n",
        "- Mode signs are arbitrary; pairs of near-equal singular values can rotate within their 2D subspace.\n"
      ],
      "metadata": {
        "id": "tdO2KyREeP0h"
      },
      "id": "tdO2KyREeP0h"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform PCA on stacked velocity components [u | v]\n",
        "# Marco\n",
        "# 2. Perform PCA on stacked velocity components [u | v]\n",
        "# Assumes: u_total, v_total, xx, yy already exist from the visualisation cell.\n",
        "print(\"Performing PCA (POD) on vector field via SVD...\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nT = u_total.shape[0]\n",
        "nxy = xx.size\n",
        "\n",
        "# Build data matrix: each row = snapshot; columns = [u(:), v(:)]\n",
        "Xu = u_total.reshape(nT, nxy)\n",
        "Xv = v_total.reshape(nT, nxy)\n",
        "X_uv = np.hstack([Xu, Xv])          # shape (nT, 2*nxy)\n",
        "\n",
        "# Center in time (do NOT standardize for POD)\n",
        "X_mean = X_uv.mean(axis=0, keepdims=True)\n",
        "X_centered = X_uv - X_mean\n",
        "\n",
        "# SVD\n",
        "U, S, VT = np.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "# 3. Visualise singular value decay and cumulative variance explained\n",
        "kmax = min(100, S.size)\n",
        "explained = (S**2) / np.sum(S**2)\n",
        "cum_expl = np.cumsum(explained)\n",
        "\n",
        "plt.figure(figsize=(13, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.semilogy(S[:kmax], marker='o')\n",
        "plt.title('Singular Values Decay')\n",
        "plt.xlabel('Mode #')\n",
        "plt.ylabel('σᵢ')\n",
        "plt.grid(visible=True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(100 * cum_expl[:kmax], marker='o')\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Number of Modes')\n",
        "plt.ylabel('Cumulative Variance Explained (%)')\n",
        "plt.grid(visible=True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Helper to extract vector spatial modes later (optional, handy for plotting):\n",
        "# i-th right singular vector splits into u- and v-parts\n",
        "def mode_uv(i):\n",
        "    vec = VT[i]\n",
        "    u_mode = vec[:nxy].reshape(yy.shape)\n",
        "    v_mode = vec[nxy:].reshape(yy.shape)\n",
        "    return u_mode, v_mode\n"
      ],
      "metadata": {
        "id": "5CwGcWciwhZe",
        "cellView": "form"
      },
      "id": "5CwGcWciwhZe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualise first 3 spatial and temporal modes\n",
        "# Marco\n",
        "# 4. Visualise first 3 spatial modes (u and v shown separately)\n",
        "print(\"Visualising first 3 spatial modes (u and v separately)...\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "modes_to_show = 3\n",
        "nxy = xx.size\n",
        "\n",
        "fig, axes = plt.subplots(2, modes_to_show, figsize=(4.6*modes_to_show, 6), constrained_layout=True)\n",
        "\n",
        "for i in range(modes_to_show):\n",
        "    # split the i-th right singular vector into u- and v-parts\n",
        "    u_mode = VT[i, :nxy].reshape(yy.shape)\n",
        "    v_mode = VT[i, nxy:].reshape(yy.shape)\n",
        "\n",
        "    # common symmetric color scale for u & v of this mode\n",
        "    lim = np.nanpercentile(np.abs(np.r_[u_mode.ravel(), v_mode.ravel()]), 99)\n",
        "    vmin, vmax = -lim, lim\n",
        "\n",
        "    im_u = axes[0, i].imshow(u_mode, origin='lower',\n",
        "                             extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "                             vmin=vmin, vmax=vmax, aspect='auto')\n",
        "    axes[0, i].set_title(f\"Spatial Mode {i+1} — u\")\n",
        "    if i == 0: axes[0, i].set_ylabel(\"y\")\n",
        "    axes[0, i].set_xlabel(\"x\")\n",
        "    cbu = fig.colorbar(im_u, ax=axes[0, i], shrink=0.8)\n",
        "    cbu.set_label(\"amplitude\")\n",
        "\n",
        "    im_v = axes[1, i].imshow(v_mode, origin='lower',\n",
        "                             extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "                             vmin=vmin, vmax=vmax, aspect='auto')\n",
        "    axes[1, i].set_title(f\"Spatial Mode {i+1} — v\")\n",
        "    if i == 0: axes[1, i].set_ylabel(\"y\")\n",
        "    axes[1, i].set_xlabel(\"x\")\n",
        "    cbv = fig.colorbar(im_v, ax=axes[1, i], shrink=0.8)\n",
        "    cbv.set_label(\"amplitude\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 5. Visualise the first 3 temporal modes (shared by u & v)\n",
        "print(\"Visualising the first 3 temporal modes (shared for u & v)...\")\n",
        "\n",
        "# Use real time axis if available; otherwise use index\n",
        "t_axis = t_space if 't_space' in locals() else np.arange(U.shape[0])\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 6), sharex=True, constrained_layout=True)\n",
        "for i in range(modes_to_show):\n",
        "    axs[i].plot(t_axis, U[:, i] * S[i])\n",
        "    axs[i].set_ylabel(f\"Mode {i+1}\")\n",
        "    axs[i].grid(True, alpha=0.3)\n",
        "\n",
        "axs[-1].set_xlabel(\"Time\" if 't_space' in locals() else \"Time index\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tMWOyU_CykTk",
        "cellView": "form"
      },
      "id": "tMWOyU_CykTk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of PCA modes to use for the flow reconstruction:"
      ],
      "metadata": {
        "id": "-ScDnw76xEQ9"
      },
      "id": "-ScDnw76xEQ9"
    },
    {
      "cell_type": "code",
      "source": [
        "n_modes_to_reconstruct = 2"
      ],
      "metadata": {
        "id": "WLPnRbuNWMOa"
      },
      "id": "WLPnRbuNWMOa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Reconstruct Data (vector-field PCA on stacked [u | v])\n",
        "# Marco\n",
        "# 6. Reconstruct Data (vector-field PCA on stacked [u | v])\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# ---- choose how many modes to keep ----\n",
        "# n_modes_to_reconstruct = 10   # <- adjust as you like\n",
        "print(f\"Reconstructing flow using {n_modes_to_reconstruct} mode(s)...\")\n",
        "\n",
        "# Reconstruct stacked field: shape (nT, 2*nxy)\n",
        "X_reconstructed = (U[:, :n_modes_to_reconstruct] * S[:n_modes_to_reconstruct]) @ VT[:n_modes_to_reconstruct, :] + X_mean\n",
        "\n",
        "# Report cumulative variance explained (fixing the format bug)\n",
        "cum = np.cumsum(S**2) / np.sum(S**2)\n",
        "pct = 100.0 * cum[n_modes_to_reconstruct - 1]\n",
        "print(f\"Cumulative variance explained by first {n_modes_to_reconstruct} mode(s): {pct:.2f}%\")\n",
        "\n",
        "# Split into components and reshape back to (nT, ny, nx)\n",
        "nT = U.shape[0]\n",
        "ny, nx = yy.shape\n",
        "nxy = nx * ny\n",
        "u_rec = X_reconstructed[:, :nxy].reshape(nT, ny, nx)\n",
        "v_rec = X_reconstructed[:, nxy:].reshape(nT, ny, nx)\n",
        "\n",
        "# ---- Helper: animate u & v components side-by-side (fast, no streamlines) ----\n",
        "def animate_uv_components(u_arr, v_arr, xx, yy, title=\"\", N_FRAMES=40, INTERVAL_MS=60, DECIM=1, vmin=None, vmax=None):\n",
        "    # Shared symmetric color scale (prefer from true data if available)\n",
        "    if vmin is None or vmax is None:\n",
        "        if 'u_total' in locals() and 'v_total' in locals():\n",
        "            abs99 = np.nanpercentile(np.abs(np.r_[u_total.ravel(), v_total.ravel()]), 99)\n",
        "        else:\n",
        "            abs99 = np.nanpercentile(np.abs(np.r_[u_arr.ravel(), v_arr.ravel()]), 99)\n",
        "        vmin, vmax = -abs99, abs99\n",
        "\n",
        "    nT = u_arr.shape[0]\n",
        "    frame_idx = np.linspace(0, nT - 1, N_FRAMES, dtype=int)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    im_u = axes[0].imshow(u_arr[frame_idx[0]][::DECIM, ::DECIM], origin='lower',\n",
        "                          extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "                          vmin=vmin, vmax=vmax, aspect='auto')\n",
        "    axes[0].set_title(f\"u(x,y,t idx={frame_idx[0]})\"); axes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\")\n",
        "    cbar_u = fig.colorbar(im_u, ax=axes[0], shrink=0.8); cbar_u.set_label(\"u\")\n",
        "\n",
        "    im_v = axes[1].imshow(v_arr[frame_idx[0]][::DECIM, ::DECIM], origin='lower',\n",
        "                          extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
        "                          vmin=vmin, vmax=vmax, aspect='auto')\n",
        "    axes[1].set_title(f\"v(x,y,t idx={frame_idx[0]})\"); axes[1].set_xlabel(\"x\"); axes[1].set_ylabel(\"y\")\n",
        "    cbar_v = fig.colorbar(im_v, ax=axes[1], shrink=0.8); cbar_v.set_label(\"v\")\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    def update(fi):\n",
        "        im_u.set_data(u_arr[fi][::DECIM, ::DECIM])\n",
        "        im_v.set_data(v_arr[fi][::DECIM, ::DECIM])\n",
        "        axes[0].set_title(f\"u(x,y,t idx={fi})\")\n",
        "        axes[1].set_title(f\"v(x,y,t idx={fi})\")\n",
        "        return [im_u, im_v]\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=frame_idx, interval=INTERVAL_MS, blit=False)\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "## ---- Make the animation for reconstructed u & v ----\n",
        "#ani_reconstructed_uv = animate_uv_components(\n",
        "#    u_rec, v_rec, xx, yy,\n",
        "#    title=f\"Reconstructed Components (r={n_modes_to_reconstruct})\",\n",
        "#    N_FRAMES=40, INTERVAL_MS=60, DECIM=1\n",
        "#)\n",
        "#\n",
        "#print(f\"\\n--- Reconstructed Components Animation (r={n_modes_to_reconstruct}) ---\")\n",
        "#display(HTML(ani_reconstructed_uv.to_html5_video()))\n",
        "\n",
        "# --- SINGLE synchronized comparison video: reconstructed vs original (u & v) ---\n",
        "# Requires: u_rec, v_rec   (from your reconstruction cell)\n",
        "#           u_total, v_total, xx, yy, n_modes_to_reconstruct\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Settings (tweak for speed/size)\n",
        "N_FRAMES = 40\n",
        "INTERVAL_MS = 60\n",
        "DECIM = 1   # increase to 2–4 to decimate grid and speed up rendering\n",
        "\n",
        "# Choose evenly spaced time indices across the series\n",
        "nT = u_rec.shape[0]\n",
        "frame_idx = np.linspace(0, nT - 1, N_FRAMES, dtype=int)\n",
        "\n",
        "# Shared color scale per component (u has its own, v has its own), across rec+orig\n",
        "abs99_u = np.nanpercentile(np.abs(np.r_[u_rec.ravel(), u_total.ravel()]), 99)\n",
        "abs99_v = np.nanpercentile(np.abs(np.r_[v_rec.ravel(), v_total.ravel()]), 99)\n",
        "vmin_u, vmax_u = -abs99_u, abs99_u\n",
        "vmin_v, vmax_v = -abs99_v, abs99_v\n",
        "\n",
        "# Decimated coordinate extents (imshow uses extents; we can keep original)\n",
        "extent = [xx.min(), xx.max(), yy.min(), yy.max()]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(11, 8))\n",
        "fig.suptitle(f\"Vector-PCA reconstruction (r={n_modes_to_reconstruct}) — synchronized comparison\", y=0.98)\n",
        "\n",
        "# First frame\n",
        "i0 = frame_idx[0]\n",
        "im_rec_u = axes[0,0].imshow(u_rec[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_u, vmax=vmax_u, aspect='auto')\n",
        "axes[0,0].set_title(\"Reconstructed u\"); axes[0,0].set_xlabel(\"x\"); axes[0,0].set_ylabel(\"y\")\n",
        "fig.colorbar(im_rec_u, ax=axes[0,0], shrink=0.8, label=\"u\")\n",
        "\n",
        "im_rec_v = axes[0,1].imshow(v_rec[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_v, vmax=vmax_v, aspect='auto')\n",
        "axes[0,1].set_title(\"Reconstructed v\"); axes[0,1].set_xlabel(\"x\"); axes[0,1].set_ylabel(\"y\")\n",
        "fig.colorbar(im_rec_v, ax=axes[0,1], shrink=0.8, label=\"v\")\n",
        "\n",
        "im_org_u = axes[1,0].imshow(u_total[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_u, vmax=vmax_u, aspect='auto')\n",
        "axes[1,0].set_title(\"Original u\"); axes[1,0].set_xlabel(\"x\"); axes[1,0].set_ylabel(\"y\")\n",
        "fig.colorbar(im_org_u, ax=axes[1,0], shrink=0.8, label=\"u\")\n",
        "\n",
        "im_org_v = axes[1,1].imshow(v_total[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_v, vmax=vmax_v, aspect='auto')\n",
        "axes[1,1].set_title(\"Original v\"); axes[1,1].set_xlabel(\"x\"); axes[1,1].set_ylabel(\"y\")\n",
        "fig.colorbar(im_org_v, ax=axes[1,1], shrink=0.8, label=\"v\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "def update(fi):\n",
        "    im_rec_u.set_data(u_rec[fi][::DECIM, ::DECIM])\n",
        "    im_rec_v.set_data(v_rec[fi][::DECIM, ::DECIM])\n",
        "    im_org_u.set_data(u_total[fi][::DECIM, ::DECIM])\n",
        "    im_org_v.set_data(v_total[fi][::DECIM, ::DECIM])\n",
        "    axes[0,0].set_title(f\"Reconstructed u (t idx={fi})\")\n",
        "    axes[0,1].set_title(f\"Reconstructed v (t idx={fi})\")\n",
        "    axes[1,0].set_title(f\"Original u (t idx={fi})\")\n",
        "    axes[1,1].set_title(f\"Original v (t idx={fi})\")\n",
        "    return [im_rec_u, im_rec_v, im_org_u, im_org_v]\n",
        "\n",
        "ani_compare = FuncAnimation(fig, update, frames=frame_idx, interval=INTERVAL_MS, blit=False)\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"\\n--- Synchronized Reconstruction vs Original (u & v) ---\")\n",
        "display(HTML(ani_compare.to_html5_video()))\n"
      ],
      "metadata": {
        "id": "v313iab6029J",
        "cellView": "form"
      },
      "id": "v313iab6029J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19bb9b39",
      "metadata": {
        "id": "19bb9b39"
      },
      "source": [
        "# Sparse PCA: purpose, differences from PCA\n",
        "\n",
        "## What it is\n",
        "Sparse PCA (SPCA) is a low-rank factorization that finds components (loadings) which are **sparse** across sensors (features). The goal is the same spirit as PCA \"compress and reveal structure\" but with **localized, interpretable modes** rather than dense ones.\n",
        "\n",
        "Typical uses:\n",
        "- Interpretability: highlight **where** variability lives (support maps).\n",
        "- Sensor selection and placement: identify a **small subset of sensors** that explain most variation.\n",
        "- Region-of-interest analysis and feature selection in high-dimensional data.\n",
        "\n",
        "## Key differences vs standard PCA\n",
        "- **PCA** has a closed-form SVD on centered data. Modes are **dense**, **orthogonal**, and **variance-ordered**; the squared singular values equal explained variance.\n",
        "- **SPCA** solves an **optimization with sparsity penalties**, not an SVD. Components are **sparse**, generally **not orthogonal**, and **not variance-ordered**. Explained variance is assessed via reconstruction fit rather than singular values.\n",
        "- SPCA trades a bit of fidelity for **localization and interpretability**.\n",
        "\n",
        "## The optimization problem\n",
        "With centered data matrix $X \\in \\mathbb{R}^{T \\times F}$, SPCA approximates\n",
        "$\n",
        "X \\approx C W,\n",
        "$\n",
        "by solving\n",
        "$\n",
        "\\min_{C,\\,W}\\ \\tfrac{1}{2}\\,\\lVert X - C W \\rVert_F^2 \\;+\\; \\alpha\\,\\lVert W \\rVert_1 \\;+\\; \\tfrac{\\beta}{2}\\,\\lVert W \\rVert_F^2\n",
        "\\quad \\text{s.t.} \\quad \\lVert C_{:j} \\rVert_2 \\le 1 \\ \\forall j,\n",
        "$\n",
        "where:\n",
        "- $W \\in \\mathbb{R}^{r \\times F}$ are the **sparse loadings** (spatial components);\n",
        "- $C \\in \\mathbb{R}^{T \\times r}$ are the **codes** (temporal activations);\n",
        "- $\\alpha$ controls sparsity (larger \\(\\alpha\\) means more zeros);\n",
        "- $\\beta$ is a small ridge term (numerical stability).\n",
        "\n",
        "The problem is solved iteratively (alternating minimization with Lasso-type steps). There is no single SVD step.\n",
        "\n",
        "## Why standardization and why scale matters\n",
        "The sparsity penalty $\\lVert W \\rVert_1$ acts **per feature**. If features have different scales, large-variance features are **implicitly cheaper** to keep nonzero, and small-variance ones are over-penalized. Therefore we:\n",
        "- **Standardize features** (mean 0, variance 1) **before** SPCA;\n",
        "- Fit SPCA in standardized space;\n",
        "- **Map components and reconstructions back** to original physical units using the inverse transform when we visualize or compare to PCA.\n",
        "\n",
        "Without standardization, sparsity patterns largely reflect **units** rather than **structure**.\n",
        "\n",
        "## Disadvantages vs PCA\n",
        "- No orthogonality or variance ordering; components can be **correlated** and their scale/sign are arbitrary.\n",
        "- **No closed form**; requires **hyperparameters** ($\\alpha$, $\\beta$, $r$) and may converge to **local minima**. Slower than PCA.\n",
        "- Explained variance is **not** read from singular values; reconstruction fit is typically **worse** than PCA for the same number of components.\n",
        "- Results depend on **standardization choices** and can be less stable across runs or slight data changes.\n",
        "\n",
        "## When to pick SPCA\n",
        "Choose SPCA when **where** matters more than squeezing out the last bit of variance: you want localized, interpretable patterns or a small set of informative sensors. Use PCA when you want the **best energy capture** and orthogonal modes for compression and low-rank modeling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute SPCA on stacked [u | v]\n",
        "# MARCO\n",
        "# === Sparse PCA on stacked [u | v] ===\n",
        "import numpy as np\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Preparing stacked [u|v] data for Sparse PCA...\")\n",
        "\n",
        "# Build X_uv if not already available (rows = snapshots, cols = [u(:), v(:)])\n",
        "if 'X_uv' not in locals():\n",
        "    nT = u_total.shape[0]\n",
        "    nxy = xx.size\n",
        "    Xu = u_total.reshape(nT, nxy)\n",
        "    Xv = v_total.reshape(nT, nxy)\n",
        "    X_uv = np.hstack([Xu, Xv])  # shape (T, 2*nxy)\n",
        "\n",
        "# 1) Standardize features (mean 0, var 1) — important for SparsePCA\n",
        "print(\"Standardizing the stacked components (mean=0, variance=1)...\")\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_scaled = scaler.fit_transform(X_uv)\n",
        "\n",
        "# 2) Define Sparse PCA parameters\n",
        "n_sparse_modes = 3\n",
        "alpha_regularization = 1.0  # tune: larger -> sparser components\n",
        "print(f\"Applying SparsePCA on standardized [u|v] (n_components={n_sparse_modes}, alpha={alpha_regularization})...\")\n",
        "\n",
        "# 3) Fit SparsePCA\n",
        "spca = SparsePCA(\n",
        "    n_components=n_sparse_modes,\n",
        "    alpha=alpha_regularization,\n",
        "    random_state=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "codes = spca.fit_transform(X_scaled)    # shape (T, n_sparse_modes)\n",
        "components_std = spca.components_       # shape (n_sparse_modes, 2*nxy), in standardized feature coords\n",
        "print(\"Sparse PCA computed.\")\n",
        "\n",
        "# --- Quick sanity checks / utilities ---\n",
        "\n",
        "# Reconstruction (standardized space -> back to original units)\n",
        "X_scaled_hat = codes @ components_std\n",
        "X_spca = scaler.inverse_transform(X_scaled_hat)  # same shape as X_uv\n",
        "\n",
        "# Map sparse components to original feature units (useful for plotting u/v parts)\n",
        "scale_vec = scaler.scale_  # per-feature std used by the scaler\n",
        "components_orig = components_std * scale_vec  # broadcast multiply, shape (n_sparse_modes, 2*nxy)\n",
        "\n",
        "def spca_mode_uv(j):\n",
        "    \"\"\"Return (u_mode, v_mode) of sparse component j in ORIGINAL units.\"\"\"\n",
        "    u_mode = components_orig[j, :nxy].reshape(yy.shape)\n",
        "    v_mode = components_orig[j, nxy:].reshape(yy.shape)\n",
        "    return u_mode, v_mode\n",
        "\n",
        "# Approx. variance captured by the SparsePCA subspace (on standardized data)\n",
        "tot = np.sum(X_scaled**2)\n",
        "resid = np.sum((X_scaled - X_scaled_hat)**2)\n",
        "r2 = 1.0 - resid / tot\n",
        "print(f\"Approx. variance captured by SparsePCA subspace: {100*r2:.2f}%\")\n",
        "\n",
        "# Optional: alignment of SparsePCA components with top PCA modes (cosine similarity)\n",
        "# Uses PCA right singular vectors VT (rows = PCA modes) in original units.\n",
        "if 'VT' in locals():\n",
        "    V = VT  # shape (r, 2*nxy)\n",
        "    comps_norm = components_orig / np.linalg.norm(components_orig, axis=1, keepdims=True)\n",
        "    # Compare to the first 6 PCA modes (adjust as you like)\n",
        "    k_compare = min(6, V.shape[0])\n",
        "    cos = np.abs(comps_norm @ V[:k_compare].T)  # (n_sparse_modes x k_compare)\n",
        "    print(\"Cosine similarity of SparsePCA components vs top PCA modes (rows = sparse comps):\")\n",
        "    print(np.round(cos, 3))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9L_Tvpu0mDSM"
      },
      "id": "9L_Tvpu0mDSM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize first 3 SparsePCA spatial modes per velocity component\n",
        "# MARCO\n",
        "# === Visualize first 3 SparsePCA spatial modes per component (u on top, v below) ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "modes_to_show = min(3, components_orig.shape[0])\n",
        "nxy = xx.size\n",
        "extent = [xx.min(), xx.max(), yy.min(), yy.max()]\n",
        "\n",
        "fig, axes = plt.subplots(2, modes_to_show, figsize=(4.8*modes_to_show, 6), constrained_layout=True)\n",
        "\n",
        "sparsity_report = []\n",
        "for j in range(modes_to_show):\n",
        "    # Get u/v parts in ORIGINAL units\n",
        "    u_mode, v_mode = spca_mode_uv(j)\n",
        "\n",
        "    # Symmetric color scale per component (shared between its u and v)\n",
        "    lim = np.nanpercentile(np.abs(np.r_[u_mode.ravel(), v_mode.ravel()]), 99)\n",
        "    vmin, vmax = -lim, lim\n",
        "\n",
        "    # Plot u\n",
        "    im_u = axes[0, j].imshow(\n",
        "        u_mode, origin=\"lower\", extent=extent, vmin=vmin, vmax=vmax, aspect=\"equal\"\n",
        "    )\n",
        "    axes[0, j].set_title(f\"Sparse comp {j+1} — u\")\n",
        "    axes[0, j].set_xlabel(\"x\"); axes[0, j].set_ylabel(\"y\")\n",
        "    cb_u = fig.colorbar(im_u, ax=axes[0, j], shrink=0.8)\n",
        "    cb_u.set_label(\"amplitude\")\n",
        "\n",
        "    # Plot v\n",
        "    im_v = axes[1, j].imshow(\n",
        "        v_mode, origin=\"lower\", extent=extent, vmin=vmin, vmax=vmax, aspect=\"equal\"\n",
        "    )\n",
        "    axes[1, j].set_title(f\"Sparse comp {j+1} — v\")\n",
        "    axes[1, j].set_xlabel(\"x\"); axes[1, j].set_ylabel(\"y\")\n",
        "    cb_v = fig.colorbar(im_v, ax=axes[1, j], shrink=0.8)\n",
        "    cb_v.set_label(\"amplitude\")\n",
        "\n",
        "    # Sparsity stats (using a small threshold to ignore numerical dust)\n",
        "    thr = 1e-8 * max(1.0, np.max(np.abs(components_orig[j])))\n",
        "    nz_u = np.count_nonzero(np.abs(components_orig[j, :nxy]) > thr)\n",
        "    nz_v = np.count_nonzero(np.abs(components_orig[j, nxy:]) > thr)\n",
        "    sparsity_report.append((j+1, nz_u, nz_v))\n",
        "\n",
        "# lock aspect and limits\n",
        "for ax in axes.ravel():\n",
        "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Optional: print a compact sparsity summary\n",
        "total_u = nxy\n",
        "total_v = nxy\n",
        "for j, nz_u, nz_v in sparsity_report:\n",
        "    pu = 100.0 * nz_u / total_u\n",
        "    pv = 100.0 * nz_v / total_v\n",
        "    print(f\"Sparse component {j}: nonzeros u = {nz_u}/{total_u} ({pu:.2f}%), v = {nz_v}/{total_v} ({pv:.2f}%)\")\n",
        "\n",
        "# Optional (temporal activations): plot the first 3 sparse codes to connect to PCA time modes\n",
        "fig, axs = plt.subplots(modes_to_show, 1, figsize=(8, 5), sharex=True, constrained_layout=True)\n",
        "t_axis = np.arange(codes.shape[0]) if \"t_space\" not in locals() else t_space\n",
        "for j in range(modes_to_show):\n",
        "    axs[j].plot(t_axis, codes[:, j])\n",
        "    axs[j].set_ylabel(f\"code {j+1}\")\n",
        "    axs[j].grid(True, alpha=0.3)\n",
        "axs[-1].set_xlabel(\"time\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ucZs0ueOm2i-"
      },
      "id": "ucZs0ueOm2i-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to read your SparsePCA plots\n",
        "\n",
        "- Zeros = “inactive sensors.” After standardizing features, the L1-like penalty drives many coefficients to 0. Non-zero patches show where each component lives on the grid.\n",
        "\n",
        "- Comp 1 looks like PCA mode 1 (global gyre sway) but with small/flat regions zeroed out.\n",
        "\n",
        "- Comp 2 captures the next coherent pattern (the left/right lobe contrast) and is much sparser—it drops background regions that don’t help distinguish that pattern.\n",
        "\n",
        "- Comp 3 is mostly small-scale/patchy: it’s isolating parts of the perturbation field your PCA mode 3 also hinted at. Its sparsity tells you those fluctuations are spatially localized."
      ],
      "metadata": {
        "id": "XWikRIz8wWE4"
      },
      "id": "XWikRIz8wWE4"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SparsePCA reconstruction and comparison\n",
        "# MARCO\n",
        "# === SparsePCA reconstruction and synchronized comparison video ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "print(\"Reconstructing data from SparsePCA components...\")\n",
        "\n",
        "# Safety: if you have not kept these names from the SPCA fit cell, adapt them here:\n",
        "# - scaler          : fitted StandardScaler on X_uv\n",
        "# - spca            : fitted SparsePCA model\n",
        "# - codes           : spca.fit_transform(X_scaled), shape (T, n_sparse_modes)\n",
        "# - components_std  : spca.components_, shape (n_sparse_modes, 2*nxy)\n",
        "# - X_uv            : original stacked data (T, 2*nxy) in physical units\n",
        "# - u_total, v_total, xx, yy\n",
        "\n",
        "# 1) Reconstruct in standardized space, then invert scaling back to physical units\n",
        "X_scaled_hat = codes @ components_std                            # standardized coords\n",
        "X_spca_hat   = scaler.inverse_transform(X_scaled_hat)            # back to original units\n",
        "\n",
        "# 2) Split into components and reshape\n",
        "nT = X_spca_hat.shape[0]\n",
        "ny, nx = yy.shape\n",
        "nxy = nx * ny\n",
        "u_rec_spca = X_spca_hat[:, :nxy].reshape(nT, ny, nx)\n",
        "v_rec_spca = X_spca_hat[:, nxy:].reshape(nT, ny, nx)\n",
        "\n",
        "print(\"Reconstruction complete.\")\n",
        "\n",
        "# 3) Explained variance style metrics\n",
        "#    Note: SparsePCA components are not orthogonal, so we report an R^2 computed on standardized data.\n",
        "X_scaled = scaler.transform(X_uv)                                # standardized original data\n",
        "num = np.sum((X_scaled - X_scaled_hat)**2)\n",
        "den = np.sum(X_scaled**2)\n",
        "r2_std = 1.0 - num/den\n",
        "print(f\"Approx. explained variance in standardized space (R^2): {100.0*r2_std:.2f}%\")\n",
        "\n",
        "# Also report relative Frobenius error in original units (purely for reference)\n",
        "rel_err_orig = np.linalg.norm(X_uv - X_spca_hat, ord='fro') / np.linalg.norm(X_uv, ord='fro')\n",
        "print(f\"Relative Frobenius error in original units: {100.0*rel_err_orig:.2f}%  (lower is better)\")\n",
        "\n",
        "# 4) Single synchronized comparison video: top = SPCA recon (u,v), bottom = original (u,v)\n",
        "N_FRAMES = 40\n",
        "INTERVAL_MS = 60\n",
        "DECIM = 1  # increase to 2..4 to speed up\n",
        "\n",
        "frame_idx = np.linspace(0, nT - 1, N_FRAMES, dtype=int)\n",
        "\n",
        "# Component-wise color scales shared between recon and original\n",
        "abs99_u = np.nanpercentile(np.r_[u_rec_spca.ravel(), u_total.ravel()], 99)\n",
        "abs99_v = np.nanpercentile(np.r_[v_rec_spca.ravel(), v_total.ravel()], 99)\n",
        "vmin_u, vmax_u = -abs99_u, abs99_u\n",
        "vmin_v, vmax_v = -abs99_v, abs99_v\n",
        "\n",
        "extent = [xx.min(), xx.max(), yy.min(), yy.max()]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(11, 8))\n",
        "fig.suptitle(f\"SparsePCA reconstruction (r={spca.n_components}) vs original\", y=0.98)\n",
        "\n",
        "i0 = frame_idx[0]\n",
        "# Row 1: SPCA reconstruction\n",
        "im_rec_u = axes[0,0].imshow(u_rec_spca[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_u, vmax=vmax_u, aspect='equal')\n",
        "axes[0,0].set_title(\"Reconstructed u\"); axes[0,0].set_xlabel(\"x\"); axes[0,0].set_ylabel(\"y\")\n",
        "fig.colorbar(im_rec_u, ax=axes[0,0], shrink=0.8, label=\"u\")\n",
        "\n",
        "im_rec_v = axes[0,1].imshow(v_rec_spca[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_v, vmax=vmax_v, aspect='equal')\n",
        "axes[0,1].set_title(\"Reconstructed v\"); axes[0,1].set_xlabel(\"x\"); axes[0,1].set_ylabel(\"y\")\n",
        "fig.colorbar(im_rec_v, ax=axes[0,1], shrink=0.8, label=\"v\")\n",
        "\n",
        "# Row 2: Original\n",
        "im_org_u = axes[1,0].imshow(u_total[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_u, vmax=vmax_u, aspect='equal')\n",
        "axes[1,0].set_title(\"Original u\"); axes[1,0].set_xlabel(\"x\"); axes[1,0].set_ylabel(\"y\")\n",
        "fig.colorbar(im_org_u, ax=axes[1,0], shrink=0.8, label=\"u\")\n",
        "\n",
        "im_org_v = axes[1,1].imshow(v_total[i0][::DECIM, ::DECIM], origin='lower', extent=extent,\n",
        "                            vmin=vmin_v, vmax=vmax_v, aspect='equal')\n",
        "axes[1,1].set_title(\"Original v\"); axes[1,1].set_xlabel(\"x\"); axes[1,1].set_ylabel(\"y\")\n",
        "fig.colorbar(im_org_v, ax=axes[1,1], shrink=0.8, label=\"v\")\n",
        "\n",
        "# lock aspect and limits\n",
        "for ax in axes.ravel():\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "def update(fi):\n",
        "    im_rec_u.set_data(u_rec_spca[fi][::DECIM, ::DECIM])\n",
        "    im_rec_v.set_data(v_rec_spca[fi][::DECIM, ::DECIM])\n",
        "    im_org_u.set_data(u_total[fi][::DECIM, ::DECIM])\n",
        "    im_org_v.set_data(v_total[fi][::DECIM, ::DECIM])\n",
        "    axes[0,0].set_title(f\"Reconstructed u (t idx={fi})\")\n",
        "    axes[0,1].set_title(f\"Reconstructed v (t idx={fi})\")\n",
        "    axes[1,0].set_title(f\"Original u (t idx={fi})\")\n",
        "    axes[1,1].set_title(f\"Original v (t idx={fi})\")\n",
        "    return [im_rec_u, im_rec_v, im_org_u, im_org_v]\n",
        "\n",
        "ani_spca_compare = FuncAnimation(fig, update, frames=frame_idx, interval=INTERVAL_MS, blit=False)\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"\\n--- SparsePCA reconstruction vs original (synchronized) ---\")\n",
        "display(HTML(ani_spca_compare.to_html5_video()))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U4udckrYph04"
      },
      "id": "U4udckrYph04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c128f84",
      "metadata": {
        "id": "8c128f84"
      },
      "source": [
        "## Classification / clustering\n",
        "When no labels guide us, clustering becomes our lens for discovering latent groupings—“natural categories” that emerge purely from the geometry of the data cloud.  Formally we are looking for a partition of the point set $\\{\\mathbf{x}_i\\}_{i=1}^N\\subset\\mathbb{R}^p$ that makes members of the same cluster more similar to one another than to points outside.  Yet similarity is in the eye of the algorithm.  **$k$-means** measures it with global Euclidean distance from movable centroids and therefore prefers compact, roughly spherical groups; **hierarchical linkage** builds a full dendrogram by agglomerating the closest pairs, exposing structure at every scale but forcing the analyst to decide where to “cut” the tree. We will illustrate these biases with a deliberately eclectic 2‑D toy set that combines tight Gaussian blobs, an elongated cigar, a curved crescent and a sprinkling of stray points.  Watching each method succeed on some shapes and stumble on others makes vivid the central lesson: clustering is not a one‑size‑fits‑all tool but a family of algorithms whose output reflects the particular notion of proximity they encode.\n",
        "\n",
        "## Why clustering and classification matter in Mechanical Engineering\n",
        "\n",
        "Clustering (unsupervised) and classification (supervised) are workhorses for turning high-dimensional engineering data into actionable insights. They help engineers discover operating regimes, detect anomalies early, and simplify models without hand-crafted thresholds.\n",
        "\n",
        "### Common use cases\n",
        "\n",
        "1) Condition monitoring and fault diagnosis\n",
        "- Group vibration or acoustic spectra from rotating machinery to identify health states (normal, imbalance, misalignment, bearing fault).\n",
        "- Unsupervised clustering flags **unknown** failure modes by detecting novel clusters or outliers.\n",
        "- Works with signals from accelerometers, microphones, strain gauges, current sensors.\n",
        "\n",
        "2) Experimental flow and structural measurements\n",
        "- Segment PIV/PTV velocity fields or DIC strain fields into coherent regions (shear layers, recirculation, damage zones).\n",
        "- Cluster POD modes or time coefficients to identify flow regimes or deformation patterns.\n",
        "- Group impact or modal test responses into similar behavior families.\n",
        "\n",
        "3) Operating regime discovery and design-of-experiments\n",
        "- Cluster multivariate telemetry (speed, torque, temperatures, pressures) to discover **natural regimes** of engines, compressors, turbomachinery.\n",
        "- Use regime labels to build regime-specific reduced-order models or controllers.\n",
        "- In DOE, cluster responses to detect **latent factors** or **batch effects**.\n",
        "\n",
        "4) Materials and manufacturing\n",
        "- Cluster microstructure descriptors (e.g., grain sizes, orientation tensors, porosity features) to categorize processing outcomes.\n",
        "- Group additive manufacturing melt-pool signatures or thermal histories to predict defects.\n",
        "- Classify tool wear states from force/AE/vibration in machining.\n",
        "\n",
        "5) Safety and reliability\n",
        "- Early **anomaly detection** in fleets via density-based clustering or autoencoder embeddings.\n",
        "- Root-cause exploration by comparing cluster centroids and feature importances.\n",
        "\n",
        "### Typical algorithms and when to use them\n",
        "\n",
        "- K-Means / GMM: compact, roughly spherical clusters; fast baselines; GMM handles anisotropy.\n",
        "- Hierarchical (Ward, single, complete): produces a dendrogram to explore **multi-scale** structure; Ward favors compact clusters, single preserves filaments but can chain noise.\n",
        "- k-NN: Able to identify anomalies in data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac13705",
      "metadata": {
        "cellView": "form",
        "id": "eac13705"
      },
      "outputs": [],
      "source": [
        "#@title Data generation and k-means clustering code -- run this cell to generate the data, no need to look at the code\n",
        "\n",
        "def k_means_step(X, centroids):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    X : (N_samples, N_features) matrix\n",
        "        Data to cluster.\n",
        "    centroids : (n_clusters, N_features) matrix\n",
        "    \"\"\"\n",
        "\n",
        "    n_clusters = centroids.shape[0]\n",
        "\n",
        "    # check connectivity of centroids\n",
        "    labels = np.zeros(X.shape[0], dtype=int)\n",
        "    d = np.zeros((n_clusters))\n",
        "    for i in range(X.shape[0]):\n",
        "        for k in range(n_clusters):\n",
        "            d[k] = np.linalg.norm(X[i,:] - centroids[k,:])\n",
        "        labels[i] = np.argmin(d)  # assign to the closest centroid\n",
        "\n",
        "\n",
        "    new_centroids = np.zeros(centroids.shape)\n",
        "    for k in range(centroids.shape[0]):\n",
        "        new_centroids[k,:] = np.mean(X[labels == k, :], axis=0)\n",
        "    return labels, new_centroids\n",
        "\n",
        "def k_means(X, n_clusters=3, max_iter=100, n_inits=10):\n",
        "    \"\"\"\n",
        "    K-means clustering algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (N_samples, N_features) array-like\n",
        "        Data to cluster.\n",
        "    n_clusters : int\n",
        "        Number of clusters.\n",
        "    max_iter : int\n",
        "        Maximum number of iterations.\n",
        "    n_inits : int\n",
        "        Number of times the algorithm will be run with different centroid seeds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    labels : (N_samples,) array-like\n",
        "        Cluster labels for each sample.\n",
        "    centroids : (n_clusters, N_features) array-like\n",
        "        Final cluster centroids.\n",
        "    \"\"\"\n",
        "    # Initialize random centroids\n",
        "    loss = np.inf\n",
        "    for k in range(n_inits):\n",
        "        np.random.seed(k)\n",
        "        initial_indices = np.random.choice(X.shape[0], n_clusters, replace=False)\n",
        "        centroids = X[initial_indices, :]\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            labels, new_centroids = k_means_step(X, centroids)\n",
        "            if np.all(centroids == new_centroids):\n",
        "                break  # Convergence\n",
        "            centroids = new_centroids\n",
        "\n",
        "        loss_new = np.sum((X - centroids[labels])**2)\n",
        "        if loss_new < loss:\n",
        "            loss = loss_new\n",
        "            best_labels = labels\n",
        "            best_centroids = centroids\n",
        "\n",
        "    return best_labels, best_centroids\n",
        "\n",
        "\n",
        "def animated_kmeans_process(X, centroids, n_frames=4, interval=100):\n",
        "    \"\"\"\n",
        "    Animated k-means clustering process.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (N_samples, N_features) array-like\n",
        "        Data to cluster.\n",
        "    n_clusters : int\n",
        "        Number of clusters.\n",
        "    n_frames : int\n",
        "        Number of frames in the animation.\n",
        "    interval : int\n",
        "        Delay between frames in milliseconds.\n",
        "    \"\"\"\n",
        "    # # Initialize random centroids\n",
        "    # np.random.seed(1)\n",
        "    # initial_indices = np.random.choice(X.shape[0], n_clusters, replace=False)\n",
        "    # centroids = X[initial_indices, :]\n",
        "\n",
        "    # get the labels for initial centroids\n",
        "    labels = np.zeros(X.shape[0], dtype=int)\n",
        "    for i in range(X.shape[0]):\n",
        "        d1 = np.linalg.norm(X[i,:] - centroids[0,:])\n",
        "        d2 = np.linalg.norm(X[i,:] - centroids[1,:])\n",
        "        d3 = np.linalg.norm(X[i,:] - centroids[2,:])\n",
        "        if d1 < d2 and d1 < d3:\n",
        "            labels[i] = 0\n",
        "        elif d2 < d1 and d2 < d3:\n",
        "            labels[i] = 1\n",
        "        else:\n",
        "            labels[i] = 2\n",
        "\n",
        "\n",
        "    # Prepare the figure\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    scat = ax.scatter(X[:, 0], X[:, 1], c='lightgray', s=10)\n",
        "    scat.set_array(labels)  # Initial colors based on labels\n",
        "    centroid_scat = ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='X')\n",
        "    ax.set_title(\"K-means Clustering Process\")\n",
        "    ax.set_xlabel(\"Feature 1\")\n",
        "    ax.set_ylabel(\"Feature 2\")\n",
        "\n",
        "    # fig.show()\n",
        "\n",
        "    def update(frame):\n",
        "        nonlocal centroids\n",
        "        nonlocal labels\n",
        "\n",
        "        scat.set_array(labels)  # Update scatter plot colors based on labels\n",
        "        centroid_scat.set_offsets(centroids)  # Update centroid positions\n",
        "\n",
        "        labels, new_centroids = k_means_step(X, centroids)\n",
        "        centroids = new_centroids\n",
        "        return scat, centroid_scat\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=n_frames, interval=interval, blit=True)\n",
        "    plt.close(fig)  # Prevent static output in Jupyter Notebook\n",
        "    return HTML(ani.to_jshtml())\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to generate toy 2D clusters\n",
        "def toy_2D_clusters(rng=0):\n",
        "    rng = np.random.RandomState(rng)\n",
        "\n",
        "    # A: two tight blobs\n",
        "    blob_A, _ = make_blobs(n_samples=150, centers=[(-7, 4), (-3, 5)],\n",
        "                           cluster_std=0.4, random_state=rng)\n",
        "    # B: elongated Gaussian\n",
        "    cov = np.array([[4, 0.0], [0.0, 0.1]])\n",
        "    blob_B = rng.multivariate_normal(mean=[0, 0], cov=cov, size=120)\n",
        "\n",
        "    # C: half‑moon\n",
        "    moon, _ = make_moons(n_samples=180, noise=0.05, random_state=rng)\n",
        "    moon *= 4; moon += [6, -1]\n",
        "\n",
        "    # D: stray noise\n",
        "    noise = rng.uniform(low=[-10, -3], high=[8, 3], size=(150, 2))\n",
        "\n",
        "    X = np.vstack([blob_A, blob_B, moon, noise])\n",
        "    return StandardScaler().fit_transform(X)  # scale for fair distance use\n",
        "\n",
        "# Generate data and visualize it\n",
        "X_all = toy_2D_clusters(rng=1827)  # Generate toy clusters\n",
        "datasets = [\"blobs\", \"elongated Gaussian\", \"half-moon\", \"noise\"]\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "for k,set in enumerate(datasets):\n",
        "    plt.scatter(X_all[k * 150:(k+1)*150-1, 0], X_all[k * 150:(k+1)*150-1, 1], s=10, label=set)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.title(\"Toy 2D Clusters\")\n",
        "plt.show()\n",
        "\n",
        "num_dict = {\"blobs\": 0,\n",
        "            \"elongated\": 1,\n",
        "            \"moons\": 2,\n",
        "            \"noise\": 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d7c20e",
      "metadata": {
        "id": "57d7c20e"
      },
      "source": [
        "### K-means\n",
        "K-means is a very simple idea for classifying data. Suppose the data lives in $\\mathbb{R}^d$, if we have $k$ points in $\\mathbb{R}^d$, it is natural to classify each point of the wider data set based on which of the $k$ points it is closest to. This then results in $k$ clusters. K-means is the algorithm of iterating this grouping with moving the central points. Simply the algorithm can be described as\n",
        "\n",
        "1. Generate $k$ random starting points, called $c_j^0$\n",
        "2. Find the data points $\\{X_i^{j,n}\\}$ which has $c_j^{n}$ the closest\n",
        "3. Update the central points to according to $c_j^{n+1} = $ mean($\\{X_i^{j,n}\\}$)\n",
        "4. Go to 2, unless $c_j^{n+1} = c_j^n$.\n",
        "\n",
        "Please watch the animated illustration on a dataset of your choosing. Here we use $k=3$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66878796",
      "metadata": {
        "id": "66878796",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of k-means clustering\n",
        "np.random.seed(1827)  # For reproducibility\n",
        "dataset = \"elongated\" # Choose dataset, \"blobs\", \"elongated\", \"moons\", or \"noise\"\n",
        "\n",
        "k  = num_dict[dataset]\n",
        "X = X_all[k * 150:(k+1)*150-1, :]\n",
        "cent = np.hstack((np.random.uniform(np.min(X[:,0]), np.max(X[:,0]), size = (3, 1)), np.random.uniform(np.min(X[:,1]), np.max(X[:,1]), size = (3, 1))))\n",
        "animated_kmeans_process(X, cent, n_frames=10, interval=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125231b6",
      "metadata": {
        "id": "125231b6"
      },
      "source": [
        "We call the central points which define the clusters, *centroids*, and the regions of $\\mathbb{R}^d$ belonging to each centroid are called *voronoids*. The algorithm described is guaranteed to converge, but because the first step in the algorithm, it is not deterministic. For this reason we must run the algorithm multiple times with different initial centroids, and chose the best results. Usually we quantify the best as being the one minimising $\\sum_{i,j} ||{c_j-X_i^j}||$ for the final centroids."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different numbers of clusters on all of the datasets:"
      ],
      "metadata": {
        "id": "h4SaEsL21Aut"
      },
      "id": "h4SaEsL21Aut"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2  # Change this to select a different number of clusters\n",
        "dataset = \"blobs\" # Choose dataset, \"blobs\", \"elongated\", \"moons\", or \"noise\""
      ],
      "metadata": {
        "id": "ki6ysdla1B6c"
      },
      "id": "ki6ysdla1B6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7d2bf1",
      "metadata": {
        "id": "ad7d2bf1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title K-Means clustering\n",
        "# K-Means on all datasets\n",
        "# Chose number of clusters\n",
        "#k = 2  # Change this to select a different number of clusters\n",
        "#dataset = \"blobs\" # Choose dataset, \"blobs\", \"elongated\", \"moons\", or \"noise\"\n",
        "\n",
        "kk = num_dict[dataset]\n",
        "X = X_all[kk * 150:(kk+1)*150-1, :]\n",
        "labels, centroids = k_means(X, n_clusters=k, max_iter=100, n_inits=10)\n",
        "\n",
        "for i in range(k):\n",
        "    plt.scatter(X[labels == i, 0], X[labels == i, 1], s=10, label=f'Cluster {i+1}')\n",
        "plt.legend()\n",
        "plt.title(f'K-Means Clustering on Dataset {dataset} with {k} Clusters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b832e0d",
      "metadata": {
        "id": "0b832e0d"
      },
      "source": [
        "An important limitation of k-means is that the centroids will always be convex. This we can see is not necessarily desirable by looking at the result of K-means on dataset 2. If you were to manually cluster this dataset, you would likely say that each of the crescents are their own clusters, but K-means can't give that as a result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1364b42b",
      "metadata": {
        "id": "1364b42b"
      },
      "source": [
        "### Hierarchical clustering and Dendrograms\n",
        "Hierarchical (agglomerative) clustering views a data set as a hierarchy of nested groups. Begin with every point in its own cluster; at each step merge the two clusters that are closest according to a *linkage* criterion. In this lab we will contrast two extremes:\n",
        "\n",
        "* **Single linkage**\n",
        "\n",
        "  $d_{\\text{single}}(A,B)=\\displaystyle\\min_{i\\in A,\\;j\\in B}\\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert$\n",
        "  Merging is driven by the *nearest* pair of points, so the algorithm is keen to join clusters connected by even a thin “chain” of samples. This tendency reveals elongated or curved structures (our crescent), but it can just as easily let stray points bridge otherwise distinct groups.\n",
        "\n",
        "* **Ward linkage**\n",
        "\n",
        "  $d_{\\text{Ward}}(A,B)=\\dfrac{|A|\\,|B|}{|A|+|B|}\\,\\lVert\\bar{\\mathbf{x}}_A-\\bar{\\mathbf{x}}_B\\rVert^{2}$\n",
        "  Here the distance equals the increase in total within‑cluster variance were $A$ and $B$ merged. The rule therefore favours compact, roughly isotropic clusters—behaviour reminiscent of $k$-means—yet it produces an entire hierarchy instead of fixing $k$ in advance.\n",
        "\n",
        "The sequence of merges is recorded in a **dendrogram**, a binary tree whose leaves are individual samples and whose branch heights mark the linkage distances at which clusters fused. Cutting the dendrogram with a horizontal line at height $h$ yields the partition first produced when inter‑cluster distance exceeded $h$; sliding this cut interactively exposes structure from fine to coarse scales. Single linkage will keep the crescent intact at low cuts but may chain noise into it; Ward will absorb noise into variance‑minimising groups yet cleanly separates the tight Gaussian blobs. Other linkage choices—complete, average, centroid—sit between these two, balancing sensitivity to chaining against a bias for compactness, but today’s exercise with single and Ward already reveals how the linkage metric encodes geometric assumptions that fundamentally shape the resulting clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d1d258",
      "metadata": {
        "cellView": "form",
        "id": "43d1d258"
      },
      "outputs": [],
      "source": [
        "#@title Functions for hierarchical clustering and dendrogram plotting -- no need to look at the code, just run the cell\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import pdist\n",
        "def heirarchical_clustering(X, method='ward', n_clusters=None):\n",
        "    \"\"\"\n",
        "    Perform hierarchical clustering on the data X using the specified method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (N_samples, N_features) array-like\n",
        "        Data to cluster.\n",
        "    method : str\n",
        "        The linkage method to use. Options are 'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Z : (N_samples-1, 4) ndarray\n",
        "        The hierarchical clustering encoded as a linkage matrix.\n",
        "    \"\"\"\n",
        "    # Compute the distance matrix\n",
        "    dist_matrix = pdist(X)\n",
        "    # Perform hierarchical clustering\n",
        "    Z = linkage(dist_matrix, method=method)\n",
        "    return Z\n",
        "\n",
        "def plot_dendrogram(Z, labels=None, title='Hierarchical Clustering Dendrogram'):\n",
        "    \"\"\"\n",
        "    Plot a dendrogram for the hierarchical clustering.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Z : (N_samples-1, 4) ndarray\n",
        "        The hierarchical clustering encoded as a linkage matrix.\n",
        "    labels : list of str, optional\n",
        "        Labels for the leaves of the dendrogram.\n",
        "    title : str, optional\n",
        "        Title of the dendrogram plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    dendrogram(Z, labels=labels, leaf_rotation=90)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample index')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.show()\n",
        "\n",
        "def plot_clusters(X, Z, n_clusters, title='Clusters'):\n",
        "    \"\"\"\n",
        "    Plot the clusters formed by hierarchical clustering.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : (N_samples, N_features) array-like\n",
        "        Data to cluster.\n",
        "    Z : (N_samples-1, 4) ndarray\n",
        "        The hierarchical clustering encoded as a linkage matrix.\n",
        "    n_clusters : int\n",
        "        The number of clusters to form.\n",
        "    title : str, optional\n",
        "        Title of the plot.\n",
        "    \"\"\"\n",
        "    from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "    labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.colorbar(label='Cluster Label')\n",
        "    plt.axis('equal')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 2  # Change this to select a different number of clusters\n",
        "dataset = \"moons\" # Choose dataset, \"blobs\", \"elongated\", \"moons\", or \"noise\"\n",
        "Method = \"ward\"  # Choose the linkage method, \"single\" or \"ward\""
      ],
      "metadata": {
        "id": "rmAaZEg_4xPj"
      },
      "id": "rmAaZEg_4xPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08766198",
      "metadata": {
        "id": "08766198",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Hierarchical clustering\n",
        "# Example usage of hierarchical clustering and dendrogram plotting\n",
        "# Choose the dataset and number of clusters\n",
        "#n_clusters = 2  # Change this to select a different number of clusters\n",
        "#dataset = \"moons\" # Choose dataset, \"blobs\", \"elongated\", \"moons\", or \"noise\"\n",
        "\n",
        "kk = num_dict[dataset]\n",
        "X = X_all[kk * 150:(kk+1)*150-1, :]\n",
        "\n",
        "# You might adjust the method. Ward doesn't work well for all datasets.\n",
        "Z = heirarchical_clustering(X, method=Method) # Try both 'ward' and 'single' methods to see the difference\n",
        "plot_dendrogram(Z, title='Hierarchical Clustering Dendrogram')\n",
        "plot_clusters(X, Z, n_clusters=n_clusters, title='Clusters from Hierarchical Clustering')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d6b882",
      "metadata": {
        "id": "c6d6b882"
      },
      "source": [
        "## Anomaly detection\n",
        "In anomaly detection we flip the usual learning question: instead of modelling *all* the variability in a data set, we first build a notion of what “normal” looks like and then flag samples that fall outside that description. Because true anomalies are typically rare and unlabeled, the task is almost always **unsupervised**. The output is not a class but a **score**—higher means “more unusual”—followed by a threshold that decides whether to raise an alarm. Setting that threshold is part of the problem: too low and the system cries wolf, too high and genuine faults slip through. Practical recipes fix it by an assumed contamination rate (e.g. the highest 2 % of scores) or by cross‑checking against domain limits.\n",
        "\n",
        "Different algorithms operationalise “unusual” in different ways. Local methods, such as the $k$-nearest‑neighbour distance score, compare each point only to its immediate neighbourhood and are quick to spot sparse outliers in dense clouds. Global boundary methods like the One‑Class SVM enclose the bulk of the data in a flexible surface and mark anything outside as suspect; they cope better when “normal” fills an irregular but connected region. In the following sections we apply $k$-NN to a synthetic pulsatile‑pressure trace, showing how the choice of score and threshold trades false positives against missed detections. Other methods can be found in the extra material.\n",
        "\n",
        "We begin by generating the data. The raw data will be a time-series of pressure measurements, with faults such as dropouts and spikes injected into the data set and some Gaussian noise. For the classification, the signal will be divided into windows and the four statistical measures will be used to analyse the windows (mean, standard deviation, skewness, kurtosis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060e03d0",
      "metadata": {
        "cellView": "form",
        "id": "060e03d0"
      },
      "outputs": [],
      "source": [
        "#@title No need to look at the code below, just run it to intitialize the synthetic pressure signal generator\n",
        "def pressure_signal(n_pts=4000,\n",
        "                    fs=200,                       # Hz\n",
        "                    noise_std=0.05,\n",
        "                    spike_frac=0.003,\n",
        "                    dropout_count=4,\n",
        "                    shift_count=0,\n",
        "                    rng=None):\n",
        "    \"\"\"\n",
        "    Generate a synthetic pulsatile pressure trace with injected anomalies.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    t      : (n_pts,)   time axis [s]\n",
        "    p      : (n_pts,)   pressure signal\n",
        "    is_anom: (n_pts,)   boolean mask, True where an anomaly was injected\n",
        "    anomaly_type: (n_pts,)   string array, type of anomaly injected at each point\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(rng+1)\n",
        "    t = np.arange(n_pts) / fs\n",
        "\n",
        "    # --- clean baseline: two harmonics ---------------------------------------\n",
        "    p = 2*np.sin(2*np.pi*2*t) + 0.5*np.sin(2*np.pi*4*t)\n",
        "    anomaly_type = np.full(n_pts, \"baseline\", dtype=object)\n",
        "\n",
        "    # --- additive Gaussian noise --------------------------------------------\n",
        "    p *= 1 + noise_std * rng.standard_normal(n_pts)\n",
        "\n",
        "    is_anom = np.zeros(n_pts, dtype=bool)\n",
        "\n",
        "    # --- spikes --------------------------------------------------------------\n",
        "    n_spike = int(spike_frac * n_pts)\n",
        "    spike_idx = rng.choice(n_pts, n_spike, replace=False)\n",
        "    p[spike_idx] += rng.uniform(3, 4, size=n_spike) * rng.choice([-1, 1], n_spike)\n",
        "    is_anom[spike_idx] = True\n",
        "    anomaly_type[spike_idx] = \"spike\"\n",
        "\n",
        "\n",
        "    # --- drop‑outs -----------------------------------------------------------\n",
        "    for _ in range(dropout_count):\n",
        "        start = rng.integers(0, n_pts-50)\n",
        "        length = rng.integers(20, 40)\n",
        "        p[start:start+length] = 0.0\n",
        "        is_anom[start:start+length] = True\n",
        "        anomaly_type[start:start+length] = \"dropout\"\n",
        "\n",
        "    # --- slow level shifts ---------------------------------------------------\n",
        "    for _ in range(shift_count):\n",
        "        start = rng.integers(0, n_pts-200)\n",
        "        amp   = rng.uniform(-1.5, 1.5)\n",
        "        decay = rng.uniform(0.02, 0.05)        # 1/decay ≈ time‑constant [s]\n",
        "        span  = np.arange(n_pts-start)\n",
        "        shift = amp * np.exp(-decay * span)\n",
        "        p[start:] += shift[: n_pts-start]\n",
        "        is_anom[start:start+150] = True        # mark first 150 pts as anomalous\n",
        "        anomaly_type[start:start+150] = \"shift\"\n",
        "\n",
        "    return t, p, is_anom, anomaly_type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80453fb",
      "metadata": {
        "cellView": "form",
        "id": "e80453fb"
      },
      "outputs": [],
      "source": [
        "#@title Run this to generate and visualize the synthetic pressure signal -- no need to look at the code\n",
        "t, p, is_anom_true,anomaly_type = pressure_signal(fs=200, rng=2020)\n",
        "plt.plot(t, p)\n",
        "\n",
        "colrs = [\"r\", \"g\", \"m\", \"y\"]\n",
        "k = 0\n",
        "for kind in np.unique(anomaly_type):\n",
        "    if kind == \"baseline\":\n",
        "        continue\n",
        "    plt.scatter(t[anomaly_type == kind], p[anomaly_type == kind], s=10, label=f'Injected {kind}', c=colrs[k])\n",
        "    k += 1\n",
        "\n",
        "plt.xlabel('Time [s]'); plt.ylabel('Pressure [arb]')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447425ec",
      "metadata": {
        "cellView": "form",
        "id": "447425ec"
      },
      "outputs": [],
      "source": [
        "#@title Run this to get the window features -- no need to look at the code\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def window_features(p, fs, win_len=0.25, stride=0.125):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    p       : 1‑D array\n",
        "    fs      : sampling frequency\n",
        "    win_len : seconds, window length\n",
        "    stride  : seconds, hop length\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X       : (n_win, 4)  feature matrix  [mean, std, skew, kurtosis]\n",
        "    idx_win : (n_win,)    index of window start in the original signal\n",
        "    \"\"\"\n",
        "    L = int(win_len * fs)\n",
        "    H = int(stride * fs)\n",
        "    idx_win = np.arange(0, len(p)-L+1, H)\n",
        "    feats = []\n",
        "    for i in idx_win:\n",
        "        seg = p[i:i+L]\n",
        "        feats.append([seg.mean(), seg.std(), skew(seg), kurtosis(seg)])\n",
        "    return np.array(feats), idx_win, stride\n",
        "\n",
        "\n",
        "X, idx, H = window_features(p, fs=200)\n",
        "\n",
        "# Plot the features extracted from the windows\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(X[:, 0], label='Mean')\n",
        "plt.title('Mean of Pressure Signal')\n",
        "plt.xlabel('Window Index'); plt.ylabel('Mean Pressure')\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(X[:, 1], label='Standard Deviation')\n",
        "plt.title('Standard Deviation of Pressure Signal')\n",
        "plt.xlabel('Window Index'); plt.ylabel('Std Pressure')\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(X[:, 2], label='Skewness')\n",
        "plt.title('Skewness of Pressure Signal')\n",
        "plt.xlabel('Window Index'); plt.ylabel('Skewness')\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(X[:, 3], label='Kurtosis')\n",
        "plt.title('Kurtosis of Pressure Signal')\n",
        "plt.xlabel('Window Index'); plt.ylabel('Kurtosis')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6075caf1",
      "metadata": {
        "id": "6075caf1"
      },
      "source": [
        "### $k$-NN for anomaly detection\n",
        "The $k$-nearest‑neighbour (kNN) anomaly score treats each sample as a probe of **local density**: if you have to travel far to reach your $k$th neighbour, you must be in a sparse part of the space and are therefore a candidate outlier.  Formally, for a point $\\mathbf{x}$ let\n",
        "\n",
        "$$\n",
        "s_{k}(\\mathbf{x})=\\frac1k\\sum_{j=1}^{k} \\lVert\\mathbf{x}-\\mathbf{x}_{(j)}\\rVert ,\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x}_{(j)}$ is the $j$th nearest neighbour of $\\mathbf{x}$ in the training set.  Large $s_{k}$ means low local density; ranking all samples by this score and thresholding the largest few per cent yields the anomaly set.\n",
        "\n",
        "The only hyper‑parameter is $k$; small values make the score sensitive to fine local structure, while larger $k$ smooth over noise at the risk of diluting rare but genuine anomalies.\n",
        "\n",
        "Once the scores are generated, we need to determine where to put the cut-off. There are many ways of doing this, you could put a hard cap and say that if $s_{k}(x)$ is greater than a specific value it is an anomaly, but this requires good knowledge about the data set in order to choose the threshold, another option is to assume that there are $p$% faults and discard the $p$% of points with the highest scores. The method we will implement here though is to use K-means in order to construct the threshold. Experiment by choosing the value of $k$ and the number of clusters of K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e041d3",
      "metadata": {
        "cellView": "form",
        "id": "b8e041d3"
      },
      "outputs": [],
      "source": [
        "#@title run this to initialize the k-NN anomaly detection code -- no need to look at the code\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def detect_knn_anomalies(X, k=5, contamination=0.05, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    k‑NN anomaly detection (distance‑based).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X            : (N_samples, N_features) array\n",
        "    k            : int, number of neighbours\n",
        "    contamination: float, fraction of samples to flag as anomalies\n",
        "    metric       : str, distance metric for scikit‑learn\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    is_anom      : (N_samples,) bool array, True where sample is anomalous\n",
        "    scores       : (N_samples,) float array, k‑NN distance score\n",
        "    cutoff       : float, threshold applied to scores\n",
        "    \"\"\"\n",
        "\n",
        "    # # standardize the data\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "    nbrs = NearestNeighbors(n_neighbors=k + 1, metric=metric).fit(X)\n",
        "    # distances[:, 0] is distance to itself (0) → skip\n",
        "    distances, _ = nbrs.kneighbors(X, n_neighbors=k + 1, return_distance=True)\n",
        "    scores = distances[:, 1:].mean(axis=1)     # average distance to k nearest neighbours\n",
        "\n",
        "    # threshold by percentile\n",
        "    cutoff = np.percentile(scores, 100 * (1 - contamination))\n",
        "    is_anom = scores > cutoff\n",
        "    return is_anom, scores, cutoff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d6b138f",
      "metadata": {
        "id": "6d6b138f"
      },
      "outputs": [],
      "source": [
        "# Chose k, contamination ratio, and K-means clusters\n",
        "k = 5  # number of neighbours for k-NN\n",
        "contamination = 0.1  # fraction of samples to flag as anomalies when using thresholding\n",
        "k_means_clusters = 3  # number of clusters for K-means\n",
        "is_anom, scores, cutoff = detect_knn_anomalies(X, k=k, contamination=contamination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69276d2",
      "metadata": {
        "cellView": "form",
        "id": "e69276d2"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the k-NN anomaly detection results -- no need to look at the code\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(scores, '.', label='score')\n",
        "plt.axhline(cutoff, color='r', linestyle='--', label='threshold')\n",
        "plt.xlabel('Window index'); plt.ylabel('k‑NN distance score')\n",
        "plt.title('k‑NN anomaly scores')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the anomalies on the original pressure signal\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(t, p, label='Pressure Signal')\n",
        "legended = False\n",
        "for i in range(len(is_anom)):\n",
        "    if is_anom[i]:\n",
        "        try:\n",
        "            plt.axvspan(t[idx[i]], t[idx[i]]+H, color='red', alpha=0.3, label='Detected windows' if not legended else \"\")\n",
        "        except IndexError:\n",
        "            plt.axvspan(t[idx[i]], t[-1], color='red', alpha=0.3, label='Detected windows' if not legended else \"\")\n",
        "        legended = True\n",
        "plt.xlabel('Time [s]'); plt.ylabel('Pressure [arb]')\n",
        "plt.legend()\n",
        "\n",
        "# overlay the true anomalies\n",
        "for kind in np.unique(anomaly_type):\n",
        "    if kind == \"baseline\":\n",
        "        continue\n",
        "    plt.scatter(t[anomaly_type == kind], p[anomaly_type == kind], s=10, label=f'Injected {kind}')\n",
        "plt.xlabel('Time [s]'); plt.ylabel('Pressure [arb]')\n",
        "plt.legend()\n",
        "plt.title('Detected Anomalies when using a percentile threshold')\n",
        "plt.show()\n",
        "\n",
        "# Do k-means clustering on the scores\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=k_means_clusters, random_state=42)\n",
        "kmeans.fit(np.reshape(scores,(len(scores), 1)))\n",
        "\n",
        "# plot the scores and the clusters\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.scatter(np.arange(len(scores)), scores, c=kmeans.labels_, cmap='viridis', s=10, label='k-means clusters')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot the time series and indicate the anomalies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(t, p, label='Pressure Signal')\n",
        "legended = False\n",
        "biggest_cluster = np.argmax(np.bincount(kmeans.labels_))  # find the largest cluster\n",
        "is_anom = kmeans.labels_ != biggest_cluster\n",
        "for i in range(len(is_anom)):\n",
        "    if is_anom[i]:\n",
        "        try:\n",
        "            plt.axvspan(t[idx[i]], t[idx[i]]+H, color='red', alpha=0.3, label='Detected windows' if not legended else \"\")\n",
        "        except IndexError:\n",
        "            plt.axvspan(t[idx[i]], t[-1], color='red', alpha=0.3, label='Detected windows' if not legended else \"\")\n",
        "        legended = True\n",
        "\n",
        "# overlay the true anomalies\n",
        "for kind in np.unique(anomaly_type):\n",
        "    if kind == \"baseline\":\n",
        "        continue\n",
        "    plt.scatter(t[anomaly_type == kind], p[anomaly_type == kind], s=10, label=f'Injected {kind}')\n",
        "plt.xlabel('Time [s]'); plt.ylabel('Pressure [arb]')\n",
        "plt.legend()\n",
        "plt.title('Detected Anomalies using k-means clustering on k-NN scores')\n",
        "plt.show()\n",
        "\n",
        "print(f\"K-means anomalies flagged: {is_anom.sum()} of {len(X)} windows\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f698249",
      "metadata": {
        "id": "7f698249"
      },
      "source": [
        "\n",
        "## EXTRA MATERIALS BELOW HERE (ONE-CLASS SVM, GRAPH BASED CLUSTERING)\n",
        "\n",
        "### One-Class Support Vector Machines\n",
        "The **One‑Class SVM** builds a *global* envelope around the training data by learning a decision function that is positive on “normal” samples and negative elsewhere. In scikit‑learn this is implemented as the ν‑formulation of the support‑vector machine: given only the uncontaminated portion of the data, the algorithm maps each point into a high‑dimensional feature space (by default with an RBF kernel) and then finds the hyper‑plane that maximises its distance from the origin while allowing at most a fraction ν of the training points to fall on the wrong side. Geometrically, this hyper‑plane translates back into input space as a smooth, flexible boundary containing roughly $(1\\!-\\!ν)\\times100\\%$ of the data. The signed distance to that boundary is returned by `decision_function`; scikit‑learn’s `predict` simply labels negative distances as anomalies.\n",
        "\n",
        "Three hyper‑parameters control the fit:\n",
        "\n",
        "* **`nu`** (0 < ν ≤ 1) – upper bound on the expected proportion of outliers *and* a lower bound on the fraction of support vectors; start around 0.05 if you believe ≤ 5 % of points are anomalous.\n",
        "* **`kernel`** – RBF (default) captures non‑linear shapes; linear and polynomial are available for strictly convex data.\n",
        "* **`gamma`** – inverse squared length‑scale of the RBF; `'scale'` (default) sets it to $1/(p · \\text{var}(X))$, but tuning tighter (larger gamma) shrinks the envelope, flagging more points.\n",
        "\n",
        "Because the model optimises a global objective, it is less sensitive than kNN to local sampling density and can enclose an irregular yet connected “normal” region without fragmenting it. Pre‑processing the data with a `StandardScaler` is essential: the kernel uses Euclidean distance, so unscaled features distort the boundary. Once fitted, the One‑Class SVM scores new observations in $O(N_{\\text{SV}} p)$ time, where $N_{\\text{SV}}$ is the number of support vectors, making it fast enough for real‑time monitoring after the initial training pass.\n",
        "\n",
        "For our lab, it will have to suffice to experiment a little with $\\nu$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8563e1b5",
      "metadata": {
        "id": "8563e1b5"
      },
      "outputs": [],
      "source": [
        "# Choose the value of nu for the one-class SVM\n",
        "nu = 0.1 # Try 0.01, 0.05, 0.1, 0.15, 0.3 or any other value between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a795016",
      "metadata": {
        "cellView": "form",
        "id": "7a795016"
      },
      "outputs": [],
      "source": [
        "#@title Run this to perform One-Class SVM anomaly detection -- no need to look at the code\n",
        "# --- One‑Class SVM anomaly detection -----------------------------------------\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# 0.  Choose a training mask  (middle 80 % of windows)\n",
        "n_win = len(idx)\n",
        "train_mask = (idx >= idx[int(0.10 * n_win)]) & (idx < idx[int(0.90 * n_win)])\n",
        "\n",
        "# 1.  Standardise features (mean‑0, var‑1)   ------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "# 2.  Fit One‑Class SVM on \"normal\" windows   -----------------------------\n",
        "ocsvm = OneClassSVM(kernel='rbf',\n",
        "                    nu=nu,         # expected fraction of anomalies\n",
        "                    gamma='scale')   # 1 / (n_features * var)\n",
        "ocsvm.fit(X_std[train_mask])\n",
        "\n",
        "# 3.  Decision scores (positive ⇒ inlier, negative ⇒ outlier) ------------\n",
        "scores = ocsvm.decision_function(X_std)      # shape (n_win,)\n",
        "\n",
        "# 4.  Flag anomalies by sign  (or set your own threshold) ----------------\n",
        "is_anom = scores < 0\n",
        "\n",
        "# 5.  Plot score curve and threshold -------------------------------------\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.plot(scores, '.-', label='decision score')\n",
        "plt.axhline(0, color='r', linewidth=1, label='default threshold')\n",
        "plt.xlabel('Window index'); plt.ylabel('OC‑SVM score')\n",
        "plt.title('One‑Class SVM anomaly scores')\n",
        "plt.legend(); plt.tight_layout()\n",
        "\n",
        "# 6.  Visualise anomalies on the original signal --------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(t, p, label='Pressure Signal')\n",
        "legended = False\n",
        "for i in range(len(is_anom)):\n",
        "    if is_anom[i]:\n",
        "        try:\n",
        "            plt.axvspan(t[idx[i]], t[idx[i]]+H, color='red', alpha=0.3, label='Anomaly windows' if not legended else \"\")\n",
        "        except IndexError:\n",
        "            plt.axvspan(t[idx[i]], t[-1], color='red', alpha=0.3, label='Anomaly windows' if not legended else \"\")\n",
        "        legended = True\n",
        "\n",
        "# overlay the true anomalies\n",
        "plt.scatter(t[is_anom_true], p[is_anom_true], c='g', s=10, label='True anomalies')\n",
        "plt.xlabel('Time [s]'); plt.ylabel('Pressure [arb]')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a94698",
      "metadata": {
        "id": "e8a94698"
      },
      "source": [
        "### Graph based clustering\n",
        "Graph‑based clustering begins by **turning the data set into a network**. Each observation is a node; an edge appears between two nodes if they are judged “similar,” and the edge weight encodes *how* similar. Building this *similarity graph* is the key modelling step. Some of the most common choices are:\n",
        "\n",
        "* **ε‑neighbourhood graph:** connect points whose Euclidean distance is below a user‑chosen ε.\n",
        "* **$k$-nearest‑neighbour graph:** draw a directed (or symmetrised) edge from each point to its $k$ closest neighbours.\n",
        "* **Kernel graph:** keep *all* pairs but weight them with a decaying function such as $w_{ij}=\\exp(-\\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert^{2}/2\\sigma^{2})$.\n",
        "\n",
        "These constructions replace geometric coordinates by **connectivity**, making curved or elongated clusters easy to capture: if points lie on the same manifold they remain linked through short paths even when straight‑line distance is misleading.\n",
        "\n",
        "In many practical problems the notion of similarity itself is multi‑faceted—think of pressure sensors that are close in space *and* highly correlated in time. A graph can express this by *combining* cues in the weight formula, for example\n",
        "\n",
        "$$\n",
        "w_{ij}= \\exp\\!\\bigl(-\\tfrac{\\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert^{2}}{2\\sigma_x^{2}}\\bigr)\\,\n",
        "        \\exp\\!\\bigl(-\\tfrac{|t_i-t_j|^{2}}{2\\sigma_t^{2}}\\bigr),\n",
        "$$\n",
        "\n",
        "or by adding separate edge sets and summing their weights. The important point is that **once the graph is built, the downstream clustering step is algorithm‑agnostic**: spectral partitioning, modularity maximisation, or label‑propagation can all act on the same adjacency matrix. Our notebooks will therefore separate the workflow into two stages—(i) graph construction and inspection, (ii) graph cutting—so that you can see how different similarity choices influence the communities that emerge when we apply Ward‑like or spectral cuts later on.\n",
        "\n",
        "\n",
        "#### $\\varepsilon$-Neighbourhood\n",
        "This is a very simple kind of graph. Here, the pointwise distance between every point is computed. If $||x_i-x_j||$ is smaller than $\\varepsilon$ for a user specified value, then the two nodes are connected. In the unweighted case this means that we set $w_{ij}=w_{ji} = 1$, while in the weighted case we say that $w_{ij} = f(||x_i-x_j||)$ for some non-negative decaying function $f$. For all unconnected nodes $x_i,x_j$, the weight value is $w_{ij}=0$.\n",
        "\n",
        "Experiment with different values for the neighbourhood size $\\varepsilon$ on the same datasets, since these graphs are not fully connected this can sometimes result in sufficiently good clustering without ever actually cutting the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82d4bed",
      "metadata": {
        "cellView": "form",
        "id": "c82d4bed"
      },
      "outputs": [],
      "source": [
        "#@title Functions for creating and plotting graphs from the data -- no need to look at the code, just run the cell to define the functions\n",
        "\n",
        "def compute_distance_matrix(X):\n",
        "    \"\"\"\n",
        "    Compute the pairwise distance matrix for the dataset X.\n",
        "    X is expected to be a 2D array where each row is a data point.\n",
        "    \"\"\"\n",
        "    D = np.zeros((X.shape[0], X.shape[0]), dtype=float)\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(i + 1, X.shape[0]):\n",
        "            D[i, j] = np.linalg.norm(X[i] - X[j])\n",
        "            D[j, i] = D[i, j]  # exploit symmetry\n",
        "    return D\n",
        "\n",
        "\n",
        "def eps_neighbourhood_graph(X, epsilon, weighted=False):\n",
        "    \"\"\"\n",
        "    Create an epsilon-neighbourhood graph from the dataset X.\n",
        "    X is expected to be a 2D array where each row is a data point.\n",
        "    Returns an adjacency matrix where entry (i, j) is True if points i and j are within epsilon distance.\n",
        "    \"\"\"\n",
        "    D = compute_distance_matrix(X)\n",
        "    adjacency_matrix = D < epsilon\n",
        "    np.fill_diagonal(adjacency_matrix, False)  # no self-loops\n",
        "\n",
        "    if weighted:\n",
        "        # The metric is exp(-D/epsilon) to give higher weights to closer points\n",
        "        adjacency_matrix = adjacency_matrix.astype(float) * np.exp(-(D / epsilon)**2)\n",
        "    return adjacency_matrix\n",
        "\n",
        "def knn_neighbourhood_graph(X, k, mode=\"and\", weighted=False):\n",
        "    \"\"\"\n",
        "    Create a k-nearest neighbour graph from the dataset X.\n",
        "    X is expected to be a 2D array where each row is a data point.\n",
        "    Returns an adjacency matrix where entry (i, j) is True if points i and j are among the k nearest neighbours.\n",
        "    \"\"\"\n",
        "    if mode not in [\"and\", \"or\"]:\n",
        "        raise ValueError(\"Mode must be 'and' or 'or'.\")\n",
        "\n",
        "    D = compute_distance_matrix(X)\n",
        "    adjacency_matrix = np.zeros(D.shape, dtype=bool)\n",
        "\n",
        "    for i in range(D.shape[0]):\n",
        "        # Get indices of the k smallest distances (excluding self)\n",
        "        knn_indices = np.argsort(D[i])[:k + 1]\n",
        "        adjacency_matrix[i, knn_indices] = True\n",
        "\n",
        "    if mode == \"and\":\n",
        "        adjacency_matrix = adjacency_matrix & adjacency_matrix.T  # make it symmetric\n",
        "    elif mode == \"or\":\n",
        "        adjacency_matrix = adjacency_matrix | adjacency_matrix.T\n",
        "\n",
        "    if weighted:\n",
        "        # The metric is exp(-D/max(D)) to give higher weights to closer points\n",
        "        adjacency_matrix = adjacency_matrix.astype(float) * np.exp(-(D / np.max(D*adjacency_matrix))**2)\n",
        "\n",
        "    np.fill_diagonal(adjacency_matrix, False)  # no self-loops\n",
        "    return adjacency_matrix\n",
        "\n",
        "\n",
        "\n",
        "def plot_graph_from_adjacency_matrix(adjacency_matrix, X, title=\"Graph\"):\n",
        "    \"\"\"\n",
        "    Plot a graph from an adjacency matrix and data points.\n",
        "    \"\"\"\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    G = nx.from_numpy_array(adjacency_matrix)\n",
        "    pos = {i: (X[i, 0], X[i, 1]) for i in range(X.shape[0])}\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    nx.draw(G, pos, with_labels=True, node_size=50, font_size=10)\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbc21cb",
      "metadata": {
        "id": "9bbc21cb"
      },
      "outputs": [],
      "source": [
        "# Choose data set\n",
        "dataset = 2  # Change this to select a different dataset 0-3\n",
        "X = X_all[dataset * 150:(dataset+1)*150-1, :]\n",
        "\n",
        "epsilon = 0.6  # Set epsilon for the epsilon-neighbourhood graph, try a range of values from 0.1 to 1.0, see if you can find a good value for the dataset\n",
        "adjacency_matrix = eps_neighbourhood_graph(X, epsilon)\n",
        "plot_graph_from_adjacency_matrix(adjacency_matrix, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cdf23aa",
      "metadata": {
        "id": "7cdf23aa"
      },
      "source": [
        "#### $k$-Nearest Neighbours for building graphs\n",
        "Often, $k$-NN is thought of as a supervised classification algorithm, but that is not the only way to use it. Here we will be using it in an unsupervised manner to build graphs. As we are only going to be working with symmetric graphs, i.e. if $x_i$ is connected to $x_j$, then $x_j$ is connected to $x_i$, there are two versions of the algorithm. We name these two version *and*-mode, and *or*-mode, the names are given based on how ambiguous situations are resolved. The algorithm goes like this, for a given value of $k$.\n",
        "1. For each $x_i$ in the data set, determine which \\{x_{i_j}\\}_{j=1}^k, are the $k$ closest other points in the data set.\n",
        "2. In and-mode, connect $x_i$ to $x_j$ only if $x_j$ is one of $x_i$'s $k$ closest neighbours AND $x_i$ is one of $x_j$'s $k$ closest neighbours. In or-mode, connect $x_i$ to $x_j$ if $x_j$ is one of $x_i$'s $k$ closest neighbours OR $x_i$ is one of $x_j$'s $k$ closest neighbours.\n",
        "3. Determine the weights $w_{ij}$, similarly to how it was described in the previous section\n",
        "\n",
        "Try both modes and different values of $k$ on the previous datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9a5f72",
      "metadata": {
        "id": "1f9a5f72"
      },
      "outputs": [],
      "source": [
        "dataset = 3  # Change this to select a different dataset 0-3\n",
        "X = X_all[dataset * 150:(dataset+1)*150-1, :]\n",
        "k = 16  # Set k for the k-nearest neighbour graph\n",
        "mode = \"or\"  # Choose mode: \"and\" or \"or\"\n",
        "adjacency_matrix_knn = knn_neighbourhood_graph(X, k, mode)\n",
        "plot_graph_from_adjacency_matrix(adjacency_matrix_knn, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f58132f5",
      "metadata": {
        "id": "f58132f5"
      },
      "source": [
        "#### Cutting graphs and spectral clustering\n",
        "Once the data have been converted into a weighted (or unweighted) graph, clustering can be phrased as cutting that graph into two or more pieces: remove a set of edges so that the remaining connected components become the clusters. A naïve strategy would be to sort all edges by weight and simply delete the weakest ones until the graph falls apart. This works on toy data but quickly shows its limitations: a single noisy point may detach prematurely, elongated clusters may shatter into chains, and large clusters are penalised because they contain more edges in absolute terms. In other words, how we measure the cost of a cut—and whether we balance that cost by cluster size—matters as much as which edges we remove. In order to continue the discussion we introduce two new matrices, the degree matrix $D$, which is diagonal and defined by $d_{ii} = \\sum_j w_{ij}$, and the Laplacian of the graph $L=D-W$. We must also introduce notions of size on a graph, we usually use two measures of the graph $G$, $|G|$ is the number of vertices (nodes) in the graph, and vol$(G)$ is the diagonal sum of the degree matrix belonging to $G$.\n",
        "\n",
        "A more principled view measures the cost of a cut as the total weight of edges that run between clusters and then balances that cost by the size of the clusters produced. Two widely used objectives embody this idea. *Ratio Cut* divides the inter‑cluster edge weight by the number of vertices on either side, discouraging tiny fragments; *Normalised Cut* divides instead by the total edge weight incident to each side, giving greater protection when degrees vary widely. More formally, we need to introduce a way to measure the connectedness of subgraphs. If $A$ and $B$ are subgraphs, their connectedness is defined as $W(A,B) = \\sum_{i\\in A, j\\in B} w_{ij}$. The values of Ratio Cut and Normalised cut then read\n",
        "* RatioCut$(A_1,\\dots A_k) = \\frac{1}{2}\\sum_{i=1}^k \\frac{W(A_i,A_i^c)}{|A_i|}$, where $A^c$ is the complement of $A$, and\n",
        "* NCut $(A_1,\\dots A_k) = \\frac{1}{2}\\sum_{i=1}^k \\frac{W(A_i,A_i^c)}{\\text{vol}(A_i)}$.\n",
        "\n",
        "Both of these quantities become large if any subgraph becomes very small. Finding the true minimiser of these two quantities is a very difficult discrete optimisation problem. Thankfully the Rayleigh-Ritz theorem, and a relaxation of the discrete nature, makes both problems into eigenvalue problems of the Laplacian $L$ and the normalised Laplacian $\\tilde{L} = D^{-1/2}LD^{-1/2}$.\n",
        "\n",
        "The methods for both are identical except for which Laplacian is used. The algorithm is (for RatioCut) as follows for $k$ clusters\n",
        "\n",
        "1. Compute the Laplacian\n",
        "2. Compute the $k$ first eigenvectors $\\mathcal{U} = [u_1,\\dots, u_k]$ of $L$\n",
        "3. To recover the discrete structure, treat each row of $\\mathcal{U}$ as $k$ dimensional data and cluster these using some other algorithm such as K-means or hierarchical clustering\n",
        "\n",
        "\n",
        "Experiment on our previous datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4babefae",
      "metadata": {
        "cellView": "form",
        "id": "4babefae"
      },
      "outputs": [],
      "source": [
        "#@title Code for spectral clustering -- run but no need to look at the code\n",
        "def spectral_clustering(adjacency_matrix, n_clusters, type = \"RatioCut\", cluster_method=\"kmeans\"):\n",
        "    \"\"\"\n",
        "    Perform spectral clustering on the given adjacency matrix.\n",
        "    Returns the cluster labels for each data point.\n",
        "    \"\"\"\n",
        "    W = adjacency_matrix\n",
        "    D = np.diag(np.sum(W, axis=1))  # degree matrix\n",
        "    L = D - W  # Laplacian matrix\n",
        "\n",
        "    if type == \"RatioCut\":\n",
        "        # Compute the eigenvalues and eigenvectors of the Laplacian\n",
        "        _, eigvecs = np.linalg.eigh(L)\n",
        "        # Use the first n_clusters eigenvectors\n",
        "        X = eigvecs[:, :n_clusters]\n",
        "    elif type == \"NormalizedCut\":\n",
        "        # Compute the normalized Laplacian\n",
        "        D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\n",
        "        L_norm = D_inv_sqrt @ L @ D_inv_sqrt\n",
        "        _, eigvecs = np.linalg.eigh(L_norm)\n",
        "        # Use the first n_clusters eigenvectors\n",
        "        X = eigvecs[:, :n_clusters]\n",
        "    else:\n",
        "        raise ValueError(\"Unknown type. Use 'RatioCut' or 'NormalizedCut'.\")\n",
        "    if cluster_method == \"kmeans\":\n",
        "        from sklearn.cluster import KMeans\n",
        "        clustering = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = clustering.fit_predict(X)\n",
        "    elif cluster_method == \"dendrogram\":\n",
        "        from scipy.cluster.hierarchy import fcluster, linkage\n",
        "        Z = linkage(X, method='ward')\n",
        "        labels = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
        "    else:\n",
        "        raise ValueError(\"Unknown cluster_method. Use 'kmeans' or 'dendrogram'.\")\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e88e58",
      "metadata": {
        "id": "67e88e58"
      },
      "outputs": [],
      "source": [
        "# Perform spectral clustering on the datasets\n",
        "dataset = 2  # Change this to select a different dataset 0-3\n",
        "# Choose method for graph construction\n",
        "graph_method = \"eps\"  # Set to knn or eps\n",
        "k_or_epsilon = 0.5  # Set k for k-nearest neighbour graph or epsilon for epsilon-neighbourhood graph\n",
        "weighted = True  # Set to True if you want weighted edges\n",
        "n_clusters = 2  # Number of clusters to find\n",
        "mode = \"and\" # Set to \"and\" or \"or\" for k-nearest neighbour graph\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# No need to read the code below, just run it to perform spectral clustering and visualize the results\n",
        "X = X_all[dataset * 150:(dataset+1)*150-1, :]\n",
        "if graph_method == \"knn\":\n",
        "    adjacency_matrix_final = knn_neighbourhood_graph(X_all[dataset * 150:(dataset+1)*150-1, :], k_or_epsilon, mode=mode, weighted=weighted)\n",
        "elif graph_method == \"eps\":\n",
        "    adjacency_matrix_final = eps_neighbourhood_graph(X_all[dataset * 150:(dataset+1)*150-1, :], k_or_epsilon, weighted=weighted)\n",
        "else:\n",
        "    raise ValueError(\"Unknown graph_method. Use 'knn' or 'eps'.\")\n",
        "plt.figure(figsize=(10, 10))\n",
        "modes = [\"RatioCut\", \"NormalizedCut\"]\n",
        "cluster_types = [\"kmeans\", \"dendrogram\"]\n",
        "for i, mode in enumerate(modes):\n",
        "    for j, cluster_type in enumerate(cluster_types):\n",
        "        labels = spectral_clustering(adjacency_matrix_final, n_clusters, type=mode, cluster_method=cluster_type)\n",
        "        plt.subplot(len(modes), len(cluster_types), i * len(cluster_types) + j + 1)\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "        plt.title(f\"{mode} - {cluster_type}\")\n",
        "        plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Show the original graph\n",
        "plot_graph_from_adjacency_matrix(adjacency_matrix_final, X, title=\"Original Graph\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}